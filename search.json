[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 654 Machine Learning",
    "section": "",
    "text": "Summary\n\nInstructor: Nipun Batra (nipun.batra@iitgn.ac.in)\nTeaching Assistants: Zeel B Patel, Shriraj Sawant, Shrutimoy, Sarth Dubey, Hitarth Gandhi, Saagar Parikh, Eshan Gujarathi, Gautam Vashishta, Aadesh Desai\nCourse Timings: Tuesday, Thursday 330-450 PM IST in 1/101\nSlack Invite\n\n\n\nPre-requisites:\n\nGood experience in Python programming\nProbability\nLinear Algebra\n\nCourse preparation: Students are encouraged to study some of the following to refresh their understanding of some of the prerequisities before the course formally begins.\n\nFirst four chapters of the Python Data Science handbook\nSome material on Linear Algebra\nKhan academy course on Stats and Probability\n\n\n\nReference textbooks:\n\nGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning with Applications in R\nChristopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006.[Freely available online]\nFriedman J, Hastie T, Tibshirani R. The elements of statistical learning. New York, NY, USA:: Springer series in statistics; 2001.[Freely available online]\nDuda RO, Hart PE, Stork DG. Pattern classification. John Wiley & Sons; 2012 Nov 9.\nMitchell TM. Machine learning. 1997. Burr Ridge, IL: McGraw Hill. 1997;45(37):870-7.\nMurphy, K. Machine Learning: A Probabilistic Perspective. MIT Press\nGoodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning. Cambridge: MIT press; 2016 Nov 18.[Freely available online]\n\n\n\nSome other ML courses\n\nNPTEL course by Balaram Ravindran\nCMU course by Tom Mitchell and Maria-Florina Balcan\nCoursera ML course by Andrew Ng\nFAST.ai course on ML\nPractical deep learning for coders by FAST.ai\nCourse by Alex Ihler, UCI"
  },
  {
    "objectID": "notebooks/posts/python-utils.html",
    "href": "notebooks/posts/python-utils.html",
    "title": "Some Python Utilities",
    "section": "",
    "text": "Function Argument Unpacking\nReference: https://www.youtube.com/watch?v=YWY4BZi_o28\n\ndef print_vec(x, y, z):\n    print(f\"[{x} \\n{y} \\n{z}]\")\n\n\nprint_vec(1, 2, 3)\n\n[1 \n2 \n3]\n\n\n\ntuple_vec = (1, 0, 1)\n#print_vec(tuple_vec)\nprint_vec(tuple_vec[0], tuple_vec[1], tuple_vec[2])\n\n[1 \n0 \n1]\n\n\n\nlist_vec = [1, 0, 1]\n\nprint_vec(tuple_vec[0], tuple_vec[1], tuple_vec[2])\nprint(\"*\"*20)\nprint_vec(*tuple_vec)\nprint(\"*\"*20)\n\nprint_vec(*list_vec)\n\n[1 \n0 \n1]\n********************\n[1 \n0 \n1]\n********************\n[1 \n0 \n1]\n\n\n\ndictionary_vec = {\"x\": 1, \"y\": 0, \"z\": 1}\nprint_vec(**dictionary_vec)\n\n[1 \n0 \n1]\n\n\n\ndictionary_vec = {\"a\": 1, \"b\": 0, \"c\":1}\nprint_vec(**dictionary_vec)\n\nTypeError: print_vec() got an unexpected keyword argument 'a'\n\n\n\nprint(*dictionary_vec)\n\na b c\n\n\n\n\nZip\n\nlist(zip([1, 2, 3], ['a', 'b', 'c'], [7, 8, 9]))\n\n[(1, 'a', 7), (2, 'b', 8), (3, 'c', 9)]\n\n\n\n\nItertools Product\n\nimport itertools\nlist(itertools.product([1, 2], ['a', 'b', 'c'], [7, 8, 9]))\n\n[(1, 'a', 7),\n (1, 'a', 8),\n (1, 'a', 9),\n (1, 'b', 7),\n (1, 'b', 8),\n (1, 'b', 9),\n (1, 'c', 7),\n (1, 'c', 8),\n (1, 'c', 9),\n (2, 'a', 7),\n (2, 'a', 8),\n (2, 'a', 9),\n (2, 'b', 7),\n (2, 'b', 8),\n (2, 'b', 9),\n (2, 'c', 7),\n (2, 'c', 8),\n (2, 'c', 9)]"
  },
  {
    "objectID": "notebooks/posts/logistic.html",
    "href": "notebooks/posts/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "# Create linearly separable data in 2d\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n                            n_clusters_per_class=2, class_sep=1.5, random_state=42)\n\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')\n\n<matplotlib.collections.PathCollection at 0x12b5ed280>\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Without regularization\nclf = LogisticRegression(random_state=0, penalty='none').fit(X, y)\nclf.coef_, clf.intercept_\n\n(array([[26.23339925, -5.01002931]]), array([1.74951957]))\n\n\n\n# Create a surface plot of the decision boundary for any theta_0, theta_1, theta_2\n\ndef plot_decision_boundary(theta_0, theta_1, theta_2):\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = -(theta_0 + theta_1 * x_lin) / theta_2\n    plt.plot(x_lin, y_lin, 'k--', label='Decision boundary ($\\sigma(z) = 0.5$))', lw=5)\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n\n    # Plot the probability of class 1 contour\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n    plt.contourf(x1, x2, z, linestyles='dashed')\n    plt.colorbar()\n\n    # Plot the data\n    plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')\n    plt.legend(loc='best')\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary(theta_0, theta_1, theta_2)>\n\n\n\n# Create a 3d plot of the decision boundary for any theta_0, theta_1, theta_2\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = np.linspace(-4, 4, 100)\n    X_g, Y_g = np.meshgrid(x_lin, y_lin)\n    Z_g = -(theta_0 + theta_1 * X_g + theta_2 * Y_g)\n    #ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2)\n    ax.set_xlabel(r'$x_1$')\n    ax.set_ylabel(r'$x_2$')\n    ax.set_zlabel(r'$x_3$')\n    ax.set_title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n    \n    # Scatter plot of data (class 1 is Z = 1, class 0 is Z = 0)\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n    # Plot the 3d sigmoid function\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n    ax.plot_surface(x1, x2, z, alpha=0.2, color='green')\n    \n    # Rotate the plot so that the sigmoid function is visible\n    ax.view_init(azim, elev)\n\n    # Plot the decision plane\n    ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2, color='k')\n\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary_3d,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1),\n            azim=FloatSlider(min=-180, max=180, step=1, value=30),\n            elev=FloatSlider(min=-180, max=180, step=1, value=30))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30)>\n\n\n\n# Create two 3d plot any theta_0, theta_1, theta_2\n# First showing the decision boundary\n# Second showing the probability of class 1\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30):\n    fig = plt.figure(figsize=(10, 8))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122, projection='3d')\n\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = np.linspace(-4, 4, 100)\n    X_g, Y_g = np.meshgrid(x_lin, y_lin)\n    Z_g = -(theta_0 + theta_1 * X_g + theta_2 * Y_g)\n    #ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2)\n    ax1.set_xlabel(r'$x_1$')\n    ax1.set_ylabel(r'$x_2$')\n    ax1.set_zlabel(r'$x_3$')\n    ax1.set_title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n\n    # Scatter plot of data (class 1 is Z = 1, class 0 is Z = 0)\n    ax1.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax1.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n    # Plot the 3d sigmoid function\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n\n    # Plot the decision plane\n    ax1.plot_surface(X_g, Y_g, Z_g, alpha=0.2, color='k')\n\n    # Plot the probability of class 1\n    ax2.plot_surface(x1, x2, z, alpha=0.2, color='black')\n    ax2.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax2.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n\n     # Rotate the plot so that the sigmoid function is visible\n    ax1.view_init(azim, elev)\n    ax2.view_init(azim, elev)\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary_3d,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1),\n            azim=FloatSlider(min=-180, max=180, step=1, value=30),\n            elev=FloatSlider(min=-180, max=180, step=1, value=30))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30)>"
  },
  {
    "objectID": "notebooks/posts/names.html",
    "href": "notebooks/posts/names.html",
    "title": "Generating names using MLPs",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Get some names from https://github.com/MASTREX/List-of-Indian-Names\n\n\n!wget https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv -O names-long.csv\n\n--2023-03-31 17:00:55--  https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 85538 (84K) [text/plain]\nSaving to: ‘names-long.csv’\n\nnames-long.csv      100%[===================>]  83.53K  --.-KB/s    in 0.07s   \n\n2023-03-31 17:00:55 (1.12 MB/s) - ‘names-long.csv’ saved [85538/85538]\n\n\n\n\n!head names-long.csv\n\n,Name\n0,aabid\n1,aabida\n2,aachal\n3,aadesh\n4,aadil\n5,aadish\n6,aaditya\n7,aaenab\n8,aafreen\n\n\n\nwords = pd.read_csv('names-long.csv')[\"Name\"]\nwords = words.str.lower()\nwords = words.str.strip()\nwords = words.str.replace(\" \", \"\")\n\nwords = words[words.str.len() > 2]\nwords = words[words.str.len() < 10]\n\n# Randomly shuffle the words\nwords = words.sample(frac=1).reset_index(drop=True)\nwords = words.tolist()\n\n# Remove words having non alphabets\nwords = [word for word in words if word.isalpha()]\nwords[:10]\n\n['sehran',\n 'iema',\n 'bajinder',\n 'manoj',\n 'nayaka',\n 'ajmal',\n 'navleen',\n 'akhtari',\n 'samsung',\n 'imtyaz']\n\n\n\nlen(words)\n\n6184\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nprint(itos)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n\n\n\nblock_size = 4 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words[:]:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    #print(''.join(itos[i] for i in context), '--->', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([44325, 4]), torch.int64, torch.Size([44325]), torch.int64)\n\n\n\n# Embedding layer for the context\n\nemb_dim = 2\nemb = torch.nn.Embedding(len(stoi), emb_dim)\n\n\nemb.weight\n\nParameter containing:\ntensor([[-0.7321, -0.0133],\n        [ 0.9890, -0.3715],\n        [-1.2565, -0.3346],\n        [ 0.3989,  1.4209],\n        [-0.5718, -0.1615],\n        [ 0.4853,  2.2499],\n        [ 0.1107,  2.7214],\n        [-1.7009,  1.1288],\n        [-0.6636, -1.1188],\n        [-1.6768,  1.0876],\n        [ 0.1645,  0.0063],\n        [-1.0367, -0.9603],\n        [-3.0939,  1.9831],\n        [-0.8332,  0.8572],\n        [-1.4305, -0.6878],\n        [-0.5197, -0.9626],\n        [-2.3395,  0.6205],\n        [-0.7045,  0.2387],\n        [ 2.8074, -0.8545],\n        [-0.2396, -0.7623],\n        [-1.1135, -0.1288],\n        [-0.5464, -0.0345],\n        [-1.2686,  1.0719],\n        [-1.3973, -0.5510],\n        [ 0.9127,  1.2566],\n        [ 0.6278,  1.1915],\n        [ 0.0465,  1.2633]], requires_grad=True)\n\n\n\n# Function to visualize the embedding in 2d space\n\ndef plot_emb(emb, itos, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    for i in range(len(itos)):\n        x, y = emb.weight[i].detach().cpu().numpy()\n        ax.scatter(x, y, color='k')\n        ax.text(x + 0.05, y + 0.05, itos[i])\n    return ax\n\nplot_emb(emb, itos)\n\n<AxesSubplot:>\n\n\n\n\n\n\nclass NextChar(nn.Module):\n  def __init__(self, block_size, vocab_size, emb_dim, hidden_size):\n    super().__init__()\n    self.emb = nn.Embedding(vocab_size, emb_dim)\n    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    x = self.emb(x)\n    x = x.view(x.shape[0], -1)\n    x = F.relu(self.lin1(x))\n    x = self.lin2(x)\n    return x\n    \n\n\n# Generate names from untrained model\n\n\nmodel = NextChar(block_size, len(stoi), emb_dim, 50).to(device)\nmodel = torch.compile(model)\n\ng = torch.Generator()\ng.manual_seed(4000002)\ndef generate_name(model, itos, stoi, block_size, max_len=10):\n    context = [0] * block_size\n    name = ''\n    for i in range(max_len):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        ch = itos[ix]\n        if ch == '.':\n            break\n        name += ch\n        context = context[1:] + [ix]\n    return name\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n\n\nihjzyewjrp\npuijjpnwfk\ngcvvflic\neeokqgmlse\nyiaghsikiv\nsmmggfnsot\nptdmdlmwfi\nyitpyixshw\ngjqapafodl\ndfuhgoowtb\n\n\n\nfor param_name, param in model.named_parameters():\n    print(param_name, param.shape)\n\nemb.weight torch.Size([27, 2])\nlin1.weight torch.Size([50, 8])\nlin1.bias torch.Size([50])\nlin2.weight torch.Size([27, 50])\nlin2.bias torch.Size([27])\n\n\n\n# Train the model\n\nloss_fn = nn.CrossEntropyLoss()\nopt = torch.optim.AdamW(model.parameters(), lr=0.01)\nimport time\n# Mini-batch training\nbatch_size = 4096*32\nprint_every = 400\nelapsed_time = []\nfor epoch in range(10000):\n    start_time = time.time()\n    for i in range(0, X.shape[0], batch_size):\n        x = X[i:i+batch_size]\n        y = Y[i:i+batch_size]\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    end_time = time.time()\n    elapsed_time.append(end_time - start_time)\n    if epoch % print_every == 0:\n        print(epoch, loss.item())\n        print(\"Time taken for 1 epochs: {} seconds\".format(sum(elapsed_time)/print_every))\n\n0 2.0890238285064697\nTime taken for 1 epochs: 6.577968597412109e-06 seconds\n400 2.0890519618988037\nTime taken for 1 epochs: 0.0018871128559112549 seconds\n800 2.088754177093506\nTime taken for 1 epochs: 0.0037400954961776733 seconds\n1200 2.0887293815612793\nTime taken for 1 epochs: 0.005593389868736267 seconds\n1600 2.0884861946105957\nTime taken for 1 epochs: 0.007446771860122681 seconds\n2000 2.0891528129577637\nTime taken for 1 epochs: 0.009300289154052734 seconds\n2400 2.0879929065704346\nTime taken for 1 epochs: 0.011153245568275452 seconds\n2800 2.088449239730835\nTime taken for 1 epochs: 0.013006348609924317 seconds\n3200 2.0879693031311035\nTime taken for 1 epochs: 0.014860434532165527 seconds\n3600 2.0881006717681885\nTime taken for 1 epochs: 0.016712880134582518 seconds\n4000 2.087575912475586\nTime taken for 1 epochs: 0.01856597661972046 seconds\n4400 2.087235689163208\nTime taken for 1 epochs: 0.020418978929519653 seconds\n4800 2.087622880935669\nTime taken for 1 epochs: 0.02227234423160553 seconds\n5200 2.087576150894165\nTime taken for 1 epochs: 0.02412549138069153 seconds\n5600 2.0879015922546387\nTime taken for 1 epochs: 0.025978831052780153 seconds\n6000 2.088137626647949\nTime taken for 1 epochs: 0.027831865549087523 seconds\n6400 2.0875606536865234\nTime taken for 1 epochs: 0.029685078263282774 seconds\n6800 2.0870373249053955\nTime taken for 1 epochs: 0.031538299322128295 seconds\n7200 2.0867090225219727\nTime taken for 1 epochs: 0.033391504883766174 seconds\n7600 2.085026979446411\nTime taken for 1 epochs: 0.035245080590248105 seconds\n8000 2.0854732990264893\nTime taken for 1 epochs: 0.03710031807422638 seconds\n8400 2.0843679904937744\nTime taken for 1 epochs: 0.03895375490188599 seconds\n8800 2.084249258041382\nTime taken for 1 epochs: 0.04080682039260864 seconds\n9200 2.0851705074310303\nTime taken for 1 epochs: 0.042660011649131774 seconds\n9600 2.0844883918762207\nTime taken for 1 epochs: 0.044512977004051206 seconds\n\n\n\n# Visualize the embedding\n\nplot_emb(model.emb, itos)\n\n<AxesSubplot:>\n\n\n\n\n\n\n# Generate names from trained model\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\nswina\nmiter\njakul\nposhaban\nnahmishsir\njaillanam\nfibwahir\ngurs\nsakinu\nramta\n\n\nTuning knobs\n\nEmbedding size\nMLP\nContext length"
  },
  {
    "objectID": "notebooks/posts/projection.html",
    "href": "notebooks/posts/projection.html",
    "title": "Linear Regression: Geometric Perspective",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Two three-dimensional vectors\nv1 = np.array([1, 1, 1])\nv2 = np.array([2, -2, 2])\n\n# y-vector\ny = np.array([2.5, -0.8, 1.2])\n\n\n# plot the vectors in 3D\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2')\nax.quiver(0, 0, 0, y[0], y[1], y[2], color='g', label='y')\n\nax.set_xlim(0, 3)\nax.set_ylim(0, 4)\nax.set_zlim(0, 3)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.legend()\n\nax.view_init(elev=45, azim=60)\n\n\n\n\n\ntheta = np.linalg.lstsq(np.array([v1, v2]).T, y, rcond=None)[0]\ntheta\n\narray([0.525 , 0.6625])\n\n\n\n# Projection of y onto the plane spanned by v1 and v2\ny_proj = np.dot(np.array([v1, v2]).T, theta)\ny_proj\n\narray([ 1.85, -0.8 ,  1.85])\n\n\n\n# Plot the x=z plane filled with color black\nfig, ax = plt.subplots(figsize=(8, 8))\n# 3d projection\nax = fig.add_subplot(111, projection='3d')\nxx, zz = np.meshgrid(np.linspace(-1, 4, 100), np.linspace(-1, 4, 100))\nyy = np.zeros_like(xx)\nax.plot_surface(xx, yy, zz, alpha=0.2, color='k')\n\n\n# plot the vectors in 3D\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2')\nax.quiver(0, 0, 0, y[0], y[1], y[2], color='g', label='y')\n\n\n# Limit the view to the x-z plane\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\n# Set the view angle so that we can see the x-z plane appear at a 45 degree angle\n#ax.view_init(azim=70, elev=200)\nax.view_init(elev=60, azim=-80, roll=90)\nax.view_init(120, -120, -120)\n#ax.view_init(roll=45)\n#ax.view_init(elev=30, azim=45, roll=15)\nax.set_ylim(-4, 4)\nax.set_xlim(0, 4)\nax.set_zlim(0, 4)\n\n\n# Plot the projection of y onto the plane spanned by v1 and v2\nax.quiver(0, 0, 0, y_proj[0], y_proj[1], y_proj[2], color='k', label='Projection of y onto\\n the plane spanned by v1 and v2')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x111d52730>"
  },
  {
    "objectID": "notebooks/posts/autodiff-helper.html",
    "href": "notebooks/posts/autodiff-helper.html",
    "title": "Autodiff",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\ntheta_0 = torch.tensor(1.0, requires_grad=True)\ntheta_1 = torch.tensor(1.0, requires_grad=True)\ntheta_2 = torch.tensor(2.0, requires_grad=True)\n\nx1 = torch.tensor(1.0)\nx2 = torch.tensor(2.0)\n\nf1 = theta_1*x1\nf2 = theta_2*x2\n\nf3 = f1 + f2\n\nf4 = f3 + theta_0\n\nf5 = f4*-1\n\nf6 = torch.exp(f5)\n\nf7 = 1 + f6\n\nf8 = 1/f7\n\nf9 = torch.log(f8)\n\nL = f9*-1\n\nall_nodes = {\"theta_0\": theta_0, \"theta_1\": theta_1, \"theta_2\": theta_2,  \n             \"f1\": f1, \"f2\": f2, \"f3\": f3, \"f4\": f4, \"f5\": f5, \"f6\": f6, \"f7\": f7, \"f8\": f8, \"f9\": f9, \"L\": L}\n\n# Retain grad for all nodes\nfor node in all_nodes.values():\n    node.retain_grad()\n\n\n# Print out the function evaluation for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.item()}\")\n\ntheta_0: 1.0\ntheta_1: 1.0\ntheta_2: 2.0\nf1: 1.0\nf2: 4.0\nf3: 5.0\nf4: 6.0\nf5: -6.0\nf6: 0.0024787522852420807\nf7: 1.0024787187576294\nf8: 0.9975274205207825\nf9: -0.0024756414350122213\nL: 0.0024756414350122213\n\n\n\nL.backward()\n\n# Print out the gradient for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.grad.item()}\")\n\ntheta_0: -0.00247262348420918\ntheta_1: -0.00247262348420918\ntheta_2: -0.00494524696841836\nf1: -0.00247262348420918\nf2: -0.00247262348420918\nf3: -0.00247262348420918\nf4: -0.00247262348420918\nf5: 0.00247262348420918\nf6: 0.9975274801254272\nf7: 0.9975274801254272\nf8: -1.0024787187576294\nf9: -1.0\nL: 1.0\n\n\n\n(-1/(f7**2))*-1.00247\n\ntensor(0.9975, grad_fn=<MulBackward0>)\n\n\n\ntorch.exp(f5)*0.9975\n\ntensor(0.0025, grad_fn=<MulBackward0>)"
  },
  {
    "objectID": "notebooks/posts/mle.html",
    "href": "notebooks/posts/mle.html",
    "title": "ES654",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndata = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n\n\nn_h = np.sum(data)\nn_t = len(data) - n_h\n    \n\n\nn_h, n_t\n\n(8, 2)\n\n\n\ndef likelihood(theta):\n    return theta**n_h * (1-theta)**n_t\n\n\ndef log_likelihood(theta):\n    return n_h * np.log(theta) + n_t * np.log(1-theta)\n\n\nlikelihood(0.1)\n\n8.100000000000005e-09\n\n\n\nlikelihood(0.9)\n\n0.004304672099999999\n\n\n\nlikelihood(0.8)\n\n0.0067108864\n\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\nx_lin = np.linspace(0.01, 0.99, 100)\ny_lin = likelihood(x_lin)\ny_lin_ll = log_likelihood(x_lin)\nax[0].plot(x_lin, y_lin)\nax[1].plot(x_lin, y_lin_ll)\n\n\n\n\n\nimport torch\n\ntheta = 0.2\nbn = torch.distributions.Bernoulli(probs= theta)\n\ndata = []\nfor i in range(100):\n    data.append(bn.sample())\n\ntensor(59.)"
  },
  {
    "objectID": "notebooks/posts/nn.html",
    "href": "notebooks/posts/nn.html",
    "title": "Neural Network",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# MNIST dataset\n\nfrom torchvision import datasets, transforms\nimport torchvision\n\n# Split MNIST into train, validation, and test sets\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n\n# Split train_data into train and validation sets\nval_data = torch.utils.data.Subset(train_data, range(50000, 51000))\n\n# Reduce the size of the training set to 5,000\ntrain_data = torch.utils.data.Subset(train_data, range(0, 5000))\n\n\n# Create data loaders\nbatch_size = 64\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n\nimg, target = next(iter(train_loader))\nprint(img.shape)\nprint(target.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\n\nplt.imshow(img[0].numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\ntarget\n\ntensor([3, 4, 1, 8, 9, 3, 9, 8, 4, 8, 3, 0, 0, 7, 7, 1, 6, 6, 9, 7, 4, 3, 3, 4,\n        5, 7, 3, 2, 8, 4, 8, 2, 8, 3, 1, 4, 2, 1, 4, 8, 5, 3, 5, 1, 8, 7, 3, 7,\n        7, 2, 0, 3, 1, 3, 7, 0, 7, 1, 7, 6, 4, 1, 8, 0])\n\n\n\n# Store the labels and Images in TensorBoard\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Clean up any old runs\n!rm -rf runs\n\n# Default `log_dir` is \"runs\"\nwriter = SummaryWriter('runs/mnist')\n\n# Add images to tensorboard in the form of a grid in batches of 64\ndataiter = iter(DataLoader(train_data, batch_size=64, shuffle=True))\n\n# Add a slider in tensorboard to iterate through the batches\nfor i in range(10):\n    images, labels = next(dataiter)\n    images = torchvision.utils.make_grid(images)\n    # Add images with labels to tensorboard\n    writer.add_image(f'mnist_images_{i}', images, global_step=i)\n\n\n# Define model for 10-class MNIST classification\n\nclass MNISTClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        z1 = self.fc1(x)\n        a1 = F.relu(z1)\n        z2 = self.fc2(a1)\n        a2 = F.relu(z2)\n        z3 = self.fc3(a2) # logits\n        return z3\n\n\nmodel = MNISTClassifier()\n\n\n# Forward pass through the model\n\n\n\nsaved_img, saved_target = next(iter(train_loader))\nsaved_img = saved_img.view(saved_img.shape[0], -1)\nprint(saved_img.shape)\n\nplt.imshow(saved_img[0].reshape(28, 28), cmap='gray_r')\nplt.title(saved_target[0].item())\n\n\nwith torch.no_grad():\n    out = model(saved_img.view(saved_img.shape[0], -1))\n\n\n\nprint(out[0])\n\n# Softmax\nprobas = F.softmax(out, dim=1)\nprint(probas[0])\n\ntorch.Size([64, 784])\ntensor([-0.1859, -0.1648,  0.0156,  0.0309,  0.1785, -0.1567, -0.0472,  0.1143,\n        -0.0400, -0.0006])\ntensor([0.0846, 0.0864, 0.1035, 0.1051, 0.1218, 0.0871, 0.0972, 0.1143, 0.0979,\n        0.1019])\n\n\n\n\n\n\nprobas[0:1]\n\ntensor([[0.0846, 0.0864, 0.1035, 0.1051, 0.1218, 0.0871, 0.0972, 0.1143, 0.0979,\n         0.1019]])\n\n\n\n# Predicted label before training\n\npred = torch.argmax(probas[0:1], dim=1).item()\nprint(f\"Prediction:  {pred}, True label: {saved_target[0].item()}\")\n\nPrediction:  4, True label: 2\n\n\n\nmodel.fc1.weight.shape, model.fc1.bias.shape\n\n(torch.Size([64, 784]), torch.Size([64]))\n\n\n\n# Number of parameters in fc1\nprint(\"fc1\", model.fc1.weight.numel() + model.fc1.bias.numel())\n\n# Number of parameters in fc2\nprint(\"fc2\", model.fc2.weight.numel() + model.fc2.bias.numel())\n\n# Number of parameters in fc3\nprint(\"fc3\", model.fc3.weight.numel() + model.fc3.bias.numel())\n\nfc1 50240\nfc2 2080\nfc3 330\n\n\n\n# Get total number of parameters\nprint(\"Total number of parameters:\", sum(p.numel() for p in model.parameters()))\n\nTotal number of parameters: 52650\n\n\n\nlen(train_loader)\n\n79\n\n\n\n# Forward pass through the model and writing to tensorboard\n\nlr = 0.001\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    for batch_idx, (data_test, target) in enumerate(train_loader):\n        # Reshape data to input to the network\n        data_test = data_test.view(data_test.shape[0], -1)\n        # Forward pass\n        output = model(data_test)\n        loss = criterion(output, target)\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iteration_num = epoch * len(train_loader) + batch_idx\n        \n        # Write to tensorboard\n        writer.add_scalar('Loss/train', loss.item(), iteration_num)\n        writer.add_scalar('Accuracy/train', (output.argmax(dim=1) == target).float().mean(), iteration_num)\n\n        # Find test loss and accuracy\n        runing_loss = 0.0\n        running_acc = 0.0\n        with torch.no_grad():\n            for data_test, target_test in val_loader:\n                data_test = data_test.view(data_test.shape[0], -1)\n                output_test = model(data_test)\n                loss_test = criterion(output_test, target_test)\n                runing_loss += loss_test.item()\n                running_acc += (output_test.argmax(dim=1) == target_test).float().mean()\n        writer.add_scalar('Loss/validation', runing_loss / len(val_loader), iteration_num)\n        writer.add_scalar('Accuracy/validation', running_acc / len(val_loader), iteration_num)\n\n        if batch_idx % 100 == 0:\n            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\nEpoch [1/1], Step [0/79], Loss: 2.3027\n\n\n\n# Prediction on the saved image\n\nwith torch.no_grad():\n    out = model(saved_img[0:1])\n    probas = F.softmax(out, dim=1)\n    pred = torch.argmax(probas, dim=1).item()\n    print(pred, saved_target[0].item())\n\n1 1\n\n\n\n# Create a HParam dictionary for batch size and learning rate for tensorboard\n\nb_sizes = [32, 64, 512]\nlrs = [0.001, 0.01, 0.1]\n\n\nnum_epochs  = 1\nfor epoch in range(num_epochs):\n    for b_size in b_sizes:\n        train_loader = DataLoader(train_data, batch_size=b_size, shuffle=True)\n        model = MNISTClassifier()\n        for lr in lrs:\n            print(f\"Batch size: {b_size}, Learning rate: {lr}\")\n            optimizer = optim.Adam(model.parameters(), lr=lr)\n            criterion = nn.CrossEntropyLoss()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                # Reshape data to input to the network\n                data = data.view(data.shape[0], -1)\n                # Forward pass\n                output = model(data)\n                loss = criterion(output, target)\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n\n                # Write loss as scalar to Tensorboard and hyperparameters to HParams\n                writer.add_scalar('training loss',\n                                loss.item(),\n                                epoch * len(train_loader) + batch_idx)\n                writer.add_hparams({'lr': lr, 'bsize': b_size},\n                                { 'hparam/loss': loss.item()})\n                \n\n    \n\nBatch size: 32, Learning rate: 0.001\nBatch size: 32, Learning rate: 0.01\nBatch size: 32, Learning rate: 0.1\nBatch size: 64, Learning rate: 0.001\nBatch size: 64, Learning rate: 0.01\nBatch size: 64, Learning rate: 0.1\nBatch size: 512, Learning rate: 0.001\nBatch size: 512, Learning rate: 0.01\nBatch size: 512, Learning rate: 0.1"
  },
  {
    "objectID": "notebooks/posts/dt-reg.html",
    "href": "notebooks/posts/dt-reg.html",
    "title": "DT Regression",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"mins-played.csv\")\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Day\n      Outlook\n      Temp\n      Humidity\n      Wind\n      Minutes Played\n    \n  \n  \n    \n      0\n      D1\n      Sunny\n      Hot\n      High\n      Weak\n      20\n    \n    \n      1\n      D2\n      Sunny\n      Hot\n      High\n      Strong\n      24\n    \n    \n      2\n      D3\n      Overcast\n      Hot\n      High\n      Weak\n      40\n    \n    \n      3\n      D4\n      Rain\n      Mild\n      High\n      Weak\n      50\n    \n    \n      4\n      D5\n      Rain\n      Cool\n      Normal\n      Weak\n      60\n    \n    \n      5\n      D6\n      Rain\n      Cool\n      Normal\n      Strong\n      10\n    \n    \n      6\n      D7\n      Overcast\n      Cool\n      Normal\n      Strong\n      4\n    \n    \n      7\n      D8\n      Sunny\n      Mild\n      High\n      Weak\n      10\n    \n    \n      8\n      D9\n      Sunny\n      Cool\n      Normal\n      Weak\n      60\n    \n    \n      9\n      D10\n      Rain\n      Mild\n      Normal\n      Weak\n      40\n    \n    \n      10\n      D11\n      Sunny\n      Mild\n      High\n      Strong\n      45\n    \n    \n      11\n      D12\n      Overcast\n      Mild\n      High\n      Strong\n      40\n    \n    \n      12\n      D13\n      Overcast\n      Hot\n      Normal\n      Weak\n      35\n    \n    \n      13\n      D14\n      Rain\n      Mild\n      High\n      Strong\n      20\n    \n  \n\n\n\n\n\ndf[\"Minutes Played\"].std()\n\n18.3111087402348\n\n\n\nimport numpy as np\n# np.std(df[\"Minutes Played\"].values)\n\n\ndf.query(\"Wind=='Weak'\")[\"Minutes Played\"].std()*len(df.query(\"Wind=='Weak'\"))/len(df)\n\n10.180585192846463\n\n\n\ndf.query(\"Wind=='Strong'\")[\"Minutes Played\"].std()*len(df.query(\"Wind=='Strong'\"))/len(df)\n\n6.933944897151599\n\n\n\nout = {}\nfor temp in df[\"Temp\"].unique():\n    print(temp)\n    out[temp] = df.query(\"Temp==@temp\")[\"Minutes Played\"].std()*len(df.query(\"Temp==@temp\"))/len(df)\n    print(out[temp])\n    print()\n\nHot\n2.6636888135137133\n\nMild\n6.696785704762413\n\nCool\n8.770699519880226\n\n\n\n\ndf[\"Minutes Played\"].std() - pd.Series(out).sum()\n\n0.17993470207844808"
  },
  {
    "objectID": "notebooks/posts/gd.html",
    "href": "notebooks/posts/gd.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "import jax.numpy as jnp\nfrom jax import random, jit, vmap, grad, jacfwd, jacrev, hessian, value_and_grad\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Simple 2D quadratic function\ndef f(theta_0, theta_1):\n    return theta_0**2 + theta_1**2\n\n\n# Plot surface and contour plots for f using jax.vmap\ndef create_plot(f):\n    theta_0 = jnp.linspace(-2, 2, 100)\n    theta_1 = jnp.linspace(-2, 2, 100)\n    theta_0, theta_1 = jnp.meshgrid(theta_0, theta_1)\n    f_vmap = jnp.vectorize(f, signature='(),()->()')\n    f_vals = f_vmap(theta_0, theta_1)\n\n    # Create a figure with 2 subplots (3d surface and 2d contour)\n    fig = plt.figure(figsize=(12, 4))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122)\n\n    # Plot surface and contour plots\n    temp = ax1.plot_surface(theta_0, theta_1, f_vals, cmap='viridis')\n\n    # Filled contour plot and marked level set values using clabel\n    # Set 20 levels between min and max of f_vals\n    levels = jnp.linspace(0.5, int(jnp.max(f_vals))+0.5, 11)\n\n    contours = ax2.contour(theta_0, theta_1, f_vals, levels=levels, cmap='viridis')\n    ax2.clabel(contours, inline=True, fontsize=8)\n\n    # Fill using imshow\n    ax2.imshow(f_vals, extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=0.5)\n  \n    # Find the global minimum of f using jax.scipy.optimize.minimize\n    from jax.scipy.optimize import minimize\n    def f_min(theta):\n        return f(theta[0], theta[1])\n    res = minimize(f_min, jnp.array([0., 0.]), method='BFGS')\n    theta_min = res.x\n    f_min = res.fun\n    print(f'Global minimum: {f_min} at {theta_min}')\n    # Plot the global minimum\n    ax2.scatter(theta_min[0], theta_min[1], marker='x', color='red', s=100)\n\n    \n\n    ax2.set_aspect('equal')\n\n    # Add labels\n    ax1.set_xlabel(r'$\\theta_0$')\n    ax1.set_ylabel(r'$\\theta_1$')\n    ax1.set_zlabel(r'$f(\\theta_0, \\theta_1)$')\n    ax2.set_xlabel(r'$\\theta_0$')\n    ax2.set_ylabel(r'$\\theta_1$')\n\n    # Add colorbar\n    fig.colorbar(temp, ax=ax1, shrink=0.5, aspect=5)\n\n    # Tight layout\n    plt.tight_layout()\n\n\ncreate_plot(f)\n\nGlobal minimum: 0.0 at [0. 0.]\n\n\n\n\n\n\n# Gradient of f at a given point\ndef grad_f(theta_0, theta_1):\n    return grad(f, argnums=(0, 1))(theta_0, theta_1)\n\n\ngrad_f(2., 1.)\n\n(Array(4., dtype=float32, weak_type=True),\n Array(2., dtype=float32, weak_type=True))\n\n\n\ntheta = jnp.array([2., 1.])\ntheta\n\nArray([2., 1.], dtype=float32)\n\n\n\nf(*theta)\n\nArray(5., dtype=float32)\n\n\n\njnp.array(grad_f(*theta))\n\nArray([4., 2.], dtype=float32)\n\n\n\nlr = 0.1\ntheta = theta- lr * jnp.array(grad_f(*theta))\ntheta\n\nArray([1.6, 0.8], dtype=float32)\n\n\n\nf(*theta)\n\nArray(3.2000003, dtype=float32)\n\n\n\n# Gradient descent loop\n\n# Initial parameters\ntheta = jnp.array([2., 1.])\n\n# Store parameters and function values for plotting\ntheta_vals = [theta]\nf_vals = [f(*theta)]\n\nfor i in range(10):\n    theta = theta - lr * jnp.array(grad_f(*theta))\n    theta_vals.append(theta)\n    f_vals.append(f(*theta))\n    print(f'Iteration {i}: theta = {theta}, f = {f(*theta)}')\n\ntheta_vals = jnp.array(theta_vals)\nf_vals = jnp.array(f_vals)\n\nIteration 0: theta = [1.6 0.8], f = 3.200000286102295\nIteration 1: theta = [1.28 0.64], f = 2.047999858856201\nIteration 2: theta = [1.0239999  0.51199996], f = 1.3107198476791382\nIteration 3: theta = [0.8191999  0.40959996], f = 0.8388606309890747\nIteration 4: theta = [0.6553599  0.32767996], f = 0.5368707776069641\nIteration 5: theta = [0.52428794 0.26214397], f = 0.34359729290008545\nIteration 6: theta = [0.41943035 0.20971517], f = 0.21990226209163666\nIteration 7: theta = [0.3355443  0.16777214], f = 0.14073745906352997\nIteration 8: theta = [0.26843542 0.13421771], f = 0.09007196873426437\nIteration 9: theta = [0.21474834 0.10737417], f = 0.05764605849981308\n\n\n\n# Plot the cost vs iterations\nplt.plot(f_vals)\n\n\n\n\n\n# Simple dataset for linear regression\n\nX = jnp.array([[1.], [2.], [3.]])\ny = jnp.array([1., 2.2, 2.8])\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlr.coef_, lr.intercept_\n\n(array([0.9000001], dtype=float32), 0.19999981)\n\n\n\n# Cost function for linear regression using jax.vmap\ndef cost(theta_0, theta_1):\n    y_hat = (theta_0 + theta_1 * X).flatten()\n    #print(y_hat, y, y-y_hat, (y-y_hat)**2)\n    return jnp.mean((y_hat- y)**2)\n    \n# Plot surface and contour plots for cost function\n#create_plot(cost)\n\n\ncost(2.0, 2.0)\n\nArray(16.826666, dtype=float32)\n\n\n\n(3**2 + 3.8**2 + 5.2**2)/3.\n\n16.826666666666668\n\n\n\n# Gradient of cost function at a given point\ndef grad_cost(theta_0, theta_1):\n    return jnp.array(grad(cost, argnums=(0, 1))(theta_0, theta_1))\n\ngrad_cost(2.0, 2.0)\n\nArray([ 8.      , 17.466667], dtype=float32)\n\n\n\ndef grad_cost_manual(theta_0, theta_1):\n    y_hat = (theta_0 + theta_1 * X).flatten()\n    return jnp.array([2*jnp.mean(y_hat - y), 2*jnp.mean((y_hat - y) * X.flatten())])\n\n\ngrad_cost_manual(2.0, 2.0)\n\nArray([ 8.      , 17.466667], dtype=float32)\n\n\n\n# Plotting cost surface and contours for three points in X individually\n\ndef cost_i(theta_0, theta_1, i = 1):\n    y_hat = theta_0 + theta_1 * X[i-1:i]\n    return jnp.mean((y_hat- y[i-1:i])**2)\n\n\n(cost_i(2.0, 2.0, 1) + cost_i(2.0, 2.0, 2) + cost_i(2.0, 2.0, 3))/3.0\n\nArray(16.826666, dtype=float32)\n\n\n\nfrom functools import partial\n\n\n# Plot surface and contour plots for cost function\nfor i in range(1, 4):\n    cost_i_p = partial(cost_i, i=i)\n    create_plot(cost_i_p)\n\nGlobal minimum: 0.0 at [0.5 0.5]\nGlobal minimum: 0.0 at [0.44000003 0.88000005]\nGlobal minimum: 0.0 at [0.28000003 0.84      ]\n\n\n\n\n\n\n\n\n\n\n\n\ngrad_cost_1 = grad(cost_i, argnums=(0, 1))\ngrad_cost_1(2.0, 2.0)\n\n(Array(6., dtype=float32, weak_type=True),\n Array(6., dtype=float32, weak_type=True))\n\n\n\njnp.array(grad_cost_1(2.0, 2.0, 1)), jnp.array(grad_cost_1(2.0, 2.0, 2)), jnp.array(grad_cost_1(2.0, 2.0, 3))\n\n(Array([6., 6.], dtype=float32),\n Array([ 7.6, 15.2], dtype=float32),\n Array([10.4     , 31.199999], dtype=float32))"
  },
  {
    "objectID": "notebooks/posts/tips.html",
    "href": "notebooks/posts/tips.html",
    "title": "Misc tips",
    "section": "",
    "text": "Miscelleneous tips\n\nTab Complete\na.b?\nBlack format\nrich\n\ntable\ninspect\n\ndir()\ntiming\ntiming with sort?!\ntimeit object\nplotting timing\narray .tolist()\nDataclasses\nAnnotation\nPlotting with pandas (sin, log, question …)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nn = 100\nk = 20\nm = 50\n\nnp.random.seed(0)\na = np.random.randn(n, k)\nb = np.random.randn(k, m)\na.shape, b.shape\n\n((100, 20), (20, 50))\n\n\n\na_list_of_list = a.tolist()\na_list_of_list[0]\n\nlist\n\n\n\n\"list\" in dir(a)[:]\n\nFalse\n\n\n\nimport rich\nrich.inspect(a.tolist, methods=True, all=True)\n\n╭──────────────────────── <built-in method tolist of numpy.ndarray object at 0x164cbb8d0> ────────────────────────╮\n│ def ndarray.tolist(...)                                                                                         │\n│                                                                                                                 │\n│ a.tolist()                                                                                                      │\n│                                                                                                                 │\n│            __doc__ = \"a.tolist()\\n\\n    Return the array as an ``a.ndim``-levels deep nested list of Python     │\n│                      scalars.\\n\\n    Return a copy of the array data as a (nested) Python list.\\n    Data items │\n│                      are converted to the nearest compatible builtin Python type, via\\n    the                  │\n│                      `~numpy.ndarray.item` function.\\n\\n    If ``a.ndim`` is 0, then since the depth of the     │\n│                      nested list is 0, it will\\n    not be a list at all, but a simple Python scalar.\\n\\n       │\n│                      Parameters\\n    ----------\\n    none\\n\\n    Returns\\n    -------\\n    y : object, or list  │\n│                      of object, or list of list of object, or ...\\n        The possibly nested list of array    │\n│                      elements.\\n\\n    Notes\\n    -----\\n    The array may be recreated via ``a =                │\n│                      np.array(a.tolist())``, although this\\n    may sometimes lose precision.\\n\\n    Examples\\n │\n│                      --------\\n    For a 1D array, ``a.tolist()`` is almost the same as ``list(a)``,\\n          │\n│                      except that ``tolist`` changes numpy scalars to Python scalars:\\n\\n    >>> a =             │\n│                      np.uint32([1, 2])\\n    >>> a_list = list(a)\\n    >>> a_list\\n    [1, 2]\\n    >>>           │\n│                      type(a_list[0])\\n    <class 'numpy.uint32'>\\n    >>> a_tolist = a.tolist()\\n    >>>        │\n│                      a_tolist\\n    [1, 2]\\n    >>> type(a_tolist[0])\\n    <class 'int'>\\n\\n    Additionally,    │\n│                      for a 2D array, ``tolist`` applies recursively:\\n\\n    >>> a = np.array([[1, 2], [3,       │\n│                      4]])\\n    >>> list(a)\\n    [array([1, 2]), array([3, 4])]\\n    >>> a.tolist()\\n    [[1,    │\n│                      2], [3, 4]]\\n\\n    The base case for this recursion is a 0D array:\\n\\n    >>> a =          │\n│                      np.array(1)\\n    >>> list(a)\\n    Traceback (most recent call last):\\n      ...\\n          │\n│                      TypeError: iteration over a 0-d array\\n    >>> a.tolist()\\n    1\"                          │\n│         __module__ = None                                                                                       │\n│           __name__ = 'tolist'                                                                                   │\n│       __qualname__ = 'ndarray.tolist'                                                                           │\n│           __self__ = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,                            │\n│                              -2.05158264e-01,  3.13067702e-01, -8.54095739e-01],                                │\n│                             [-2.55298982e+00,  6.53618595e-01,  8.64436199e-01, ...,                            │\n│                               1.20237985e+00, -3.87326817e-01, -3.02302751e-01],                                │\n│                             [-1.04855297e+00, -1.42001794e+00, -1.70627019e+00, ...,                            │\n│                               3.02471898e-01, -6.34322094e-01, -3.62741166e-01],                                │\n│                             ...,                                                                                │\n│                             [ 8.73311836e-01,  1.19973618e+00,  4.56153036e-01, ...,                            │\n│                              -1.24021634e+00,  9.00054243e-01,  1.80224223e+00],                                │\n│                             [-2.08285103e-01,  1.57437124e+00,  1.98989494e-01, ...,                            │\n│                               4.32837621e-01, -8.08717532e-01, -1.10412399e+00],                                │\n│                             [-7.89102180e-01,  1.24845579e-03, -1.59939788e-01, ...,                            │\n│                               1.58433847e-01, -1.14190142e+00, -1.31097037e+00]])                               │\n│ __text_signature__ = None                                                                                       │\n│           __call__ = def __call__(*args, **kwargs): Call self as a function.                                    │\n│          __class__ = class __class__():                                                                         │\n│        __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                                   │\n│            __dir__ = def __dir__(): Default dir() implementation.                                               │\n│             __eq__ = def __eq__(value, /): Return self==value.                                                  │\n│         __format__ = def __format__(format_spec, /): Default object formatter.                                  │\n│             __ge__ = def __ge__(value, /): Return self>=value.                                                  │\n│   __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                                 │\n│             __gt__ = def __gt__(value, /): Return self>value.                                                   │\n│           __hash__ = def __hash__(): Return hash(self).                                                         │\n│           __init__ = def __init__(*args, **kwargs): Initialize self.  See help(type(self)) for accurate         │\n│                      signature.                                                                                 │\n│  __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.               │\n│             __le__ = def __le__(value, /): Return self<=value.                                                  │\n│             __lt__ = def __lt__(value, /): Return self<value.                                                   │\n│             __ne__ = def __ne__(value, /): Return self!=value.                                                  │\n│            __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for accurate │\n│                      signature.                                                                                 │\n│         __reduce__ = def __reduce__(...) Helper for pickle.                                                     │\n│      __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                         │\n│           __repr__ = def __repr__(): Return repr(self).                                                         │\n│        __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).                     │\n│         __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                                      │\n│            __str__ = def __str__(): Return str(self).                                                           │\n│   __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize issubclass().    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nlambda x: np.power(x, 3)\n\n\ndef f(x):\n    return np.power(x, 3)\n\n\nd = {\n    r\"$\\sin(x)$\": np.sin,\n    r\"$\\log(x)$\": np.log,\n    r\"$\\frac{sin(x)}{x}$\": lambda x: np.sin(x) / x,\n    r\"$x^3$\": lambda x: np.power(x, 3),\n}\n\n\nd\n\n{'$\\\\sin(x)$': <ufunc 'sin'>,\n '$\\\\log(x)$': <ufunc 'log'>,\n '$\\\\frac{sin(x)}{x}$': <function __main__.<lambda>(x)>,\n '$x^3$': <function __main__.<lambda>(x)>}\n\n\n\nx = np.arange(0.005, 10.0, 0.005)\n\n\ne = {k: v(x) for k, v in d.items()}\n\n\ne\n\n{'$\\\\sin(x)$': array([ 0.00499998,  0.00999983,  0.01499944, ..., -0.53137431,\n        -0.53560333, -0.53981897]),\n '$\\\\log(x)$': array([-5.29831737, -4.60517019, -4.19970508, ...,  2.30108397,\n         2.30158459,  2.30208497]),\n '$\\\\frac{sin(x)}{x}$': array([ 0.99999583,  0.99998333,  0.9999625 , ..., -0.05321726,\n        -0.05361395, -0.0540089 ]),\n '$x^3$': array([1.25000000e-07, 1.00000000e-06, 3.37500000e-06, ...,\n        9.95506747e+02, 9.97002999e+02, 9.98500750e+02])}\n\n\n\ndf = pd.DataFrame(e, index=x)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      $\\sin(x)$\n      $\\log(x)$\n      $\\frac{sin(x)}{x}$\n      $x^3$\n    \n  \n  \n    \n      0.005\n      0.005000\n      -5.298317\n      0.999996\n      1.250000e-07\n    \n    \n      0.010\n      0.010000\n      -4.605170\n      0.999983\n      1.000000e-06\n    \n    \n      0.015\n      0.014999\n      -4.199705\n      0.999963\n      3.375000e-06\n    \n    \n      0.020\n      0.019999\n      -3.912023\n      0.999933\n      8.000000e-06\n    \n    \n      0.025\n      0.024997\n      -3.688879\n      0.999896\n      1.562500e-05\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9.975\n      -0.522877\n      2.300082\n      -0.052419\n      9.925187e+02\n    \n    \n      9.980\n      -0.527132\n      2.300583\n      -0.052819\n      9.940120e+02\n    \n    \n      9.985\n      -0.531374\n      2.301084\n      -0.053217\n      9.955067e+02\n    \n    \n      9.990\n      -0.535603\n      2.301585\n      -0.053614\n      9.970030e+02\n    \n    \n      9.995\n      -0.539819\n      2.302085\n      -0.054009\n      9.985007e+02\n    \n  \n\n1999 rows × 4 columns\n\n\n\n\ndf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.plot(subplots=True)\n\narray([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>],\n      dtype=object)\n\n\n\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      $\\sin$\n      log\n      $\\frac{sin(x)}{x}$\n      x^3\n    \n  \n  \n    \n      0.005\n      0.005000\n      -5.298317\n      0.999996\n      1.250000e-07\n    \n    \n      0.010\n      0.010000\n      -4.605170\n      0.999983\n      1.000000e-06\n    \n    \n      0.015\n      0.014999\n      -4.199705\n      0.999963\n      3.375000e-06\n    \n    \n      0.020\n      0.019999\n      -3.912023\n      0.999933\n      8.000000e-06\n    \n    \n      0.025\n      0.024997\n      -3.688879\n      0.999896\n      1.562500e-05\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9.975\n      -0.522877\n      2.300082\n      -0.052419\n      9.925187e+02\n    \n    \n      9.980\n      -0.527132\n      2.300583\n      -0.052819\n      9.940120e+02\n    \n    \n      9.985\n      -0.531374\n      2.301084\n      -0.053217\n      9.955067e+02\n    \n    \n      9.990\n      -0.535603\n      2.301585\n      -0.053614\n      9.970030e+02\n    \n    \n      9.995\n      -0.539819\n      2.302085\n      -0.054009\n      9.985007e+02\n    \n  \n\n1999 rows × 4 columns\n\n\n\n\n((df - df.min(axis=0)) / (df.max(axis=0) - df.min(axis=0))).plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.randn(1000, 5),\n                  columns=\"a,b,c,d,e\".split(\",\"))\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n      e\n    \n  \n  \n    \n      0\n      1.593274\n      0.568722\n      -0.114487\n      0.251630\n      -1.210856\n    \n    \n      1\n      -0.393734\n      0.085253\n      0.099422\n      -1.530616\n      0.327623\n    \n    \n      2\n      0.279196\n      -0.377051\n      0.004175\n      -1.483492\n      -1.479796\n    \n    \n      3\n      0.134687\n      -0.667723\n      -0.011556\n      0.839491\n      -0.173930\n    \n    \n      4\n      -2.810668\n      -0.150654\n      -0.481044\n      -0.234694\n      0.899731\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.837658\n      1.315288\n      -0.364523\n      1.993571\n      1.584878\n    \n    \n      996\n      -2.104663\n      -2.553118\n      -1.242666\n      0.201987\n      -0.305332\n    \n    \n      997\n      -1.195587\n      -1.577903\n      0.849912\n      0.327590\n      -0.001670\n    \n    \n      998\n      -0.035563\n      -0.489252\n      1.930498\n      -0.262645\n      0.825932\n    \n    \n      999\n      -0.643267\n      -0.828981\n      -0.202735\n      -0.257866\n      0.070815\n    \n  \n\n1000 rows × 5 columns\n\n\n\n\ndf = df.assign(f=df.sum(axis=1))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n      e\n      f\n    \n  \n  \n    \n      0\n      1.593274\n      0.568722\n      -0.114487\n      0.251630\n      -1.210856\n      1.088284\n    \n    \n      1\n      -0.393734\n      0.085253\n      0.099422\n      -1.530616\n      0.327623\n      -1.412052\n    \n    \n      2\n      0.279196\n      -0.377051\n      0.004175\n      -1.483492\n      -1.479796\n      -3.056967\n    \n    \n      3\n      0.134687\n      -0.667723\n      -0.011556\n      0.839491\n      -0.173930\n      0.120969\n    \n    \n      4\n      -2.810668\n      -0.150654\n      -0.481044\n      -0.234694\n      0.899731\n      -2.777329\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.837658\n      1.315288\n      -0.364523\n      1.993571\n      1.584878\n      3.691555\n    \n    \n      996\n      -2.104663\n      -2.553118\n      -1.242666\n      0.201987\n      -0.305332\n      -6.003792\n    \n    \n      997\n      -1.195587\n      -1.577903\n      0.849912\n      0.327590\n      -0.001670\n      -1.597657\n    \n    \n      998\n      -0.035563\n      -0.489252\n      1.930498\n      -0.262645\n      0.825932\n      1.968971\n    \n    \n      999\n      -0.643267\n      -0.828981\n      -0.202735\n      -0.257866\n      0.070815\n      -1.862035\n    \n  \n\n1000 rows × 6 columns\n\n\n\n\ndf.query(\"f > 5\")[\"f\"].std()\n\n0.6609257763922614\n\n\n\ndf.query(\"f <= 5\")[\"f\"].std()\n\n2.1891090850524444\n\n\n\n\"tolist\" in dir(df)\n\nFalse\n\n\n\ndef search(obj, query):\n    import re\n\n    return list(filter(lambda x: re.search(query, x), dir(obj)))\n\n\nsearch(a, \"lis\")\n\n['tolist']\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndef search(obj, query):\n    import re\n    from rich import inspect\n    from rich.pretty import Pretty\n    from rich.panel import Panel\n\n    z = list(filter(lambda x: re.search(query, x), dir(obj)))\n    p = Panel(Pretty(\"a\"), title=f\"Searching for `{query}`\")\n    # return p\n    ps = []\n    for q in z:\n        ps.append(Panel(Pretty(inspect(getattr(obj, q), methods=True, docs=True))))\n\n\nsearch(np.random, \"normal\")\n\n╭─ <built-in method lognormal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.lognormal(...)                                                         │\n│                                                                                        │\n│ lognormal(mean=0.0, sigma=1.0, size=None)                                              │\n│                                                                                        │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                           │\n╰────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method multivariate_normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.multivariate_normal(...)                                                         │\n│                                                                                                  │\n│ multivariate_normal(mean, cov, size=None, check_valid='warn', tol=1e-8)                          │\n│                                                                                                  │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.normal(...)                                                         │\n│                                                                                     │\n│ normal(loc=0.0, scale=1.0, size=None)                                               │\n│                                                                                     │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                        │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method standard_normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.standard_normal(...)                                                         │\n│                                                                                              │\n│ standard_normal(size=None)                                                                   │\n│                                                                                              │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                                 │\n╰──────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\ngetattr(pd.DataFrame().values, \"tolist\")\n\n<function ndarray.tolist>\n\n\n\npd.DataFrame.to\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n  \n\n\n\n\n\ndf = pd.DataFrame()\nsearch(df.values, \"list\")\n\n╭───────────────────────────────────────────── Searching for {query} ─────────────────────────────────────────────╮\n│ ['tolist']                                                                                                      │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nsearch(pd.DataFrame, \"to\")\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ [                                                                                                               │\n│     '_constructor',                                                                                             │\n│     '_constructor_sliced',                                                                                      │\n│     '_to_dict_of_blocks',                                                                                       │\n│     'kurtosis',                                                                                                 │\n│     'to_clipboard',                                                                                             │\n│     'to_csv',                                                                                                   │\n│     'to_dict',                                                                                                  │\n│     'to_excel',                                                                                                 │\n│     'to_feather',                                                                                               │\n│     'to_gbq',                                                                                                   │\n│     'to_hdf',                                                                                                   │\n│     'to_html',                                                                                                  │\n│     'to_json',                                                                                                  │\n│     'to_latex',                                                                                                 │\n│     'to_markdown',                                                                                              │\n│     'to_numpy',                                                                                                 │\n│     'to_orc',                                                                                                   │\n│     'to_parquet',                                                                                               │\n│     'to_period',                                                                                                │\n│     'to_pickle',                                                                                                │\n│     'to_records',                                                                                               │\n│     'to_sql',                                                                                                   │\n│     'to_stata',                                                                                                 │\n│     'to_string',                                                                                                │\n│     'to_timestamp',                                                                                             │\n│     'to_xarray',                                                                                                │\n│     'to_xml'                                                                                                    │\n│ ]                                                                                                               │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nq = \"normal\"\n\n\nnp.random.normal\n\n<function RandomState.normal>\n\n\n\nfrom rich import inspect\n\ninspect(eval(f\"np.random.{q}\"), methods=True, all=True)\n\n╭─────────────── <built-in method normal of numpy.random.mtrand.RandomState object at 0x10c627840> ───────────────╮\n│ def RandomState.normal(...)                                                                                     │\n│                                                                                                                 │\n│ normal(loc=0.0, scale=1.0, size=None)                                                                           │\n│                                                                                                                 │\n│            __doc__ = '\\n        normal(loc=0.0, scale=1.0, size=None)\\n\\n        Draw random samples from a     │\n│                      normal (Gaussian) distribution.\\n\\n        The probability density function of the normal  │\n│                      distribution, first\\n        derived by De Moivre and 200 years later by both Gauss and    │\n│                      Laplace\\n        independently [2]_, is often called the bell curve because of\\n           │\n│                      its characteristic shape (see the example below).\\n\\n        The normal distributions      │\n│                      occurs often in nature.  For example, it\\n        describes the commonly occurring         │\n│                      distribution of samples influenced\\n        by a large number of tiny, random              │\n│                      disturbances, each with its own\\n        unique distribution [2]_.\\n\\n        .. note::\\n  │\n│                      New code should use the ``normal`` method of a ``default_rng()``\\n            instance     │\n│                      instead; please see the :ref:`random-quick-start`.\\n\\n        Parameters\\n                 │\n│                      ----------\\n        loc : float or array_like of floats\\n            Mean (\"centre\") of    │\n│                      the distribution.\\n        scale : float or array_like of floats\\n            Standard     │\n│                      deviation (spread or \"width\") of the distribution. Must be\\n            non-negative.\\n    │\n│                      size : int or tuple of ints, optional\\n            Output shape.  If the given shape is,   │\n│                      e.g., ``(m, n, k)``, then\\n            ``m * n * k`` samples are drawn.  If size is        │\n│                      ``None`` (default),\\n            a single value is returned if ``loc`` and ``scale`` are   │\n│                      both scalars.\\n            Otherwise, ``np.broadcast(loc, scale).size`` samples are        │\n│                      drawn.\\n\\n        Returns\\n        -------\\n        out : ndarray or scalar\\n              │\n│                      Drawn samples from the parameterized normal distribution.\\n\\n        See Also\\n            │\n│                      --------\\n        scipy.stats.norm : probability density function, distribution or\\n       │\n│                      cumulative density function, etc.\\n        random.Generator.normal: which should be used   │\n│                      for new code.\\n\\n        Notes\\n        -----\\n        The probability density for the     │\n│                      Gaussian distribution is\\n\\n        .. math:: p(x) = \\\\frac{1}{\\\\sqrt{ 2 \\\\pi \\\\sigma^2    │\n│                      }}\\n                         e^{ - \\\\frac{ (x - \\\\mu)^2 } {2 \\\\sigma^2} },\\n\\n             │\n│                      where :math:`\\\\mu` is the mean and :math:`\\\\sigma` the standard\\n        deviation. The    │\n│                      square of the standard deviation, :math:`\\\\sigma^2`,\\n        is called the variance.\\n\\n  │\n│                      The function has its peak at the mean, and its \"spread\" increases with\\n        the        │\n│                      standard deviation (the function reaches 0.607 times its maximum at\\n        :math:`x +    │\n│                      \\\\sigma` and :math:`x - \\\\sigma` [2]_).  This implies that\\n        normal is more likely  │\n│                      to return samples lying close to the mean, rather\\n        than those far away.\\n\\n        │\n│                      References\\n        ----------\\n        .. [1] Wikipedia, \"Normal distribution\",\\n         │\n│                      https://en.wikipedia.org/wiki/Normal_distribution\\n        .. [2] P. R. Peebles Jr.,       │\n│                      \"Central Limit Theorem\" in \"Probability,\\n               Random Variables and Random       │\n│                      Signal Principles\", 4th ed., 2001,\\n               pp. 51, 51, 125.\\n\\n        Examples\\n  │\n│                      --------\\n        Draw samples from the distribution:\\n\\n        >>> mu, sigma = 0, 0.1 #  │\n│                      mean and standard deviation\\n        >>> s = np.random.normal(mu, sigma, 1000)\\n\\n         │\n│                      Verify the mean and the variance:\\n\\n        >>> abs(mu - np.mean(s))\\n        0.0  # may  │\n│                      vary\\n\\n        >>> abs(sigma - np.std(s, ddof=1))\\n        0.1  # may vary\\n\\n            │\n│                      Display the histogram of the samples, along with\\n        the probability density          │\n│                      function:\\n\\n        >>> import matplotlib.pyplot as plt\\n        >>> count, bins, ignored │\n│                      = plt.hist(s, 30, density=True)\\n        >>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) │\n│                      *\\n        ...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\\n        ...    │\n│                      linewidth=2, color=\\'r\\')\\n        >>> plt.show()\\n\\n        Two-by-four array of samples  │\n│                      from N(3, 6.25):\\n\\n        >>> np.random.normal(3, 2.5, size=(2, 4))\\n                    │\n│                      array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\\n               [  │\n│                      0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\\n\\n        '                │\n│         __module__ = None                                                                                       │\n│           __name__ = 'normal'                                                                                   │\n│       __qualname__ = 'RandomState.normal'                                                                       │\n│           __self__ = RandomState(MT19937) at 0x10C627840                                                        │\n│ __text_signature__ = None                                                                                       │\n│           __call__ = def __call__(*args, **kwargs): Call self as a function.                                    │\n│          __class__ = class __class__():                                                                         │\n│        __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                                   │\n│            __dir__ = def __dir__(): Default dir() implementation.                                               │\n│             __eq__ = def __eq__(value, /): Return self==value.                                                  │\n│         __format__ = def __format__(format_spec, /): Default object formatter.                                  │\n│             __ge__ = def __ge__(value, /): Return self>=value.                                                  │\n│   __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                                 │\n│             __gt__ = def __gt__(value, /): Return self>value.                                                   │\n│           __hash__ = def __hash__(): Return hash(self).                                                         │\n│           __init__ = def __init__(*args, **kwargs): Initialize self.  See help(type(self)) for accurate         │\n│                      signature.                                                                                 │\n│  __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.               │\n│             __le__ = def __le__(value, /): Return self<=value.                                                  │\n│             __lt__ = def __lt__(value, /): Return self<value.                                                   │\n│             __ne__ = def __ne__(value, /): Return self!=value.                                                  │\n│            __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for accurate │\n│                      signature.                                                                                 │\n│         __reduce__ = def __reduce__(...) Helper for pickle.                                                     │\n│      __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                         │\n│           __repr__ = def __repr__(): Return repr(self).                                                         │\n│        __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).                     │\n│         __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                                      │\n│            __str__ = def __str__(): Return str(self).                                                           │\n│   __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize issubclass().    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\ninspect(inspect)\n\n╭─────────────────────────────────────── <function inspect at 0x10f3fc550> ───────────────────────────────────────╮\n│ def inspect(obj: Any, *, console: Optional[ForwardRef('Console')] = None, title: Optional[str] = None, help:    │\n│ bool = False, methods: bool = False, docs: bool = True, private: bool = False, dunder: bool = False, sort: bool │\n│ = True, all: bool = False, value: bool = True) -> None:                                                         │\n│                                                                                                                 │\n│ Inspect any Python object.                                                                                      │\n│                                                                                                                 │\n│ * inspect(<OBJECT>) to see summarized info.                                                                     │\n│ * inspect(<OBJECT>, methods=True) to see methods.                                                               │\n│ * inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.                                              │\n│ * inspect(<OBJECT>, private=True) to see private attributes (single underscore).                                │\n│ * inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.                            │\n│ * inspect(<OBJECT>, all=True) to see all attributes.                                                            │\n│                                                                                                                 │\n│ Args:                                                                                                           │\n│     obj (Any): An object to inspect.                                                                            │\n│     title (str, optional): Title to display over inspect result, or None use type. Defaults to None.            │\n│     help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.             │\n│     methods (bool, optional): Enable inspection of callables. Defaults to False.                                │\n│     docs (bool, optional): Also render doc strings. Defaults to True.                                           │\n│     private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.           │\n│     dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.                │\n│     sort (bool, optional): Sort attributes alphabetically. Defaults to True.                                    │\n│     all (bool, optional): Show all attributes. Defaults to False.                                               │\n│     value (bool, optional): Pretty print value. Defaults to True.                                               │\n│                                                                                                                 │\n│ 35 attribute(s) not shown. Run inspect(inspect) for options.                                                    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InventoryItem:\n    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n\n    name: str\n    unit_price: float\n    quantity_on_hand: int = 0\n\n\nc = InventoryItem(\"A\", 20)\n\n\nc\n\nInventoryItem(name='A', unit_price=20, quantity_on_hand=0)\n\n\n\nclass InventoryItemOld:\n    def __init__(self, name: str, unit_price: float, \n                 quantity_on_hand: int = 0):\n        self.name = name\n        self.unit_price = unit_price\n        self.quantity_on_hand = quantity_on_hand\n\n\nd = InventoryItemOld(\"A\", 2)\nd\n\n<__main__.InventoryItemOld at 0x166340430>\n\n\n\ndef greeting(name: str) -> str:\n    return 'Hello ' + name\n\n\ngreeting(\"Abc\")\n\n'Hello Abc'\n\n\n\ngreeting(10)\n\nTypeError: can only concatenate str (not \"int\") to str"
  },
  {
    "objectID": "notebooks/posts/1d-cnn.html",
    "href": "notebooks/posts/1d-cnn.html",
    "title": "1d CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# Simple 1d dataset\n\ny = torch.Tensor([1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1])\nx = torch.arange(0, len(y))\n\nplt.plot(x, y, 'o-')\n\n\n\n\n\n# Filter 1 (detect silence)\n\nwith torch.no_grad():\n    f1 = nn.Conv1d(1, 1, 3, padding=1)\n    f1.weight.data = torch.Tensor([[[-1, -1, -1]]])\n    f1.bias.data = torch.Tensor([0])\n    y1 = F.relu(f1(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y1, 'o-', label='filtered f1 (silence)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfcccb80>\n\n\n\n\n\n\n# Filter 2 (detect falling edge)\n\nwith torch.no_grad():\n    f2 = nn.Conv1d(1, 1, 3, padding=1)\n    f2.weight.data = torch.Tensor([[[1, 0, -1]]])\n    f2.bias.data = torch.Tensor([0])\n    y2 = F.relu(f2(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y2, 'o-', label='filtered f2 (falling edge)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfbc7250>\n\n\n\n\n\n\n# Filter 3 (detect rising edge)\n\nwith torch.no_grad():\n    f3 = nn.Conv1d(1, 1, 3, padding=1)\n    f3.weight.data = torch.Tensor([[[-1, 0, 1]]])\n    f3.bias.data = torch.Tensor([0])\n    y3 = F.relu(f3(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y3, 'o-', label='filtered f3 (rising edge)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfbb88e0>\n\n\n\n\n\n\n# Filter 4 (detect high amplitude)\n\nwith torch.no_grad():\n    f4 = nn.Conv1d(1, 1, 3, padding=1)\n    f4.weight.data = torch.Tensor([[[1, 1, 1]]])\n    f4.bias.data = torch.Tensor([0])\n    y4 = F.relu(f4(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original') \nplt.plot(x, y4, 'o-', label='filtered f4 (high amplitude)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfb2dfd0>"
  },
  {
    "objectID": "notebooks/posts/parametric-non-parametric.html",
    "href": "notebooks/posts/parametric-non-parametric.html",
    "title": "Parametric v/s Non-Parametric",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nAim:\nGiven Dataset (X, y), learn a function f that maps X to y, i.e. y = f(X).\nWe will consider two cases: - Parametric: f is a function of a fixed number of parameters, e.g. f(x) = ax + b - Non-parametric: f is a function of number of parameters that grows with the size of the dataset."
  },
  {
    "objectID": "notebooks/posts/parametric-non-parametric.html#learnt-functions",
    "href": "notebooks/posts/parametric-non-parametric.html#learnt-functions",
    "title": "Parametric v/s Non-Parametric",
    "section": "Learnt functions",
    "text": "Learnt functions\n\nLogistic Regression\nlogits = X @ w + b\nprob = sigmoid(logits)\ny_pred = prob > 0.5\n\n\nDecision Tree\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], class_names=['0', '1'], filled=True, rounded=True, special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\n\nMLP\nlogits = nn.predict(X)\nprobs = sigmoid(logits)\ny_pred = probs>0.5\n\n\nKNN\nif X1 < 0.5 and X2 < 0.5:\n    y = 0\nelif X1 < 0.5 and X2 >= 0.5:\n    ...\n\n# Sophisticated dataset with 2 classes\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=0, cluster_std=2)\n\n# Plot the dataset\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n\n<matplotlib.collections.PathCollection at 0x7f7a011d2d30>\n\n\n\n\n\n\nlr = LogisticRegression()\nknn = KNeighborsClassifier(n_neighbors=1)\ndt = DecisionTreeClassifier()\nnn = MLPClassifier(hidden_layer_sizes=(100, 100), activation='logistic', max_iter=10000)\n\n# Plot the decision boundaries\n\nplt.figure(figsize=(12, 8))\n\nfor i, model in enumerate([lr, knn, dt, nn]):\n    plt.subplot(2, 2, i + 1)\n    model.fit(X, y)\n    plot_decision_boundary(model, X, y)\n    plt.title(model.__class__.__name__)\n\n\n\n\n\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], class_names=['0', '1'], filled=True, rounded=True, special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\ndt.get_depth(), dt.get_n_leaves()\n\n(9, 24)\n\n\n\nlr.coef_\n\narray([[ 0.19758375, -0.7298237 ]])\n\n\n\n# Now, more noise\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=0, cluster_std=5)\n\n# Plot the dataset\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n\n<matplotlib.collections.PathCollection at 0x7f772abcffa0>\n\n\n\n\n\n\nlr = LogisticRegression()\nknn = KNeighborsClassifier(n_neighbors=1)\ndt = DecisionTreeClassifier()\nnn = MLPClassifier(hidden_layer_sizes=(100, 100), activation='logistic', max_iter=10000)\n\n# Plot the decision boundaries\n\nplt.figure(figsize=(12, 8))\n\nfor i, model in enumerate([lr, knn, dt, nn]):\n    plt.subplot(2, 2, i + 1)\n    model.fit(X, y)\n    plot_decision_boundary(model, X, y)\n    plt.title(model.__class__.__name__)\n\n\n\n\n\ndot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], class_names=['0', '1'], filled=True, rounded=True, special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\ndt.get_depth(), dt.get_n_leaves()\n\n(11, 36)\n\n\n\nlr.coef_\n\narray([[ 0.03933004, -0.10323595]])"
  },
  {
    "objectID": "notebooks/posts/taylor.html",
    "href": "notebooks/posts/taylor.html",
    "title": "Taylor Series",
    "section": "",
    "text": "import jax.numpy as jnp\nfrom jax import random, jit, vmap, grad, jacfwd, jacrev, hessian, value_and_grad\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Define the function to be approximated\n\ndef f(x):\n    return jnp.sin(x)\n\n\n# Plot the function\n\nx = jnp.linspace(-jnp.pi, jnp.pi, 100)\nplt.plot(x, f(x))\n\n\n\n\n\n# First order Taylor approximation for f(x) at x = 0\n\ndef taylor1(f, x, x0=0.):\n    return f(x0) + grad(f)(x0) * (x - x0)\n\n\n# Plot the Taylor approximation\n\nplt.plot(x, f(x), label='f(x)')\nplt.plot(x, taylor1(f, x), label='Taylor approximation')\n\n\n\n\n\n# factorial function in JAX\n\ndef factorial(n):\n    return jnp.prod(jnp.arange(1, n + 1))\n\n\n# Find the nth order Taylor approximation for f(x) at x = 0\n\ndef taylor(f, x, n, x0=0.):\n    grads = {0:f}\n    output = f(x0)\n    for i in range(1, n+1):\n        grads[i] = grad(grads[i-1])\n        output += grads[i](x0) * (x - x0)**i / factorial(i)\n    return output\n\n\nplt.plot(x, f(x), label='f(x)', lw=5)\nplt.plot(x, taylor(f, x, 1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(f, x, 3), label='Taylor approximation, n=3')\nplt.plot(x, taylor(f, x, 5), label='Taylor approximation, n=5')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1aea5ea90>\n\n\n\n\n\n\nx = jnp.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\nplt.plot(x, g(x), label='g(x)', lw=4, alpha=0.5)\nplt.plot(x, taylor(g, x, 1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(g, x, 2), label='Taylor approximation, n=3', ls='--')\n\n\n\n\n\nplt.plot(x, g(x), label='g(x)', lw=4, alpha=0.5)\nplt.plot(x, taylor(g, x, 1, 4.1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(g, x, 2, 4.1), label='Taylor approximation, n=3', ls='--')\nplt.ylim((-2, 20))\n\n(-2.0, 20.0)"
  },
  {
    "objectID": "notebooks/posts/log-sum-exp.html",
    "href": "notebooks/posts/log-sum-exp.html",
    "title": "ES654",
    "section": "",
    "text": "import torch\n\n\nxs = torch.linspace(0.01, 1, 1000)\nxs\n\ntensor([0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0159, 0.0169, 0.0179,\n        0.0189, 0.0199, 0.0209, 0.0219, 0.0229, 0.0239, 0.0249, 0.0259, 0.0268,\n        0.0278, 0.0288, 0.0298, 0.0308, 0.0318, 0.0328, 0.0338, 0.0348, 0.0358,\n        0.0368, 0.0377, 0.0387, 0.0397, 0.0407, 0.0417, 0.0427, 0.0437, 0.0447,\n        0.0457, 0.0467, 0.0477, 0.0486, 0.0496, 0.0506, 0.0516, 0.0526, 0.0536,\n        0.0546, 0.0556, 0.0566, 0.0576, 0.0586, 0.0595, 0.0605, 0.0615, 0.0625,\n        0.0635, 0.0645, 0.0655, 0.0665, 0.0675, 0.0685, 0.0695, 0.0705, 0.0714,\n        0.0724, 0.0734, 0.0744, 0.0754, 0.0764, 0.0774, 0.0784, 0.0794, 0.0804,\n        0.0814, 0.0823, 0.0833, 0.0843, 0.0853, 0.0863, 0.0873, 0.0883, 0.0893,\n        0.0903, 0.0913, 0.0923, 0.0932, 0.0942, 0.0952, 0.0962, 0.0972, 0.0982,\n        0.0992, 0.1002, 0.1012, 0.1022, 0.1032, 0.1041, 0.1051, 0.1061, 0.1071,\n        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1150, 0.1160,\n        0.1170, 0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250,\n        0.1259, 0.1269, 0.1279, 0.1289, 0.1299, 0.1309, 0.1319, 0.1329, 0.1339,\n        0.1349, 0.1359, 0.1368, 0.1378, 0.1388, 0.1398, 0.1408, 0.1418, 0.1428,\n        0.1438, 0.1448, 0.1458, 0.1468, 0.1477, 0.1487, 0.1497, 0.1507, 0.1517,\n        0.1527, 0.1537, 0.1547, 0.1557, 0.1567, 0.1577, 0.1586, 0.1596, 0.1606,\n        0.1616, 0.1626, 0.1636, 0.1646, 0.1656, 0.1666, 0.1676, 0.1686, 0.1695,\n        0.1705, 0.1715, 0.1725, 0.1735, 0.1745, 0.1755, 0.1765, 0.1775, 0.1785,\n        0.1795, 0.1805, 0.1814, 0.1824, 0.1834, 0.1844, 0.1854, 0.1864, 0.1874,\n        0.1884, 0.1894, 0.1904, 0.1914, 0.1923, 0.1933, 0.1943, 0.1953, 0.1963,\n        0.1973, 0.1983, 0.1993, 0.2003, 0.2013, 0.2023, 0.2032, 0.2042, 0.2052,\n        0.2062, 0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2141,\n        0.2151, 0.2161, 0.2171, 0.2181, 0.2191, 0.2201, 0.2211, 0.2221, 0.2231,\n        0.2241, 0.2250, 0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320,\n        0.2330, 0.2340, 0.2350, 0.2359, 0.2369, 0.2379, 0.2389, 0.2399, 0.2409,\n        0.2419, 0.2429, 0.2439, 0.2449, 0.2459, 0.2468, 0.2478, 0.2488, 0.2498,\n        0.2508, 0.2518, 0.2528, 0.2538, 0.2548, 0.2558, 0.2568, 0.2577, 0.2587,\n        0.2597, 0.2607, 0.2617, 0.2627, 0.2637, 0.2647, 0.2657, 0.2667, 0.2677,\n        0.2686, 0.2696, 0.2706, 0.2716, 0.2726, 0.2736, 0.2746, 0.2756, 0.2766,\n        0.2776, 0.2786, 0.2795, 0.2805, 0.2815, 0.2825, 0.2835, 0.2845, 0.2855,\n        0.2865, 0.2875, 0.2885, 0.2895, 0.2905, 0.2914, 0.2924, 0.2934, 0.2944,\n        0.2954, 0.2964, 0.2974, 0.2984, 0.2994, 0.3004, 0.3014, 0.3023, 0.3033,\n        0.3043, 0.3053, 0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123,\n        0.3132, 0.3142, 0.3152, 0.3162, 0.3172, 0.3182, 0.3192, 0.3202, 0.3212,\n        0.3222, 0.3232, 0.3241, 0.3251, 0.3261, 0.3271, 0.3281, 0.3291, 0.3301,\n        0.3311, 0.3321, 0.3331, 0.3341, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390,\n        0.3400, 0.3410, 0.3420, 0.3430, 0.3440, 0.3450, 0.3459, 0.3469, 0.3479,\n        0.3489, 0.3499, 0.3509, 0.3519, 0.3529, 0.3539, 0.3549, 0.3559, 0.3568,\n        0.3578, 0.3588, 0.3598, 0.3608, 0.3618, 0.3628, 0.3638, 0.3648, 0.3658,\n        0.3668, 0.3677, 0.3687, 0.3697, 0.3707, 0.3717, 0.3727, 0.3737, 0.3747,\n        0.3757, 0.3767, 0.3777, 0.3786, 0.3796, 0.3806, 0.3816, 0.3826, 0.3836,\n        0.3846, 0.3856, 0.3866, 0.3876, 0.3886, 0.3895, 0.3905, 0.3915, 0.3925,\n        0.3935, 0.3945, 0.3955, 0.3965, 0.3975, 0.3985, 0.3995, 0.4005, 0.4014,\n        0.4024, 0.4034, 0.4044, 0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104,\n        0.4114, 0.4123, 0.4133, 0.4143, 0.4153, 0.4163, 0.4173, 0.4183, 0.4193,\n        0.4203, 0.4213, 0.4223, 0.4232, 0.4242, 0.4252, 0.4262, 0.4272, 0.4282,\n        0.4292, 0.4302, 0.4312, 0.4322, 0.4332, 0.4341, 0.4351, 0.4361, 0.4371,\n        0.4381, 0.4391, 0.4401, 0.4411, 0.4421, 0.4431, 0.4441, 0.4450, 0.4460,\n        0.4470, 0.4480, 0.4490, 0.4500, 0.4510, 0.4520, 0.4530, 0.4540, 0.4550,\n        0.4559, 0.4569, 0.4579, 0.4589, 0.4599, 0.4609, 0.4619, 0.4629, 0.4639,\n        0.4649, 0.4659, 0.4668, 0.4678, 0.4688, 0.4698, 0.4708, 0.4718, 0.4728,\n        0.4738, 0.4748, 0.4758, 0.4768, 0.4777, 0.4787, 0.4797, 0.4807, 0.4817,\n        0.4827, 0.4837, 0.4847, 0.4857, 0.4867, 0.4877, 0.4886, 0.4896, 0.4906,\n        0.4916, 0.4926, 0.4936, 0.4946, 0.4956, 0.4966, 0.4976, 0.4986, 0.4995,\n        0.5005, 0.5015, 0.5025, 0.5035, 0.5045, 0.5055, 0.5065, 0.5075, 0.5085,\n        0.5095, 0.5105, 0.5114, 0.5124, 0.5134, 0.5144, 0.5154, 0.5164, 0.5174,\n        0.5184, 0.5194, 0.5204, 0.5214, 0.5223, 0.5233, 0.5243, 0.5253, 0.5263,\n        0.5273, 0.5283, 0.5293, 0.5303, 0.5313, 0.5323, 0.5332, 0.5342, 0.5352,\n        0.5362, 0.5372, 0.5382, 0.5392, 0.5402, 0.5412, 0.5422, 0.5432, 0.5441,\n        0.5451, 0.5461, 0.5471, 0.5481, 0.5491, 0.5501, 0.5511, 0.5521, 0.5531,\n        0.5541, 0.5550, 0.5560, 0.5570, 0.5580, 0.5590, 0.5600, 0.5610, 0.5620,\n        0.5630, 0.5640, 0.5650, 0.5659, 0.5669, 0.5679, 0.5689, 0.5699, 0.5709,\n        0.5719, 0.5729, 0.5739, 0.5749, 0.5759, 0.5768, 0.5778, 0.5788, 0.5798,\n        0.5808, 0.5818, 0.5828, 0.5838, 0.5848, 0.5858, 0.5868, 0.5877, 0.5887,\n        0.5897, 0.5907, 0.5917, 0.5927, 0.5937, 0.5947, 0.5957, 0.5967, 0.5977,\n        0.5986, 0.5996, 0.6006, 0.6016, 0.6026, 0.6036, 0.6046, 0.6056, 0.6066,\n        0.6076, 0.6086, 0.6095, 0.6105, 0.6115, 0.6125, 0.6135, 0.6145, 0.6155,\n        0.6165, 0.6175, 0.6185, 0.6195, 0.6205, 0.6214, 0.6224, 0.6234, 0.6244,\n        0.6254, 0.6264, 0.6274, 0.6284, 0.6294, 0.6304, 0.6314, 0.6323, 0.6333,\n        0.6343, 0.6353, 0.6363, 0.6373, 0.6383, 0.6393, 0.6403, 0.6413, 0.6423,\n        0.6432, 0.6442, 0.6452, 0.6462, 0.6472, 0.6482, 0.6492, 0.6502, 0.6512,\n        0.6522, 0.6532, 0.6541, 0.6551, 0.6561, 0.6571, 0.6581, 0.6591, 0.6601,\n        0.6611, 0.6621, 0.6631, 0.6641, 0.6650, 0.6660, 0.6670, 0.6680, 0.6690,\n        0.6700, 0.6710, 0.6720, 0.6730, 0.6740, 0.6750, 0.6759, 0.6769, 0.6779,\n        0.6789, 0.6799, 0.6809, 0.6819, 0.6829, 0.6839, 0.6849, 0.6859, 0.6868,\n        0.6878, 0.6888, 0.6898, 0.6908, 0.6918, 0.6928, 0.6938, 0.6948, 0.6958,\n        0.6968, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017, 0.7027, 0.7037, 0.7047,\n        0.7057, 0.7067, 0.7077, 0.7086, 0.7096, 0.7106, 0.7116, 0.7126, 0.7136,\n        0.7146, 0.7156, 0.7166, 0.7176, 0.7186, 0.7195, 0.7205, 0.7215, 0.7225,\n        0.7235, 0.7245, 0.7255, 0.7265, 0.7275, 0.7285, 0.7295, 0.7305, 0.7314,\n        0.7324, 0.7334, 0.7344, 0.7354, 0.7364, 0.7374, 0.7384, 0.7394, 0.7404,\n        0.7414, 0.7423, 0.7433, 0.7443, 0.7453, 0.7463, 0.7473, 0.7483, 0.7493,\n        0.7503, 0.7513, 0.7523, 0.7532, 0.7542, 0.7552, 0.7562, 0.7572, 0.7582,\n        0.7592, 0.7602, 0.7612, 0.7622, 0.7632, 0.7641, 0.7651, 0.7661, 0.7671,\n        0.7681, 0.7691, 0.7701, 0.7711, 0.7721, 0.7731, 0.7741, 0.7750, 0.7760,\n        0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820, 0.7830, 0.7840, 0.7850,\n        0.7859, 0.7869, 0.7879, 0.7889, 0.7899, 0.7909, 0.7919, 0.7929, 0.7939,\n        0.7949, 0.7959, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008, 0.8018, 0.8028,\n        0.8038, 0.8048, 0.8058, 0.8068, 0.8077, 0.8087, 0.8097, 0.8107, 0.8117,\n        0.8127, 0.8137, 0.8147, 0.8157, 0.8167, 0.8177, 0.8186, 0.8196, 0.8206,\n        0.8216, 0.8226, 0.8236, 0.8246, 0.8256, 0.8266, 0.8276, 0.8286, 0.8295,\n        0.8305, 0.8315, 0.8325, 0.8335, 0.8345, 0.8355, 0.8365, 0.8375, 0.8385,\n        0.8395, 0.8405, 0.8414, 0.8424, 0.8434, 0.8444, 0.8454, 0.8464, 0.8474,\n        0.8484, 0.8494, 0.8504, 0.8514, 0.8523, 0.8533, 0.8543, 0.8553, 0.8563,\n        0.8573, 0.8583, 0.8593, 0.8603, 0.8613, 0.8623, 0.8632, 0.8642, 0.8652,\n        0.8662, 0.8672, 0.8682, 0.8692, 0.8702, 0.8712, 0.8722, 0.8732, 0.8741,\n        0.8751, 0.8761, 0.8771, 0.8781, 0.8791, 0.8801, 0.8811, 0.8821, 0.8831,\n        0.8841, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900, 0.8910, 0.8920,\n        0.8930, 0.8940, 0.8950, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999, 0.9009,\n        0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9068, 0.9078, 0.9088, 0.9098,\n        0.9108, 0.9118, 0.9128, 0.9138, 0.9148, 0.9158, 0.9168, 0.9177, 0.9187,\n        0.9197, 0.9207, 0.9217, 0.9227, 0.9237, 0.9247, 0.9257, 0.9267, 0.9277,\n        0.9286, 0.9296, 0.9306, 0.9316, 0.9326, 0.9336, 0.9346, 0.9356, 0.9366,\n        0.9376, 0.9386, 0.9395, 0.9405, 0.9415, 0.9425, 0.9435, 0.9445, 0.9455,\n        0.9465, 0.9475, 0.9485, 0.9495, 0.9505, 0.9514, 0.9524, 0.9534, 0.9544,\n        0.9554, 0.9564, 0.9574, 0.9584, 0.9594, 0.9604, 0.9614, 0.9623, 0.9633,\n        0.9643, 0.9653, 0.9663, 0.9673, 0.9683, 0.9693, 0.9703, 0.9713, 0.9723,\n        0.9732, 0.9742, 0.9752, 0.9762, 0.9772, 0.9782, 0.9792, 0.9802, 0.9812,\n        0.9822, 0.9832, 0.9841, 0.9851, 0.9861, 0.9871, 0.9881, 0.9891, 0.9901,\n        0.9911, 0.9921, 0.9931, 0.9941, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,\n        1.0000])\n\n\n\nxs.prod()\n\ntensor(0.)\n\n\n\nxs = torch.linspace(0.01, 1, 100)\nxs.prod()\n\ntensor(9.3326e-43)\n\n\n\nxs = torch.linspace(0.01, 1, 1000)\na = xs.log().sum()\n\nb = torch.linspace(0.01, 1, 10000).log().sum()\nprint(a, b)\n\ntensor(-954.8404) tensor(-9536.1816)\n\n\n\ntorch.linspace(0.01, 1, 1000).log().sum()\n\ntensor(-954.8404)\n\n\n\ntorch.linspace(0.01, 1, 10000).log().sum()\n\ntensor(-9536.1816)\n\n\n\n# Trivial example to test the binary cross entropy loss\n\ny_true = torch.tensor([0., 1., 1., 0.])\nlogits = torch.tensor([0.1, 0.9, 0.8, 0.2])\n\nloss = torch.nn.BCEWithLogitsLoss()\nloss(logits, y_true)\n\ntensor(0.5637)\n\n\n\n\ny_hat = torch.nn.Sigmoid()(logits)\ny_hat\n\ntensor([0.5250, 0.7109, 0.6900, 0.5498])\n\n\n\n# BCE\n-(y_true * torch.log(y_hat) + (1 - y_true) * torch.log(1 - y_hat)).mean()\n\ntensor(0.5637)\n\n\n\ndef our_bce(y_true, logits):\n    y_hat = torch.nn.Sigmoid()(logits)\n    return -(y_true * torch.log(y_hat) + (1 - y_true) * torch.log(1 - y_hat)).mean()\n\n\n# check that our implementation is correct\nassert our_bce(y_true, logits) == loss(logits, y_true)\n\n\n# Larger logits\nlogits = torch.tensor([100., 200., 300., 400.])\n\nassert our_bce(y_true, logits) == loss(logits, y_true)\n\n\nAssertionError: \n\n\n\ntorch.nn.Sigmoid()(logits)\n\ntensor([1., 1., 1., 1.])\n\n\n\ntorch.exp(logits)/(1 + torch.exp(logits))\n\ntensor([nan, nan, nan, nan])\n\n\n\ntorch.exp(-logits)\n\ntensor([3.7835e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n\n\n\n1/(1+torch.exp(-logits))\n\ntensor([1., 1., 1., 1.])"
  },
  {
    "objectID": "notebooks/posts/pivot-cross.html",
    "href": "notebooks/posts/pivot-cross.html",
    "title": "Pandas tips",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\nDataset creation\n\n# Create a DataFrame for whether or not a person plays tennis. \n# It has discrete features and 14 rows.\n\ndf = pd.DataFrame({\n    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n})\n\n\n\nCounting values of outcome variable (useful for calculating entropy)\n\nser = df['PlayTennis'].value_counts()\nser\n\nYes    9\nNo     5\nName: PlayTennis, dtype: int64\n\n\n\n\nUsing cross tab to quickly capture the relationship between two variables\n\ndf_out = pd.crosstab(df['Outlook'], df['PlayTennis'])\ndf_out\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\ndf_out.index, df_out.columns\n\n(Index(['Overcast', 'Rain', 'Sunny'], dtype='object', name='Outlook'),\n Index(['No', 'Yes'], dtype='object', name='PlayTennis'))\n\n\n\ne = df_out.sum(axis=1)\ne\n\nOutlook\nOvercast    4\nRain        5\nSunny       5\ndtype: int64\n\n\n\n# Find the fraction of each row that is a 'Yes' and 'No' for PlayTennis\ndf_out['Yes'] = df_out['Yes'] / e\ndf_out['No'] = df_out['No'] / e\ndf_out\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0.0\n      1.0\n    \n    \n      Rain\n      0.4\n      0.6\n    \n    \n      Sunny\n      0.6\n      0.4\n    \n  \n\n\n\n\n\n\nMore efficient cross tabulation (using normalize)\n\npd.crosstab(df['Outlook'], df['PlayTennis'], normalize='index')\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0.0\n      1.0\n    \n    \n      Rain\n      0.4\n      0.6\n    \n    \n      Sunny\n      0.6\n      0.4\n    \n  \n\n\n\n\n\n\nUsing pd.groupby()\n\ndf.groupby([\"Outlook\"]).groups\n\n{'Overcast': [2, 6, 11, 12], 'Rain': [3, 4, 5, 9, 13], 'Sunny': [0, 1, 7, 8, 10]}\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")\n\n\n\n\n\n  \n    \n      \n      Outlook\n      Temperature\n      Humidity\n      Wind\n      PlayTennis\n    \n  \n  \n    \n      0\n      Sunny\n      Hot\n      High\n      Weak\n      No\n    \n    \n      1\n      Sunny\n      Hot\n      High\n      Strong\n      No\n    \n    \n      7\n      Sunny\n      Mild\n      High\n      Weak\n      No\n    \n    \n      8\n      Sunny\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      10\n      Sunny\n      Mild\n      Normal\n      Strong\n      Yes\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")[\"PlayTennis\"]\n\n0      No\n1      No\n7      No\n8     Yes\n10    Yes\nName: PlayTennis, dtype: object\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")[\"PlayTennis\"].value_counts()\n\nNo     3\nYes    2\nName: PlayTennis, dtype: int64\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).groups\n\n{('Overcast', 'Yes'): [2, 6, 11, 12], ('Rain', 'No'): [5, 13], ('Rain', 'Yes'): [3, 4, 9], ('Sunny', 'No'): [0, 1, 7], ('Sunny', 'Yes'): [8, 10]}\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).get_group((\"Sunny\", \"Yes\"))\n\n\n\n\n\n  \n    \n      \n      Outlook\n      Temperature\n      Humidity\n      Wind\n      PlayTennis\n    \n  \n  \n    \n      8\n      Sunny\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      10\n      Sunny\n      Mild\n      Normal\n      Strong\n      Yes\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size()\n\nOutlook   PlayTennis\nOvercast  Yes           4\nRain      No            2\n          Yes           3\nSunny     No            3\n          Yes           2\ndtype: int64\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().index\n\nMultiIndex([('Overcast', 'Yes'),\n            (    'Rain',  'No'),\n            (    'Rain', 'Yes'),\n            (   'Sunny',  'No'),\n            (   'Sunny', 'Yes')],\n           names=['Outlook', 'PlayTennis'])\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().unstack()\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      NaN\n      4.0\n    \n    \n      Rain\n      2.0\n      3.0\n    \n    \n      Sunny\n      3.0\n      2.0\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\n\nUsing pd.pivot_table\n\npivot_table = df.pivot_table(index='Outlook', columns='PlayTennis', aggfunc='size', fill_value=0)\npivot_table\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\ndf_out.plot(kind='bar', stacked=True)\n\n<AxesSubplot:xlabel='Outlook'>\n\n\n\n\n\n\ndf_out.plot(kind='bar', stacked=False)\n\n<AxesSubplot:xlabel='Outlook'>"
  },
  {
    "objectID": "notebooks/posts/autodiff-jax-torch.html",
    "href": "notebooks/posts/autodiff-jax-torch.html",
    "title": "AutoDiff in JAX and PyTorch",
    "section": "",
    "text": "import jax.numpy as jnp\nimport jax\n\nimport torch\nprint(torch.__version__)\nprint(jax.__version__)\n\nArray(1., dtype=float32, weak_type=True)\n\n\n\ndef f(x):\n    return jnp.sin(x)\n\nArray(1., dtype=float32, weak_type=True)\n\n\n\nz = torch.tensor(0.0, requires_grad=True)\ntorch.sin(z).backward()\nprint(jax.grad(f)(0.0), z.grad)\n\ntensor(1.)\n\n\n\ndef f(x):\n    return jnp.abs(x)\n\n\nz1 = torch.tensor(0.0001, requires_grad=True)\ntorch.abs(z1).backward()\n\nz2 = torch.tensor(-0.0001, requires_grad=True)\ntorch.abs(z2).backward()\n\nz3 = torch.tensor(0.0, requires_grad=True)\ntorch.abs(z3).backward()\n\nprint(jax.grad(f)(0.0), z1.grad, z2.grad, z3.grad)\n\n1.0 tensor(1.) tensor(-1.) tensor(0.)\n\n\n\n# Use functorch\n\nimport functorch\n\nImportError: dlopen(/Users/nipun/miniconda3/lib/python3.9/site-packages/functorch/_C.cpython-39-darwin.so, 0x0002): Symbol not found: __ZN2at4_ops10as_strided4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEES8_NS5_8optionalIS7_EE\n  Referenced from: <12715304-4308-3E9B-A374-E4ADB3345E65> /Users/nipun/miniconda3/lib/python3.9/site-packages/functorch/_C.cpython-39-darwin.so\n  Expected in:     <22ECBAD5-EEDD-3C80-9B5A-0564B60B6811> /Users/nipun/miniconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n\n\n\n\n\n'1.12.1'"
  },
  {
    "objectID": "notebooks/posts/condition-inverse.html",
    "href": "notebooks/posts/condition-inverse.html",
    "title": "Conditioning and Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Showing that np.linalg.solve is better conditioned than np.linalg.inv for linear regression normal equations\n\n# Generate data\nn = 100\np = 10\nX = np.random.randn(n, p)\ntheta = np.random.randn(p)\ny = X @ theta + np.random.randn(n)\n\n# Solve normal equations\ntheta_hat = np.linalg.solve(X.T @ X, X.T @ y)\ntheta_hat_inv = np.linalg.inv(X.T @ X) @ X.T @ y\n\n# Compare the condition numbers\nprint(np.linalg.cond(X.T @ X))\nprint(np.linalg.cond(np.linalg.inv(X.T @ X)))\n\n# Plot the difference between the two solutions\nplt.plot(theta_hat - theta_hat_inv)\nplt.title('Difference between solutions')\nplt.xlabel('Index')\nplt.ylabel('Difference')\nplt.show()\n\n\n2.980877596192165\n2.980877596192165"
  },
  {
    "objectID": "notebooks/posts/cnn.html",
    "href": "notebooks/posts/cnn.html",
    "title": "CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# MNIST dataset\n\nfrom torchvision import datasets, transforms\nimport torchvision\n\n# Split MNIST into train, validation, and test sets\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n\n# Split train_data into train and validation sets\nval_data = torch.utils.data.Subset(train_data, range(50000, 51000))\n\n# Reduce the size of the training set to 5,000\ntrain_data = torch.utils.data.Subset(train_data, range(0, 5000))\n\n\n# Create data loaders\nbatch_size = 64\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n\nimg, target = next(iter(train_loader))\nprint(img.shape)\nprint(target.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\n\nplt.imshow(img[0].numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\nimg[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Create a simple LeNet like CNN\n\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5) \n        # 6 input image channel, 16 output channels, 5x5 square convolution\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x) # 28x28x1 -> 24x24x6\n        x = F.max_pool2d(F.relu(x), 2) # 24x24x6 -> 12x12x6\n        x = self.conv2(x) # 12x12x6 -> 8x8x16\n        x = F.max_pool2d(F.relu(x), 2) # 8x8x16 -> 4x4x16\n        x = x.view(-1, self.num_flat_features(x)) # 4x4x16 -> 256\n        x = self.fc1(x) # 256 -> 120\n        x = F.relu(x)\n        x = self.fc2(x) # 120 -> 84\n        x = F.relu(x)\n        x = self.fc3(x) # 84 -> 10\n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\n# Create a model\n\nmodel = LeNet5()\nprint(model)\n\nLeNet5(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n# Train the model\n\n# Define the loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nn_epochs = 10\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(n_epochs):\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    # Train the model\n    model.train()\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        \n    # Evaluate the model\n    model.eval()\n    for data, target in val_loader:\n        output = model(data)\n        loss = criterion(output, target)\n        val_loss += loss.item()*data.size(0)\n        \n    # Calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    val_loss = val_loss/len(val_loader.sampler)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    # Print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch+1, \n        train_loss,\n        val_loss\n        ))\n\nEpoch: 1    Training Loss: 1.437300     Validation Loss: 0.653900\nEpoch: 2    Training Loss: 0.424091     Validation Loss: 0.367598\nEpoch: 3    Training Loss: 0.303504     Validation Loss: 0.308797\nEpoch: 4    Training Loss: 0.219186     Validation Loss: 0.257062\nEpoch: 5    Training Loss: 0.195089     Validation Loss: 0.214157\nEpoch: 6    Training Loss: 0.153489     Validation Loss: 0.190220\nEpoch: 7    Training Loss: 0.130065     Validation Loss: 0.189110\nEpoch: 8    Training Loss: 0.114033     Validation Loss: 0.173153\nEpoch: 9    Training Loss: 0.103402     Validation Loss: 0.167645\nEpoch: 10   Training Loss: 0.089715     Validation Loss: 0.156438\n\n\n\n# Plot the training and validation loss\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(val_losses, label='Validation loss')\n\n\n\n\n\n# Test the model\n\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for data, target in test_loader:\n        output = model(data)\n        _, predicted = torch.max(output.data, 1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n    print('Test Accuracy: {}%'.format(100 * correct / total))\n\nTest Accuracy: 96.1%\n\n\n\n# Now, let us take an image and walk it through the model\n\ntest_img = train_data[1][0].unsqueeze(0)\n\n\nplt.imshow(test_img.numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\n# Get weights and biases from the first convolutional layer\n\nweights = model.conv1.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False, annot=True)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n# Get output from model's first conv1 layer\n\nconv1 = F.relu(model.conv1(test_img))\n\n# For plotting bring all the images to the same scale\nc1 = conv1 - conv1.min()\nc1 = c1 / conv1.max()\n\nprint(c1.shape)\nprint(\"1 image, 6 channels, 24x24 pixels\")\n\ntorch.Size([1, 6, 24, 24])\n1 image, 6 channels, 24x24 pixels\n\n\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\n\n\n\n\n# Get output from model after max pooling\n\npool1 = F.max_pool2d(conv1, 2)\n\n# For plotting bring all the images to the same scale\np1 = pool1 - pool1.min()\np1 = p1 / pool1.max()\n\nprint(p1.shape)\nprint(\"1 image, 6 channels, 12x12 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 6, 12, 12])\n1 image, 6 channels, 12x12 pixels\n\n\n\n\n\n\n# Visualize the filters in the second convolutional layer\n\nweights = model.conv2.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n# Get output from model's second conv2 layer\n\nconv2 = F.relu(model.conv2(pool1))\n\n# For plotting bring all the images to the same scale\nc2 = conv2 - conv2.min()\nc2 = c2 / conv2.max()\n\nprint(c2.shape)\nprint(\"1 image, 16 channels, 8x8 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 8, 8])\n1 image, 16 channels, 8x8 pixels\n\n\n\n\n\n\n# Get output from model after max pooling\n\npool2 = F.max_pool2d(conv2, 2)\n\n# For plotting bring all the images to the same scale\np2 = pool2 - pool2.min()\np2 = p2 / pool2.max()\n\nprint(p2.shape)\nprint(\"1 image, 16 channels, 4x4 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 4, 4])\n1 image, 16 channels, 4x4 pixels\n\n\n\n\n\n\n# Flatten the output of the second convolutional layer\n\nflat = pool2.view(pool2.size(0), -1)\nprint(flat.shape)\n\ntorch.Size([1, 256])\n\n\n\n# Repeat the above process as a function to visualize the convolution outputs for any image for any layer\ndef scale_img(img):\n    \"\"\"\n    Scale the image to the same scale\n    \"\"\"\n    img = img - img.min()\n    img = img / img.max()\n    return img\n\ndef visualize_conv_output(model, img):\n    \"\"\"\n    Visualize the output of a convolutional layer\n    \"\"\"\n    # Get output from model's first conv1 layer\n    conv1 = F.relu(model.conv1(img))\n\n    # For plotting bring all the images to the same scale\n    c1 = scale_img(conv1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 1', fontsize=16)\n    \n    # Get output from model after max pooling\n    pool1 = F.max_pool2d(conv1, 2)\n\n    # For plotting bring all the images to the same scale\n    p1 = scale_img(pool1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 1', fontsize=16)\n\n    # Get output from model's second conv2 layer\n    conv2 = F.relu(model.conv2(pool1))\n\n    # For plotting bring all the images to the same scale\n    c2 = scale_img(conv2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    \n    for i in range(16):\n        sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 2', fontsize=16)\n\n    # Get output from model after max pooling\n    pool2 = F.max_pool2d(conv2, 2)\n\n    # For plotting bring all the images to the same scale\n    p2 = scale_img(pool2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    for i in range(16):\n        sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 2', fontsize=16)\n\n\nvisualize_conv_output(model, train_data[2][0].unsqueeze(0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_conv_output(model, train_data[4][0].unsqueeze(0))"
  },
  {
    "objectID": "notebooks/posts/maths-jax.html",
    "href": "notebooks/posts/maths-jax.html",
    "title": "Maths and JAX",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\n\n\ndef func(x, y, z):\n    return x**2 + jnp.sin(y) + z\n\n\nfunc(1, 2, 3)\n\nDeviceArray(4.9092975, dtype=float32, weak_type=True)\n\n\n\nfrom sympy import *\ninit_printing()\n\nx, y, z = symbols('x y z')\nf = x**2 + sin(y) + z\nf\n\n\\(\\displaystyle x^{2} + z + \\sin{\\left(y \\right)}\\)\n\n\n\ndiff(f, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\n# Find the derivative of f with respect to x, y, and z using sympy\ndel_x, del_y, del_z = diff(f, x), diff(f, y), diff(f, z)\ndel_x, del_y, del_z\n\n\\(\\displaystyle \\left( 2 x, \\  \\cos{\\left(y \\right)}, \\  1\\right)\\)\n\n\n\ngrad_f = lambdify((x, y, z), [del_x, del_y, del_z])\ngrad_f(1, 2, 3)\n\n\\(\\displaystyle \\left[ 2, \\  -0.416146836547142, \\  1\\right]\\)\n\n\n\ngrad_f_jax = jax.grad(func, argnums=(0, 1, 2))\ngrad_f_jax(1., 2., 3.)\n\n(DeviceArray(2., dtype=float32, weak_type=True),\n DeviceArray(-0.41614684, dtype=float32, weak_type=True),\n DeviceArray(1., dtype=float32, weak_type=True))\n\n\n\nn = 20\nA = jax.random.normal(shape=(1, n), key=jax.random.PRNGKey(0), dtype=jnp.float32)\ntheta = jax.random.normal(shape=(n, 1), key=jax.random.PRNGKey(0), dtype=jnp.float32)\nb = A @ theta \n\nb\n\nDeviceArray([[28.684494]], dtype=float32)\n\n\n\nb.flatten(), b.item()\n\n(DeviceArray([28.684494], dtype=float32), 28.684494018554688)\n\n\n\ndef a_theta(A, theta):\n    return A @ theta\n\n\na_theta(A, theta)\n\nDeviceArray([[28.684494]], dtype=float32)\n\n\n\ngrad_a_theta = jax.grad(a_theta, argnums=1)\n\n\njax.jacobian(a_theta, argnums=1)(A, theta)[0, 0, :].shape\n\n\\(\\displaystyle \\left( 20, \\  1\\right)\\)\n\n\n\nA.shape\n\n\\(\\displaystyle \\left( 1, \\  20\\right)\\)\n\n\n\n# Sympy version\n\n\nA = MatrixSymbol('A', 1, n)\ntheta = MatrixSymbol('theta', n, 1)\nA, theta\n\n\\(\\displaystyle \\left( A, \\  \\theta\\right)\\)\n\n\n\ndiff(A*theta, theta)\n\n\\(\\displaystyle A^{T}\\)"
  },
  {
    "objectID": "notebooks/posts/cnn-edge.html",
    "href": "notebooks/posts/cnn-edge.html",
    "title": "CNN Edge 2d",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\nimport seaborn as sns\n%matplotlib inline\n\n\n# Create a tensor of size 6x6 with first three columns as 1 and rest as 0\nx = torch.zeros(6, 6)\nx[:, :3] = 1\nprint(x)\n\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])\n\n\n\nx.shape\n\ntorch.Size([6, 6])\n\n\n\n# Plot the tensor with equal aspect ratio\nplt.figure(figsize=(6, 6))\nsns.heatmap(x, cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\n# Create a 3x3 kernel with first column as 1, second as 0 and third as -1\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\nprint(k)\n\ntensor([[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]])\n\n\n\n# Apply the kernel to the image\n\n\ny = F.conv2d(x.view(1, 1, 6, 6), k.view(1, 1, 3, 3))\nprint(y)\n\n# Create figure of size of y\nplt.figure(figsize=(y.shape[2], y.shape[3]))\nsns.heatmap(y[0, 0], cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\ntensor([[[[0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.]]]])\n\n\n<AxesSubplot: >\n\n\n\n\n\n\nim = plt.imread('lm.jpeg')\nplt.imshow(im)\n\n<matplotlib.image.AxesImage at 0x151220670>\n\n\n\n\n\n\n# Crop to left 180 X 180 pixels\n\nim = im[:180, :180]\nplt.imshow(im, cmap='gray')\n\n<matplotlib.image.AxesImage at 0x1512b3cd0>\n\n\n\n\n\n\n# Convert to grayscale\nim = im.mean(axis=2)\nplt.imshow(im, cmap='gray')\n\n<matplotlib.image.AxesImage at 0x15133dac0>\n\n\n\n\n\n\nim.shape\n\n(180, 180)\n\n\n\n# Detect edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\n\n\n\n# plot the result\n#plt.figure(figsize=(y.shape[2], y.shape[3]))\nplt.imshow(y[0, 0], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x197097430>\n\n\n\n\n\n\n# Detect horizontal edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float().T\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\nplt.imshow(y[0, 0], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x197105730>"
  },
  {
    "objectID": "notebooks/posts/knn-lsh.html",
    "href": "notebooks/posts/knn-lsh.html",
    "title": "KNN LSH",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Generate some data\n\nX = np.random.randn(3000, 2)\n\n# Plot the data\n\nplt.scatter(X[:, 0], X[:, 1], s=30)\n\n<matplotlib.collections.PathCollection at 0x7f321da32c10>\n\n\n\n\n\n\n# Naive KNN\n\ndef naive_knn_for_loop(X, x_test, k=3):\n    dists = np.zeros(X.shape[0])\n    for i in range(X.shape[0]): # N iterations (N = number of data points)\n        dists[i] = np.dot(X[i] - x_test, X[i] - x_test) # Time complexity: O(D)\n    \n    # Time complexity to create the distance array: O(N*D)\n\n    # Now, we need to find the k smallest distances\n    return np.argpartition(dists, k)[:k] # Time complexity: O(Nk) or O(N) depending on the implementation\n  \n\n\nnaive_knn_for_loop(X, np.array([0, 0]))\n\narray([2529,  958,  804])\n\n\n\nX[naive_knn_for_loop(X, np.array([0, 0]))]\n\narray([[-0.02103967,  0.02703294],\n       [ 0.0092843 ,  0.02548091],\n       [-0.03094897,  0.01750535]])\n\n\n\n%timeit naive_knn_for_loop(X, np.array([0, 0]))\n\n12.3 ms ± 47.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n# Implement using numpy\n\ndef naive_knn_numpy(X, x_test, k=3):\n    dists = np.sum((X - x_test)**2, axis=1)\n    #return np.partition(dists, k)[:k]\n    sorted_dists = np.argsort(dists)\n    return sorted_dists[:k]\n\n\nnaive_knn_numpy(X, np.array([0, 0]))\n\narray([ 958, 2529,  804])\n\n\n\n%timeit naive_knn_numpy(X, np.array([0, 0]))\n\n240 µs ± 631 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n# Implement using numpy\n\ndef naive_knn_numpy(X, x_test, k=3):\n    dists = np.sum((X - x_test)**2, axis=1)\n    return np.argpartition(dists, k)[:k]\n    #sorted_dists = np.argsort(dists)\n    #return sorted_dists[:k]\n\n\n%timeit naive_knn_numpy(X, np.array([0, 0]))\n\n84.6 µs ± 607 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n# Show LSH implementation step by step\n\n# Creating a random separating hyperplane\n\nw = np.random.randn(2)\nb = np.random.randn(1)/4.0\n\n# Plot the separating hyperplane\n\nx = np.linspace(-3, 3, 100)\ny = -(w[0] * x + b) / w[1]\n\nplt.scatter(X[:, 0], X[:, 1], s=30)\nplt.plot(x, y, 'r', linewidth=3)\n\n# Color the points based on which side of the hyperplane they are on\n\ncolors = X[:, 0]*w[0] + X[:, 1]*w[1] + b > 0\n\nplt.scatter(X[:, 0], X[:, 1], s=30, c=colors)\n\n<matplotlib.collections.PathCollection at 0x7f31106b4ac0>\n\n\n\n\n\n\n# Create three random hyperplanes and color the points based on which side of the hyperplane they are on. \n# there should be 2^3 = 8 different colors\n# each separating hyperplane corresponds to a bit in the hash\n\nhash = np.zeros((X.shape[0], 3)).astype(int)\nws = []\nbs = []\n# Cost for creating the hash table: O(N*H*D) \nfor H in range(3): # H = number of hyperplanes\n    w = np.random.randn(2)\n    b = np.random.randn(1)/4.0\n    ws.append(w)\n    bs.append(b)\n    hash[:, H] = X[:, 0]*w[0] + X[:, 1]*w[1] + b > 0 # D computations per iteration\n\n# Convert the hash to a decimal number\n\nhash_dec = np.sum(hash * 2**np.arange(3)[::-1], axis=1)\n\n# Plot the hash\n\nplt.scatter(X[:, 0], X[:, 1], s=30, c=hash)\n\n# Plot the hash with the separating hyperplanes\n\nplt.scatter(X[:, 0], X[:, 1], s=30, c=hash_dec)\nfor w, b in zip(ws, bs):\n    print(w, b)\n    x = np.linspace(-3, 3, 100)\n    y = -(w[0] * x + b) / w[1]\n    plt.plot(x, y, 'r')\n\n# Mark the test point\n\nx_test = np.array([0, 0])\nplt.scatter(x_test[0], x_test[1], s=100, c='r')\n\n[-1.78798897 -1.3408181 ] [-0.08094113]\n[ 0.9447324  -2.47059549] [0.09350769]\n[0.20531227 0.97521902] [-0.22471283]\n\n\n<matplotlib.collections.PathCollection at 0x7f31105bf280>\n\n\n\n\n\n\ndf = pd.DataFrame(hash)\ndf.columns = ['h1', 'h2', 'h3']\ndf['hash_dec'] = hash_dec\ndf['x'] = X[:, 0]\ndf['y'] = X[:, 1]\n\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      h1\n      h2\n      h3\n      hash_dec\n      x\n      y\n    \n  \n  \n    \n      0\n      1\n      1\n      0\n      6\n      -1.289013\n      -0.497073\n    \n    \n      1\n      1\n      1\n      0\n      6\n      0.721631\n      -1.923390\n    \n    \n      2\n      1\n      1\n      0\n      6\n      0.042595\n      -0.177549\n    \n    \n      3\n      1\n      1\n      0\n      6\n      0.148706\n      -0.452442\n    \n    \n      4\n      1\n      1\n      0\n      6\n      -0.047372\n      -0.431685\n    \n    \n      5\n      1\n      1\n      0\n      6\n      -0.478764\n      -0.304759\n    \n    \n      6\n      0\n      1\n      0\n      2\n      0.812057\n      -0.574337\n    \n    \n      7\n      1\n      0\n      1\n      5\n      -1.493164\n      1.209339\n    \n    \n      8\n      0\n      1\n      0\n      2\n      0.820065\n      -0.575965\n    \n    \n      9\n      0\n      0\n      1\n      1\n      1.045276\n      1.143788\n    \n  \n\n\n\n\n\n\npd.DataFrame(hash).value_counts()\n\n0  1  2\n1  1  0    846\n0  0  1    827\n   1  0    491\n1  0  0    346\n      1    243\n0  1  1    210\n   0  0     37\ndtype: int64\n\n\n\n# Predict the K nearest neighbors using LSH\n\n# Compute the hash for the test point\n\nx_test = np.array([0, 0])\nhash_test = x_test[0]*ws[0][0] + x_test[1]*ws[0][1] + bs[0] > 0\n\n#convert to decimal\nhash_test_dec = np.sum(hash_test * 2**np.arange(3)[::-1])\n\nhash_test_dec\n\n0\n\n\n\n# Find subset of points with the same hash\n\nX_subset = X[hash_dec == hash_test_dec]\nX_subset.shape\n\n(37, 2)\n\n\n\n# Now, we can use the naive KNN implementation to find the K nearest neighbors\n\nix = naive_knn_numpy(X_subset, x_test, k=3)\nX_subset[ix]\n\narray([[-0.04090763,  0.07013394],\n       [-0.00419256,  0.08614131],\n       [-0.05284791,  0.06786371]])\n\n\n\n%timeit naive_knn_numpy(X_subset, x_test, k=3)\n\n10.5 µs ± 31.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n# Using FAISS from Facebook\n\nimport faiss\n\n# Create an index\nindex = faiss.IndexFlatL2(2)   # build the index\n\n# Add the data to the index\nindex.add(X.astype(np.float32))                  # add vectors to the index\n\n\n# Search for the K nearest neighbors\nD, I = index.search(x_test.astype(np.float32).reshape(1, -1), k=3)     # actual search\n\n\nD\n\narray([[0.00073547, 0.00117345, 0.00126428]], dtype=float32)\n\n\n\nI\n\narray([[ 958, 2529,  804]])\n\n\n\nX[I[0]]\n\narray([[ 0.0092843 ,  0.02548091],\n       [-0.02103967,  0.02703294],\n       [-0.03094897,  0.01750535]])\n\n\n\n%timeit index.search(x_test.astype(np.float32).reshape(1, -1), k=3)\n\n50.9 µs ± 212 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n# Now, run on GPU\n\nres = faiss.StandardGpuResources()  # use a single GPU\n\n# Create an index\nindex = faiss.IndexFlatL2(2)   # build the index\n\ngpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n\n# Add the data to the index\ngpu_index_flat.add(X.astype(np.float32))                  # add vectors to the index\n\n\n%timeit gpu_index_flat.search(x_test.astype(np.float32).reshape(1, -1), k=3)\n\n79.8 µs ± 674 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n# The above is slow because\n# 1. We are copying the data to the GPU\n# 2. We are copying the data back to the CPU\n# 3. Not enough data and low dimensional data"
  },
  {
    "objectID": "notebooks/posts/lowrank-matrix.html",
    "href": "notebooks/posts/lowrank-matrix.html",
    "title": "Maths and JAX: Low Rank",
    "section": "",
    "text": "Multiplying a matrix A with a vector x transforms x\n\n\n\n\nTransforming a vector via a low rank matrix in the shown examples leads to a line\n\nWe first study Goal 1. The interpretation of matrix vector product is borrowed from the excellent videos from the 3Blue1Brown channel. I’ll first set up the environment by importing a few relevant libraries.\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nfrom sympy import Matrix, MatrixSymbol, Eq, MatMul\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=0.75)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nsympy_A = MatrixSymbol(\"A\", 2, 2)\nsympy_x = MatrixSymbol(\"x\", 2, 1)\ny = MatrixSymbol(\"y\", 2, 1)\n\nEq(y, sympy_A*sympy_x, evaluate=False)\n\n\\(\\displaystyle y = A x\\)\n\n\nGiven a matrix A and a vector x, we are trying to get y=Ax. Let us first see the values for a specific instance in the 2d space.\n\nA = np.array([[2, 1], [1, 4]])\n\nx = np.array([1, 1])\nAx = A @ x\n\nEq(Matrix(Ax), MatMul(Matrix(A), Matrix(x)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}3\\\\5\\end{matrix}\\right] = \\left[\\begin{matrix}2 & 1\\\\1 & 4\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\nHere, we have A = \\(\\left[\\begin{matrix}2 & 1\\\\1 & 4\\end{matrix}\\right]\\) and x = \\({\\text{[1 1]}}\\)\nNow some code to create arrows to represent arrows.\n\ndef plot_arrow(ax, x, color, label):\n    x_head, y_head = x[0], x[1]\n    x_tail = 0.0\n    y_tail = 0.0\n    dx = x_head - x_tail\n    dy = y_head - y_tail\n\n    arrow = mpatches.FancyArrowPatch(\n        (x_tail, y_tail), (x_head, y_head), mutation_scale=10, color=color, label=label\n    )\n\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6, 1), borderaxespad=0)\n\nNow some code to plot the vector corresponding to Ax\n\ndef plot_transform(A, x):\n    Ax = A @ x\n    fig, ax = plt.subplots()\n    plot_arrow(ax, x, \"k\", f\"Original (x) {x}\")\n    plot_arrow(ax, Ax, \"g\", f\"Transformed (Ax) {Ax}\")\n    plt.xlim((-5, 5))\n    plt.ylim((-5, 5))\n    plt.grid(alpha=0.1)\n    ax.set_aspect(\"equal\")\n    plt.title(f\"A = {A}\")\n    sns.despine(left=True, bottom=True)\n    plt.tight_layout()\n\n\nplot_transform(np.array([[1.0, 1.0], [1.0, -1.0]]), [1.0, 2.0])\nplt.savefig(\"Ax1.png\", dpi=100)\n\n\n\n\nIn the plot above, we can see that the vector [1, 2] is transformed to [3, -1] via the matrix A.\nLet us now write some code to create the rotation matrix and apply it on our input x\n\ndef rot(angle):\n    theta = np.radians(angle)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array(((c, -s), (s, c)))\n    return np.round(R, 2)\n\n\nx = np.array([1.0, 2.0])\nplot_transform(rot(90), x)\nplt.savefig(\"Ax2\", dpi=100)\n\n\n\n\nAs we can see above, creating the 90 degree rotation matrix indeed transforms our vector anticlockwise 90 degrees.\nNow let us talk about matrices A that are low rank. I am creating a simple low rank matrix where the second row is some constant times the first row.\n\ndef plot_lr(x, slope):\n    low_rank = np.array([1.0, 2.0])\n    low_rank = np.vstack((low_rank, slope * low_rank))\n    plot_transform(low_rank, x)\n    x_lin = np.linspace(-5, 5, 100)\n    y = x_lin * slope\n    plt.plot(x_lin, y, alpha=0.4, lw=5, label=f\"y = {slope}x\")\n    plt.legend(bbox_to_anchor=(1.2, 1), borderaxespad=0)\n\n\nplot_lr(x, 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-1.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([1.0, -1.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-2.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([0.5, -0.7], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-3.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([-1.0, 0.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-4.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\nTo summarize\n\nIn the above plots we can see that changing our x to any vector in the 2d space leads to us to transformed vector not covering the whole 2d space, but on line in the 2d space. One can easily take this learning to higher dimensional matrices A."
  },
  {
    "objectID": "notebooks/posts/movie-recommendation-knn-mf.html",
    "href": "notebooks/posts/movie-recommendation-knn-mf.html",
    "title": "Movie Recommendation using KNN and Matrix Factorization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Generate some toy user and movie data\n\n# Number of users\nn_users = 100\n\n# Number of movies\nn_movies = 10\n\n# Number of ratings\nn_ratings = 1000\n\n# Generate random user ids\nuser_ids = np.random.randint(0, n_users, n_ratings)\n\n# Generate random movie ids\nmovie_ids = np.random.randint(0, n_movies, n_ratings)\n\n# Generate random ratings\nratings = np.random.randint(1, 6, n_ratings)\n\n# Create a dataframe with the data\ndf = pd.DataFrame({'user_id': user_ids, 'movie_id': movie_ids, 'rating': ratings})\n\n# We should not have any duplicate ratings for the same user and movie\n# Drop any rows that have duplicate user_id and movie_id pairs\ndf = df.drop_duplicates(['user_id', 'movie_id'])\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      user_id\n      movie_id\n      rating\n    \n  \n  \n    \n      0\n      42\n      2\n      1\n    \n    \n      1\n      89\n      1\n      2\n    \n    \n      2\n      86\n      1\n      4\n    \n    \n      3\n      12\n      6\n      3\n    \n    \n      4\n      80\n      6\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      981\n      27\n      0\n      2\n    \n    \n      982\n      1\n      2\n      2\n    \n    \n      984\n      81\n      4\n      5\n    \n    \n      988\n      11\n      7\n      5\n    \n    \n      997\n      56\n      4\n      3\n    \n  \n\n623 rows × 3 columns\n\n\n\n\n# Create a user-item matrix\n\nA = df.pivot(index='user_id', columns='movie_id', values='rating')\nA\n\n\n\n\n\n  \n    \n      movie_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n    \n      user_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      4.0\n      3.0\n      5.0\n      5.0\n      NaN\n      NaN\n      NaN\n      5.0\n      3.0\n      NaN\n    \n    \n      1\n      1.0\n      1.0\n      2.0\n      5.0\n      NaN\n      5.0\n      NaN\n      1.0\n      3.0\n      NaN\n    \n    \n      2\n      3.0\n      NaN\n      5.0\n      4.0\n      1.0\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n    \n    \n      3\n      NaN\n      4.0\n      1.0\n      3.0\n      NaN\n      5.0\n      2.0\n      1.0\n      NaN\n      1.0\n    \n    \n      4\n      4.0\n      3.0\n      3.0\n      2.0\n      1.0\n      NaN\n      4.0\n      NaN\n      4.0\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      3.0\n      NaN\n      1.0\n      NaN\n      1.0\n      3.0\n      3.0\n      3.0\n      4.0\n      4.0\n    \n    \n      96\n      1.0\n      2.0\n      NaN\n      NaN\n      3.0\n      NaN\n      2.0\n      3.0\n      NaN\n      2.0\n    \n    \n      97\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      3.0\n      1.0\n      NaN\n      3.0\n      NaN\n    \n    \n      98\n      NaN\n      4.0\n      4.0\n      1.0\n      1.0\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n    \n    \n      99\n      1.0\n      2.0\n      1.0\n      NaN\n      2.0\n      NaN\n      1.0\n      4.0\n      3.0\n      2.0\n    \n  \n\n100 rows × 10 columns\n\n\n\n\n# Fill in the missing values with zeros\nA = A.fillna(0)\n\nA\n\n\n\n\n\n  \n    \n      movie_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n    \n      user_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      4.0\n      3.0\n      5.0\n      5.0\n      0.0\n      0.0\n      0.0\n      5.0\n      3.0\n      0.0\n    \n    \n      1\n      1.0\n      1.0\n      2.0\n      5.0\n      0.0\n      5.0\n      0.0\n      1.0\n      3.0\n      0.0\n    \n    \n      2\n      3.0\n      0.0\n      5.0\n      4.0\n      1.0\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n    \n    \n      3\n      0.0\n      4.0\n      1.0\n      3.0\n      0.0\n      5.0\n      2.0\n      1.0\n      0.0\n      1.0\n    \n    \n      4\n      4.0\n      3.0\n      3.0\n      2.0\n      1.0\n      0.0\n      4.0\n      0.0\n      4.0\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      3.0\n      0.0\n      1.0\n      0.0\n      1.0\n      3.0\n      3.0\n      3.0\n      4.0\n      4.0\n    \n    \n      96\n      1.0\n      2.0\n      0.0\n      0.0\n      3.0\n      0.0\n      2.0\n      3.0\n      0.0\n      2.0\n    \n    \n      97\n      0.0\n      0.0\n      0.0\n      3.0\n      0.0\n      3.0\n      1.0\n      0.0\n      3.0\n      0.0\n    \n    \n      98\n      0.0\n      4.0\n      4.0\n      1.0\n      1.0\n      0.0\n      5.0\n      0.0\n      0.0\n      0.0\n    \n    \n      99\n      1.0\n      2.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      4.0\n      3.0\n      2.0\n    \n  \n\n100 rows × 10 columns\n\n\n\n\n# Cosine similarity between U1 and U2\n\n# User 1\nu1 = A.loc[0]\n\n# User 2\nu2 = A.loc[1]\n\n# Compute the dot product\ndot = np.dot(u1, u2)\n\n# Compute the L2 norm\nnorm_u1 = np.linalg.norm(u1)\nnorm_u2 = np.linalg.norm(u2)\n\n# Compute the cosine similarity\ncos_sim = dot / (norm_u1 * norm_u2)\ncos_sim\n\n0.6602414870635759\n\n\n\n# Calculate the cosine similarity between users\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsim_matrix = cosine_similarity(A)\n\npd.DataFrame(sim_matrix)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      90\n      91\n      92\n      93\n      94\n      95\n      96\n      97\n      98\n      99\n    \n  \n  \n    \n      0\n      1.000000\n      0.660241\n      0.777679\n      0.469409\n      0.699862\n      0.479770\n      0.755574\n      0.416966\n      0.545591\n      0.554525\n      ...\n      0.583891\n      0.888070\n      0.424048\n      0.299175\n      0.603374\n      0.503721\n      0.430077\n      0.434429\n      0.461384\n      0.666361\n    \n    \n      1\n      0.660241\n      1.000000\n      0.792212\n      0.766282\n      0.507726\n      0.526331\n      0.537985\n      0.504328\n      0.414964\n      0.461888\n      ...\n      0.250122\n      0.782586\n      0.194625\n      0.269131\n      0.480750\n      0.514929\n      0.132647\n      0.907222\n      0.272428\n      0.350325\n    \n    \n      2\n      0.777679\n      0.792212\n      1.000000\n      0.550823\n      0.816830\n      0.520153\n      0.812736\n      0.786007\n      0.598532\n      0.679392\n      ...\n      0.540720\n      0.721315\n      0.156556\n      0.510296\n      0.723532\n      0.792913\n      0.391239\n      0.729769\n      0.515625\n      0.626224\n    \n    \n      3\n      0.469409\n      0.766282\n      0.550823\n      1.000000\n      0.468293\n      0.663451\n      0.395348\n      0.525726\n      0.338742\n      0.639021\n      ...\n      0.420539\n      0.754386\n      0.544510\n      0.413714\n      0.350438\n      0.459105\n      0.404418\n      0.650814\n      0.569050\n      0.356026\n    \n    \n      4\n      0.699862\n      0.507726\n      0.816830\n      0.468293\n      1.000000\n      0.489525\n      0.967349\n      0.709197\n      0.438397\n      0.606478\n      ...\n      0.733388\n      0.561951\n      0.447214\n      0.588968\n      0.742392\n      0.676123\n      0.486834\n      0.489979\n      0.721117\n      0.614919\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.503721\n      0.514929\n      0.792913\n      0.459105\n      0.676123\n      0.335847\n      0.611577\n      0.826380\n      0.819761\n      0.589453\n      ...\n      0.500921\n      0.538260\n      0.037796\n      0.690654\n      0.722806\n      1.000000\n      0.622541\n      0.542105\n      0.311211\n      0.774827\n    \n    \n      96\n      0.430077\n      0.132647\n      0.391239\n      0.404418\n      0.486834\n      0.394961\n      0.325482\n      0.459922\n      0.709876\n      0.731717\n      ...\n      0.593057\n      0.475786\n      0.340777\n      0.729291\n      0.452563\n      0.622541\n      1.000000\n      0.067884\n      0.491035\n      0.823545\n    \n    \n      97\n      0.434429\n      0.907222\n      0.729769\n      0.650814\n      0.489979\n      0.277054\n      0.483494\n      0.532327\n      0.395437\n      0.303915\n      ...\n      0.024001\n      0.525657\n      0.000000\n      0.236113\n      0.428571\n      0.542105\n      0.067884\n      1.000000\n      0.196827\n      0.298807\n    \n    \n      98\n      0.461384\n      0.272428\n      0.515625\n      0.569050\n      0.721117\n      0.620299\n      0.624518\n      0.500069\n      0.105939\n      0.739759\n      ...\n      0.644826\n      0.465586\n      0.658710\n      0.365978\n      0.311643\n      0.311211\n      0.491035\n      0.196827\n      1.000000\n      0.391109\n    \n    \n      99\n      0.666361\n      0.350325\n      0.626224\n      0.356026\n      0.614919\n      0.386334\n      0.556215\n      0.485866\n      0.900638\n      0.644160\n      ...\n      0.542173\n      0.628281\n      0.350000\n      0.518558\n      0.657376\n      0.774827\n      0.823545\n      0.298807\n      0.391109\n      1.000000\n    \n  \n\n100 rows × 100 columns\n\n\n\n\nimport seaborn as sns\n\nsns.heatmap(sim_matrix, cmap='Greys')\n\n<AxesSubplot:>\n\n\n\n\n\n\n# Find the most similar users to user u \n\ndef k_nearest_neighbors(A, u, k):\n    \"\"\"Find the k nearest neighbors for user u\"\"\"\n    # Find the index of the user in the matrix\n    u_index = A.index.get_loc(u)\n    \n    # Compute the similarity between the user and all other users\n    sim_matrix = cosine_similarity(A)\n\n    # Find the k most similar users\n    k_nearest = np.argsort(sim_matrix[u_index])[::-1][1:k+1]\n    \n    # Return the user ids\n    return A.index[k_nearest]\n\n\nk_nearest_neighbors(A, 0, 5)\n\nInt64Index([20, 91, 65, 12, 2], dtype='int64', name='user_id')\n\n\n\n# Show matrix of movie ratings for u and k nearest neighbors\n\ndef show_neighbors(A, u, k):\n    \"\"\"Show the movie ratings for user u and k nearest neighbors\"\"\"\n    # Get the user ids of the k nearest neighbors\n    neighbors = k_nearest_neighbors(A, u, k)\n    \n    # Get the movie ratings for user u and the k nearest neighbors\n    df = A.loc[[u] + list(neighbors)]\n    \n    # Return the dataframe\n    return df\n\n\nshow_neighbors(A, 0, 5)\n\n\n\n\n\n  \n    \n      movie_id\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n    \n      user_id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      4.0\n      3.0\n      5.0\n      5.0\n      0.0\n      0.0\n      0.0\n      5.0\n      3.0\n      0.0\n    \n    \n      20\n      4.0\n      2.0\n      3.0\n      4.0\n      3.0\n      0.0\n      2.0\n      5.0\n      4.0\n      0.0\n    \n    \n      91\n      2.0\n      3.0\n      3.0\n      3.0\n      0.0\n      3.0\n      0.0\n      4.0\n      1.0\n      0.0\n    \n    \n      65\n      2.0\n      0.0\n      3.0\n      2.0\n      2.0\n      2.0\n      0.0\n      4.0\n      1.0\n      0.0\n    \n    \n      12\n      1.0\n      1.0\n      4.0\n      2.0\n      0.0\n      1.0\n      3.0\n      5.0\n      5.0\n      0.0\n    \n    \n      2\n      3.0\n      0.0\n      5.0\n      4.0\n      1.0\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n    \n  \n\n\n\n\n\n# Rating for user u for movie 0 is: (4.0 + 3.0) / 2 = 3.5 (Discard 0s)\n\ndef predict_rating(A, u, m, k=5):\n    \"\"\"Predict the rating for user u for movie m\"\"\"\n    # Get the user ids of the k nearest neighbors\n    neighbors = k_nearest_neighbors(A, u, k)\n    \n    # Get the movie ratings for user u and the k nearest neighbors\n    df = A.loc[[u] + list(neighbors)]\n    \n    # Get the ratings for movie m\n    ratings = df[m]\n    \n    # Calculate the mean of the ratings\n    mean = ratings[1:][ratings != 0].mean()\n    \n    # Return the mean\n    return mean\n\n\npredict_rating(A, 0, 5)\n\n2.25\n\n\n\n# Now working with real data\n\n# Load the data\n\ndf = pd.read_excel(\"mov-rec.xlsx\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Timestamp\n      Your name\n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n  \n  \n    \n      0\n      2023-04-11 10:58:44.990\n      Nipun\n      4.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      4.0\n      5.0\n      4.0\n      5.0\n    \n    \n      1\n      2023-04-11 10:59:49.617\n      Gautam Vashishtha\n      3.0\n      4.0\n      4.0\n      5.0\n      3.0\n      1.0\n      5.0\n      5.0\n      4.0\n      3.0\n    \n    \n      2\n      2023-04-11 11:12:44.033\n      Eshan Gujarathi\n      4.0\n      NaN\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      NaN\n      4.0\n    \n    \n      3\n      2023-04-11 11:13:48.674\n      Sai Krishna Avula\n      5.0\n      3.0\n      3.0\n      4.0\n      4.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n    \n      4\n      2023-04-11 11:13:55.658\n      Ankit Yadav\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n  \n\n\n\n\n\n# Discard the timestamp column\n\ndf = df.drop('Timestamp', axis=1)\n\n# Make the \"Your Name\" column the index\n\ndf = df.set_index('Your name')\ndf\n\n\n\n\n\n  \n    \n      \n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n    \n      Your name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Nipun\n      4.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      4.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Gautam Vashishtha\n      3.0\n      4.0\n      4.0\n      5.0\n      3.0\n      1.0\n      5.0\n      5.0\n      4.0\n      3.0\n    \n    \n      Eshan Gujarathi\n      4.0\n      NaN\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      NaN\n      4.0\n    \n    \n      Sai Krishna Avula\n      5.0\n      3.0\n      3.0\n      4.0\n      4.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n    \n      Ankit Yadav\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n    \n      Dhruv\n      NaN\n      NaN\n      5.0\n      5.0\n      3.0\n      NaN\n      5.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Saatvik Rao\n      4.0\n      3.0\n      4.0\n      5.0\n      2.0\n      2.0\n      4.0\n      5.0\n      3.0\n      5.0\n    \n    \n      Zeel B Patel\n      5.0\n      4.0\n      5.0\n      4.0\n      4.0\n      4.0\n      NaN\n      2.0\n      NaN\n      5.0\n    \n    \n      Neel\n      4.0\n      NaN\n      5.0\n      5.0\n      3.0\n      3.0\n      5.0\n      5.0\n      NaN\n      4.0\n    \n    \n      Sachin Jalan\n      4.0\n      NaN\n      5.0\n      5.0\n      3.0\n      4.0\n      4.0\n      5.0\n      NaN\n      3.0\n    \n    \n      Ayush Shrivastava\n      5.0\n      4.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n      4.0\n      NaN\n      4.0\n    \n    \n      ....\n      4.0\n      4.0\n      NaN\n      4.0\n      4.0\n      4.0\n      NaN\n      4.0\n      NaN\n      4.0\n    \n    \n      Hari Hara Sudhan\n      4.0\n      3.0\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n      5.0\n      3.0\n      5.0\n    \n    \n      Etikikota Hrushikesh\n      NaN\n      NaN\n      1.0\n      1.0\n      1.0\n      2.0\n      1.0\n      1.0\n      NaN\n      NaN\n    \n    \n      Chirag\n      5.0\n      3.0\n      4.0\n      5.0\n      5.0\n      2.0\n      3.0\n      4.0\n      2.0\n      5.0\n    \n    \n      Aaryan Darad\n      5.0\n      4.0\n      4.0\n      5.0\n      3.0\n      4.0\n      3.0\n      3.0\n      NaN\n      4.0\n    \n    \n      Hetvi Patel\n      4.0\n      3.0\n      2.0\n      5.0\n      4.0\n      4.0\n      5.0\n      3.0\n      3.0\n      5.0\n    \n    \n      Kalash Kankaria\n      4.0\n      NaN\n      4.0\n      5.0\n      3.0\n      4.0\n      NaN\n      NaN\n      NaN\n      3.0\n    \n    \n      Rachit Verma\n      NaN\n      NaN\n      4.0\n      5.0\n      3.0\n      5.0\n      5.0\n      5.0\n      NaN\n      4.0\n    \n    \n      shriraj\n      3.0\n      2.0\n      5.0\n      4.0\n      2.0\n      3.0\n      4.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Bhavini Korthi\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      Hitarth Gandhi\n      3.0\n      NaN\n      4.0\n      5.0\n      3.0\n      4.0\n      5.0\n      5.0\n      NaN\n      NaN\n    \n    \n      Radhika Joglekar\n      3.0\n      3.0\n      3.0\n      4.0\n      5.0\n      5.0\n      2.0\n      1.0\n      2.0\n      5.0\n    \n    \n      Medhansh Singh\n      4.0\n      3.0\n      5.0\n      5.0\n      3.0\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n    \n    \n      Arun Mani\n      NaN\n      NaN\n      4.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      4.0\n      NaN\n    \n    \n      Satyam\n      3.0\n      5.0\n      5.0\n      5.0\n      4.0\n      3.0\n      5.0\n      5.0\n      NaN\n      5.0\n    \n    \n      Karan Kumar\n      4.0\n      3.0\n      5.0\n      4.0\n      5.0\n      5.0\n      3.0\n      5.0\n      5.0\n      4.0\n    \n    \n      R Yeeshu Dhurandhar\n      5.0\n      NaN\n      4.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      NaN\n      NaN\n    \n    \n      Satyam Gupta\n      5.0\n      5.0\n      NaN\n      5.0\n      4.0\n      4.0\n      NaN\n      4.0\n      NaN\n      2.0\n    \n    \n      rushali\n      NaN\n      NaN\n      NaN\n      5.0\n      4.0\n      3.0\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      shridhar\n      5.0\n      4.0\n      5.0\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n      3.0\n      3.0\n    \n    \n      Tanvi Jain\n      4.0\n      3.0\n      NaN\n      NaN\n      4.0\n      5.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      Manish Prabhubhai Salvi\n      4.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      NaN\n      4.0\n      NaN\n      5.0\n    \n    \n      Varun Barala\n      5.0\n      5.0\n      5.0\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n      3.0\n      4.0\n    \n    \n      Kevin Shah\n      3.0\n      4.0\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      4.0\n      3.0\n      5.0\n    \n    \n      Inderjeet\n      4.0\n      NaN\n      4.0\n      5.0\n      4.0\n      3.0\n      5.0\n      5.0\n      NaN\n      3.0\n    \n    \n      Gangaram Siddam\n      4.0\n      4.0\n      3.0\n      3.0\n      5.0\n      5.0\n      4.0\n      4.0\n      3.0\n      5.0\n    \n    \n      Aditi\n      4.0\n      4.0\n      NaN\n      5.0\n      1.0\n      3.0\n      5.0\n      4.0\n      NaN\n      4.0\n    \n    \n      Madhuri Awachar\n      5.0\n      4.0\n      5.0\n      4.0\n      5.0\n      3.0\n      5.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Anupam\n      5.0\n      5.0\n      NaN\n      5.0\n      5.0\n      5.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      Jinay\n      3.0\n      1.0\n      4.0\n      3.0\n      4.0\n      3.0\n      5.0\n      5.0\n      4.0\n      3.0\n    \n    \n      Shrutimoy\n      5.0\n      5.0\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      NaN\n      2.0\n    \n    \n      Aadesh Desai\n      4.0\n      4.0\n      3.0\n      5.0\n      3.0\n      3.0\n      5.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Dhairya\n      5.0\n      4.0\n      4.0\n      5.0\n      3.0\n      5.0\n      NaN\n      4.0\n      NaN\n      4.0\n    \n    \n      Rahul C\n      3.0\n      3.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      5.0\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.index\n\nIndex(['Nipun', 'Gautam Vashishtha', 'Eshan Gujarathi', 'Sai Krishna Avula',\n       'Ankit Yadav ', 'Dhruv', 'Saatvik Rao ', 'Zeel B Patel', 'Neel ',\n       'Sachin Jalan ', 'Ayush Shrivastava', '....', 'Hari Hara Sudhan',\n       'Etikikota Hrushikesh', 'Chirag', 'Aaryan Darad', 'Hetvi Patel',\n       'Kalash Kankaria', 'Rachit Verma', 'shriraj', 'Bhavini Korthi ',\n       'Hitarth Gandhi ', 'Radhika Joglekar ', 'Medhansh Singh', 'Arun Mani',\n       'Satyam ', 'Karan Kumar ', 'R Yeeshu Dhurandhar', 'Satyam Gupta',\n       'rushali', 'shridhar', 'Tanvi Jain ', 'Manish Prabhubhai Salvi ',\n       'Varun Barala', 'Kevin Shah ', 'Inderjeet', 'Gangaram Siddam ', 'Aditi',\n       'Madhuri Awachar', 'Anupam', 'Jinay', 'Shrutimoy', 'Aadesh Desai',\n       'Dhairya', 'Rahul C'],\n      dtype='object', name='Your name')\n\n\n\n# Get index for user and movie\nuser = 'Rahul C'\n\nprint(user in df.index)\n\n# Get the movie ratings for user\nuser_ratings = df.loc[user]\nuser_ratings\n\nTrue\n\n\nSholay                      3.0\nSwades (We The People)      3.0\nThe Matrix (I)              4.0\nInterstellar                4.0\nDangal                      4.0\nTaare Zameen Par            4.0\nShawshank Redemption        4.0\nThe Dark Knight             5.0\nNotting Hill                NaN\nUri: The Surgical Strike    NaN\nName: Rahul C, dtype: float64\n\n\n\ndf_copy = df.copy()\ndf_copy.fillna(0, inplace=True)\nshow_neighbors(df_copy, user, 5)\n\n\n\n\n\n  \n    \n      \n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n    \n      Your name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Rahul C\n      3.0\n      3.0\n      4.0\n      4.0\n      4.0\n      4.0\n      4.0\n      5.0\n      0.0\n      0.0\n    \n    \n      Shrutimoy\n      5.0\n      5.0\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      0.0\n      2.0\n    \n    \n      Hitarth Gandhi\n      3.0\n      0.0\n      4.0\n      5.0\n      3.0\n      4.0\n      5.0\n      5.0\n      0.0\n      0.0\n    \n    \n      R Yeeshu Dhurandhar\n      5.0\n      0.0\n      4.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      0.0\n      0.0\n    \n    \n      shridhar\n      5.0\n      4.0\n      5.0\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n      3.0\n      3.0\n    \n    \n      Sachin Jalan\n      4.0\n      0.0\n      5.0\n      5.0\n      3.0\n      4.0\n      4.0\n      5.0\n      0.0\n      3.0\n    \n  \n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n  \n  \n    \n      count\n      39.000000\n      32.000000\n      38.000000\n      43.000000\n      45.000000\n      44.000000\n      35.000000\n      40.000000\n      21.000000\n      39.000000\n    \n    \n      mean\n      4.102564\n      3.718750\n      4.131579\n      4.581395\n      3.644444\n      3.977273\n      4.400000\n      4.250000\n      3.476190\n      4.230769\n    \n    \n      std\n      0.753758\n      0.958304\n      0.991070\n      0.793802\n      1.003529\n      1.067242\n      0.976187\n      1.080123\n      0.813575\n      0.902089\n    \n    \n      min\n      3.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      2.000000\n      2.000000\n    \n    \n      25%\n      4.000000\n      3.000000\n      4.000000\n      4.000000\n      3.000000\n      3.000000\n      4.000000\n      4.000000\n      3.000000\n      4.000000\n    \n    \n      50%\n      4.000000\n      4.000000\n      4.000000\n      5.000000\n      4.000000\n      4.000000\n      5.000000\n      5.000000\n      3.000000\n      4.000000\n    \n    \n      75%\n      5.000000\n      4.000000\n      5.000000\n      5.000000\n      4.000000\n      5.000000\n      5.000000\n      5.000000\n      4.000000\n      5.000000\n    \n    \n      max\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n      5.000000\n    \n  \n\n\n\n\n\n# Predict the rating for user u for movie m\n\npredict_rating(df_copy, user, 'The Dark Knight')\n\n4.8\n\n\n\npredict_rating(df_copy, user, 'Sholay')\n\n4.4\n\n\n\n# Generic Matrix Factorization (without missing values)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n# D is a matrix of size (n_users, n_movies) randomly generated values between 1 and 5\nD = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nD\n\ntensor([[4., 4., 2., 5., 2., 3., 5., 5., 3., 2.],\n        [5., 3., 5., 1., 2., 3., 5., 5., 2., 3.],\n        [1., 3., 4., 3., 1., 2., 5., 1., 4., 3.],\n        [3., 5., 1., 2., 2., 2., 4., 2., 1., 1.],\n        [2., 5., 3., 1., 2., 5., 4., 2., 4., 5.],\n        [3., 5., 1., 1., 3., 1., 5., 3., 3., 3.],\n        [1., 2., 5., 1., 1., 4., 3., 4., 4., 5.],\n        [2., 5., 1., 4., 3., 1., 5., 3., 3., 1.],\n        [4., 3., 2., 2., 4., 5., 5., 5., 3., 3.],\n        [1., 2., 5., 1., 2., 1., 3., 5., 3., 3.],\n        [3., 2., 3., 3., 2., 1., 5., 3., 1., 5.],\n        [5., 5., 5., 3., 5., 4., 4., 2., 4., 5.],\n        [5., 2., 2., 1., 4., 3., 3., 5., 3., 3.],\n        [4., 4., 5., 4., 1., 1., 1., 1., 3., 1.],\n        [2., 5., 5., 2., 4., 5., 1., 1., 4., 4.],\n        [3., 2., 1., 2., 1., 1., 2., 1., 2., 2.],\n        [5., 5., 3., 3., 3., 3., 4., 1., 4., 3.],\n        [5., 2., 4., 4., 2., 1., 4., 4., 5., 5.],\n        [3., 3., 4., 2., 2., 3., 4., 1., 3., 4.],\n        [3., 5., 3., 5., 5., 4., 1., 3., 1., 4.],\n        [2., 1., 2., 4., 5., 2., 2., 1., 1., 1.],\n        [2., 4., 3., 1., 4., 4., 2., 4., 1., 4.],\n        [5., 3., 2., 5., 1., 4., 3., 5., 3., 3.],\n        [5., 3., 3., 5., 5., 4., 3., 1., 1., 4.],\n        [4., 4., 1., 5., 2., 5., 5., 4., 5., 5.],\n        [3., 1., 2., 3., 3., 5., 4., 4., 4., 3.],\n        [4., 1., 2., 2., 4., 5., 4., 3., 4., 4.],\n        [5., 2., 4., 2., 3., 1., 2., 4., 2., 1.],\n        [5., 3., 1., 3., 2., 4., 1., 2., 1., 5.],\n        [1., 2., 1., 3., 3., 2., 2., 5., 2., 4.],\n        [3., 2., 4., 5., 4., 2., 2., 3., 5., 1.],\n        [5., 5., 3., 3., 1., 3., 3., 4., 5., 4.],\n        [5., 1., 3., 5., 4., 2., 3., 4., 3., 2.],\n        [3., 2., 2., 3., 5., 5., 1., 1., 4., 1.],\n        [3., 4., 4., 3., 5., 1., 5., 5., 4., 1.],\n        [2., 5., 1., 5., 4., 2., 4., 1., 4., 5.],\n        [5., 5., 2., 4., 2., 4., 4., 1., 2., 1.],\n        [2., 5., 5., 1., 1., 4., 2., 4., 4., 5.],\n        [4., 2., 2., 3., 3., 3., 3., 2., 2., 2.],\n        [5., 2., 1., 4., 3., 1., 3., 1., 4., 5.],\n        [4., 1., 3., 3., 5., 1., 1., 2., 4., 1.],\n        [4., 4., 1., 1., 4., 2., 2., 3., 1., 3.],\n        [3., 5., 4., 3., 1., 1., 4., 1., 3., 1.],\n        [1., 2., 4., 1., 4., 2., 4., 1., 2., 5.],\n        [4., 4., 1., 1., 5., 5., 3., 3., 3., 3.],\n        [1., 1., 1., 5., 5., 2., 5., 2., 1., 4.],\n        [3., 4., 4., 5., 2., 2., 3., 2., 5., 5.],\n        [1., 2., 5., 3., 2., 1., 3., 1., 2., 5.],\n        [5., 4., 4., 2., 4., 5., 4., 2., 3., 2.],\n        [4., 2., 4., 5., 3., 2., 1., 4., 3., 3.],\n        [4., 2., 3., 2., 1., 4., 5., 2., 1., 1.],\n        [2., 3., 3., 5., 3., 4., 1., 5., 3., 2.],\n        [3., 5., 5., 3., 2., 5., 4., 4., 4., 5.],\n        [5., 3., 5., 5., 2., 5., 3., 3., 4., 3.],\n        [4., 3., 1., 2., 5., 3., 3., 4., 4., 5.],\n        [2., 2., 1., 1., 3., 5., 1., 2., 3., 5.],\n        [1., 3., 5., 1., 3., 5., 3., 4., 4., 2.],\n        [4., 5., 5., 3., 4., 1., 3., 3., 2., 5.],\n        [1., 4., 4., 5., 2., 1., 3., 1., 1., 5.],\n        [1., 2., 3., 3., 5., 4., 3., 5., 1., 4.],\n        [1., 1., 5., 2., 1., 3., 5., 5., 4., 5.],\n        [5., 1., 4., 4., 2., 3., 5., 4., 3., 4.],\n        [4., 3., 2., 4., 3., 3., 4., 4., 5., 2.],\n        [2., 5., 1., 2., 1., 1., 3., 2., 1., 4.],\n        [5., 3., 3., 3., 2., 3., 1., 2., 2., 5.],\n        [2., 2., 3., 5., 4., 4., 1., 2., 1., 2.],\n        [5., 5., 4., 4., 4., 3., 2., 4., 2., 2.],\n        [2., 5., 3., 5., 5., 2., 3., 4., 1., 3.],\n        [4., 1., 4., 2., 4., 1., 4., 3., 4., 2.],\n        [3., 3., 2., 1., 5., 2., 2., 2., 2., 5.],\n        [2., 5., 2., 5., 4., 5., 4., 3., 4., 3.],\n        [3., 1., 5., 4., 3., 2., 5., 3., 4., 1.],\n        [5., 3., 1., 3., 2., 3., 1., 1., 1., 1.],\n        [1., 4., 5., 2., 4., 5., 3., 2., 4., 5.],\n        [4., 1., 4., 5., 3., 2., 3., 5., 5., 4.],\n        [4., 2., 4., 3., 4., 5., 5., 5., 1., 5.],\n        [3., 2., 1., 3., 2., 2., 4., 5., 5., 4.],\n        [5., 5., 5., 1., 4., 1., 4., 4., 1., 3.],\n        [5., 5., 5., 2., 2., 3., 2., 5., 3., 4.],\n        [2., 4., 4., 4., 4., 5., 3., 1., 3., 2.],\n        [5., 1., 2., 2., 1., 5., 4., 3., 2., 3.],\n        [5., 1., 2., 5., 3., 5., 2., 4., 5., 4.],\n        [2., 5., 5., 1., 4., 2., 2., 5., 4., 3.],\n        [1., 3., 1., 2., 4., 3., 2., 3., 4., 4.],\n        [2., 3., 5., 3., 3., 4., 4., 1., 5., 5.],\n        [1., 5., 2., 4., 1., 5., 2., 1., 1., 1.],\n        [4., 3., 5., 5., 3., 3., 3., 1., 1., 5.],\n        [2., 2., 4., 3., 5., 2., 5., 2., 4., 1.],\n        [2., 1., 1., 5., 1., 3., 2., 2., 3., 5.],\n        [2., 2., 1., 5., 5., 4., 4., 1., 3., 2.],\n        [5., 3., 3., 3., 3., 2., 1., 3., 4., 2.],\n        [1., 1., 5., 4., 5., 1., 2., 1., 5., 5.],\n        [3., 4., 4., 4., 5., 1., 5., 3., 2., 1.],\n        [5., 5., 3., 1., 5., 1., 1., 1., 1., 3.],\n        [3., 2., 4., 3., 2., 4., 2., 2., 3., 5.],\n        [4., 2., 4., 5., 4., 3., 1., 2., 4., 3.],\n        [1., 1., 2., 2., 4., 3., 5., 2., 4., 4.],\n        [5., 1., 3., 1., 1., 5., 2., 1., 4., 1.],\n        [1., 4., 1., 2., 5., 2., 2., 4., 1., 5.],\n        [5., 3., 4., 5., 5., 4., 5., 1., 4., 1.]])\n\n\n\nD.shape\n\ntorch.Size([100, 10])\n\n\nLet us decompose D as AB. A is of shape (n, k) and B is of shape (k, n). We can write the above equation as: D = AB\n\n# Randomly initialize A and B\n\nA = torch.randn(n_users, 2, requires_grad=True)\nB = torch.randn(n_movies, 2, requires_grad=True)\n\n# Compute the loss\n\nloss = torch.norm(torch.mm(A, B.t()) - D)\nloss\n\ntensor(120.8659, grad_fn=<LinalgVectorNormBackward0>)\n\n\n\npd.DataFrame(torch.mm(A, B.t()).detach().numpy())\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      -1.264860\n      -1.512566\n      -2.037402\n      -1.262417\n      -0.132023\n      -1.027238\n      3.666524\n      2.496095\n      -0.179421\n      -0.089113\n    \n    \n      1\n      -1.775146\n      -1.131623\n      -0.252141\n      -1.085573\n      -0.387802\n      0.962701\n      0.262229\n      0.092588\n      1.281358\n      0.974351\n    \n    \n      2\n      1.072980\n      0.693813\n      0.178206\n      0.662960\n      0.232401\n      -0.558108\n      -0.206829\n      -0.089713\n      -0.759340\n      -0.578063\n    \n    \n      3\n      -2.261358\n      -1.489118\n      -0.446266\n      -1.415824\n      -0.484307\n      1.111051\n      0.568306\n      0.281543\n      1.558778\n      1.188488\n    \n    \n      4\n      -2.327101\n      -1.577842\n      -0.578746\n      -1.488436\n      -0.489104\n      1.033144\n      0.808672\n      0.446056\n      1.533820\n      1.172646\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.050433\n      -0.142864\n      -0.453207\n      -0.090314\n      0.046777\n      -0.451902\n      0.854854\n      0.599582\n      -0.307123\n      -0.221812\n    \n    \n      96\n      3.034490\n      1.814688\n      0.116038\n      1.772819\n      0.687387\n      -1.936142\n      0.141715\n      0.253754\n      -2.375615\n      -1.798406\n    \n    \n      97\n      5.616947\n      4.195326\n      2.414580\n      3.860471\n      1.101509\n      -1.555234\n      -3.858034\n      -2.407849\n      -3.103768\n      -2.401302\n    \n    \n      98\n      -1.941730\n      -1.688903\n      -1.462366\n      -1.499716\n      -0.332028\n      -0.041199\n      2.509351\n      1.653427\n      0.703850\n      0.565434\n    \n    \n      99\n      1.936188\n      1.340739\n      0.555041\n      1.257751\n      0.401233\n      -0.791799\n      -0.810526\n      -0.467291\n      -1.232934\n      -0.944662\n    \n  \n\n100 rows × 10 columns\n\n\n\n\npd.DataFrame(D)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      1.0\n      4.0\n      3.0\n      5.0\n      2.0\n      5.0\n      3.0\n      4.0\n      1.0\n      3.0\n    \n    \n      1\n      5.0\n      2.0\n      4.0\n      4.0\n      1.0\n      3.0\n      2.0\n      3.0\n      4.0\n      3.0\n    \n    \n      2\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      4.0\n      2.0\n      2.0\n      3.0\n    \n    \n      3\n      4.0\n      3.0\n      3.0\n      2.0\n      2.0\n      4.0\n      2.0\n      1.0\n      1.0\n      1.0\n    \n    \n      4\n      4.0\n      1.0\n      5.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      1.0\n      4.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      1.0\n      5.0\n      5.0\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n      2.0\n      4.0\n    \n    \n      96\n      3.0\n      2.0\n      3.0\n      4.0\n      3.0\n      3.0\n      5.0\n      2.0\n      3.0\n      2.0\n    \n    \n      97\n      1.0\n      3.0\n      2.0\n      3.0\n      5.0\n      3.0\n      5.0\n      1.0\n      2.0\n      3.0\n    \n    \n      98\n      4.0\n      3.0\n      2.0\n      1.0\n      2.0\n      3.0\n      3.0\n      1.0\n      4.0\n      2.0\n    \n    \n      99\n      5.0\n      2.0\n      4.0\n      5.0\n      2.0\n      5.0\n      2.0\n      5.0\n      2.0\n      3.0\n    \n  \n\n100 rows × 10 columns\n\n\n\n\n# Optimizer\n\noptimizer = optim.Adam([A, B], lr=0.01)\n\n# Train the model\n\nfor i in range(1000):\n    # Compute the loss\n    loss = torch.norm(torch.mm(A, B.t()) - D)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 10 == 0:\n        print(loss.item())\n\n115.35802459716797\n111.52149200439453\n108.49187469482422\n106.08045959472656\n103.93647766113281\n101.58767700195312\n98.55469512939453\n94.49311828613281\n89.3100814819336\n83.21151733398438\n76.62467956542969\n70.03575134277344\n63.89227294921875\n58.51639175415039\n54.00450897216797\n50.31562805175781\n47.390052795410156\n45.16101837158203\n43.53628158569336\n42.39887619018555\n41.624820709228516\n41.10096740722656\n40.736263275146484\n40.46616744995117\n40.25019073486328\n40.065284729003906\n39.89917755126953\n39.74546432495117\n39.60075759887695\n39.463157653808594\n39.331581115722656\n39.20536422729492\n39.08412170410156\n38.96763610839844\n38.855804443359375\n38.748600006103516\n38.64604568481445\n38.548213958740234\n38.45516586303711\n38.36697769165039\n38.28369903564453\n38.20534896850586\n38.131935119628906\n38.06340789794922\n37.99967956542969\n37.940650939941406\n37.88616180419922\n37.8360481262207\n37.790103912353516\n37.748111724853516\n37.70984649658203\n37.6750602722168\n37.64350128173828\n37.61494827270508\n37.589141845703125\n37.56585693359375\n37.54487609863281\n37.5259895324707\n37.50899124145508\n37.49371337890625\n37.479976654052734\n37.467628479003906\n37.45652389526367\n37.44654083251953\n37.437557220458984\n37.42947006225586\n37.422183990478516\n37.415611267089844\n37.40968322753906\n37.404327392578125\n37.39948272705078\n37.39509582519531\n37.391117095947266\n37.387508392333984\n37.38422393798828\n37.3812370300293\n37.37851333618164\n37.37602615356445\n37.37375259399414\n37.37166213989258\n37.369754791259766\n37.36799621582031\n37.36637878417969\n37.364891052246094\n37.36351013183594\n37.362239837646484\n37.361061096191406\n37.35996627807617\n37.35894775390625\n37.35799789428711\n37.357112884521484\n37.356292724609375\n37.35552215576172\n37.354793548583984\n37.354122161865234\n37.353485107421875\n37.35288619995117\n37.352325439453125\n37.3517951965332\n37.351295471191406\n\n\n\npd.DataFrame(torch.mm(A, B.t()).detach().numpy()).head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      2.760296\n      3.178874\n      2.978039\n      3.232967\n      2.928783\n      3.293471\n      3.438638\n      3.024349\n      3.257597\n      3.163118\n    \n    \n      1\n      4.100434\n      3.274340\n      3.823128\n      3.091235\n      2.616060\n      3.863670\n      1.691365\n      3.049109\n      3.051083\n      2.469662\n    \n  \n\n\n\n\n\npd.DataFrame(D).head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      1.0\n      4.0\n      3.0\n      5.0\n      2.0\n      5.0\n      3.0\n      4.0\n      1.0\n      3.0\n    \n    \n      1\n      5.0\n      2.0\n      4.0\n      4.0\n      1.0\n      3.0\n      2.0\n      3.0\n      4.0\n      3.0\n    \n  \n\n\n\n\n\ndef factorize(D, k):\n    \"\"\"Factorize the matrix D into A and B\"\"\"\n    # Randomly initialize A and B\n    A = torch.randn(D.shape[0], k, requires_grad=True)\n    B = torch.randn(D.shape[1], k, requires_grad=True)\n    \n    # Optimizer\n    optimizer = optim.Adam([A, B], lr=0.01)\n    \n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        loss = torch.norm(torch.mm(A, B.t()) - D)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return A, B, loss\n\n\nfor k in [1, 2, 3, 4, 5, 6, 9]:\n    A, B, loss = factorize(D, k)\n    print(k, loss.item())\n\n1 41.327362060546875\n2 37.35048294067383\n\n\nKeyboardInterrupt: \n\n\n\npd.DataFrame(torch.mm(A, B.t()).detach().numpy()).head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      2.793340\n      3.182317\n      3.004699\n      3.226319\n      2.927514\n      3.308313\n      3.400554\n      3.022366\n      3.250936\n      3.144955\n    \n    \n      1\n      4.119824\n      3.272779\n      3.811543\n      3.107022\n      2.594617\n      3.870888\n      1.656050\n      3.066180\n      3.058107\n      2.471609\n    \n  \n\n\n\n\n\npd.DataFrame(D).head(2)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      1.0\n      4.0\n      3.0\n      5.0\n      2.0\n      5.0\n      3.0\n      4.0\n      1.0\n      3.0\n    \n    \n      1\n      5.0\n      2.0\n      4.0\n      4.0\n      1.0\n      3.0\n      2.0\n      3.0\n      4.0\n      3.0\n    \n  \n\n\n\n\n\n# With missing values\n\n# Randomly replace some entries with NaN\n\nD = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nD[torch.rand(D.shape) < 0.5] = float('nan')\nD\n\ntensor([[nan, nan, nan, 4., 4., 2., nan, 3., nan, 1.],\n        [nan, nan, 3., nan, 2., nan, nan, 2., 4., 4.],\n        [nan, 5., 5., nan, nan, nan, 1., 2., nan, nan],\n        [5., 3., 5., nan, nan, nan, 4., 3., 4., nan],\n        [nan, nan, 5., 3., 4., nan, nan, 3., 5., nan],\n        [nan, 5., nan, 2., 1., 4., nan, 5., nan, nan],\n        [nan, nan, nan, nan, 4., nan, nan, nan, nan, 4.],\n        [4., 3., nan, nan, 3., nan, 1., nan, nan, 3.],\n        [2., 4., 5., nan, 3., nan, 2., 2., nan, nan],\n        [nan, 2., 1., nan, 2., nan, nan, nan, 3., 1.],\n        [nan, nan, nan, nan, 1., 4., nan, 5., nan, 5.],\n        [nan, nan, 4., 3., nan, nan, nan, nan, nan, 3.],\n        [nan, 5., 2., 2., 4., nan, nan, 3., nan, nan],\n        [5., nan, nan, 1., 4., 1., 3., nan, nan, nan],\n        [nan, nan, nan, nan, 4., 5., nan, 1., 3., 1.],\n        [1., nan, nan, 5., nan, nan, 4., 2., nan, nan],\n        [4., nan, nan, nan, nan, nan, 1., nan, 4., nan],\n        [1., nan, 4., nan, nan, nan, 5., nan, nan, 2.],\n        [nan, 2., 1., nan, nan, 4., 2., 2., nan, 3.],\n        [nan, nan, 3., nan, 3., 1., nan, 1., nan, nan],\n        [nan, 5., nan, 2., 2., nan, 1., 3., 2., 3.],\n        [3., nan, nan, 5., nan, 5., 1., 2., nan, nan],\n        [nan, 4., nan, 3., 1., nan, nan, 4., 1., nan],\n        [nan, nan, nan, 5., 3., nan, nan, nan, nan, 4.],\n        [nan, 1., 5., nan, 3., nan, nan, 2., nan, nan],\n        [nan, nan, 2., nan, 5., nan, nan, 1., 4., 4.],\n        [nan, nan, nan, 5., 2., nan, nan, nan, nan, nan],\n        [2., 5., 1., nan, nan, nan, nan, 1., 4., nan],\n        [nan, 5., nan, nan, 2., 5., nan, 2., 3., nan],\n        [1., nan, nan, nan, 5., nan, 2., nan, 1., nan],\n        [nan, nan, 5., nan, 3., nan, 2., nan, 2., nan],\n        [nan, nan, 3., 4., 1., nan, 5., nan, nan, nan],\n        [nan, 2., nan, 1., 1., nan, nan, 3., nan, nan],\n        [nan, nan, nan, nan, 2., nan, 1., 3., nan, nan],\n        [4., nan, nan, 3., nan, 4., 5., 3., 1., nan],\n        [4., 1., 5., nan, nan, nan, nan, 5., nan, 3.],\n        [nan, 4., nan, 3., nan, nan, 3., 4., nan, nan],\n        [2., 4., nan, 3., 4., 4., 1., nan, nan, nan],\n        [nan, 2., nan, 5., nan, nan, 2., 1., 4., 1.],\n        [2., nan, 5., 5., 4., nan, nan, nan, 5., 2.],\n        [nan, nan, nan, nan, nan, 4., 2., 4., 4., nan],\n        [5., nan, nan, 3., 1., nan, 1., 2., 1., 4.],\n        [3., nan, nan, 2., 5., 4., nan, nan, nan, nan],\n        [nan, 3., nan, nan, 2., 5., nan, nan, 2., nan],\n        [1., 1., nan, 4., 5., 2., nan, 5., 5., nan],\n        [nan, 4., nan, nan, 1., nan, nan, nan, 3., 4.],\n        [4., nan, 5., 2., 5., nan, 1., nan, 1., 1.],\n        [1., 4., 1., 4., 2., 1., 2., nan, 2., nan],\n        [1., nan, 5., 3., nan, 5., 2., nan, nan, 4.],\n        [4., 4., 5., nan, 2., nan, nan, nan, nan, 3.],\n        [nan, 4., 5., nan, nan, nan, 3., 1., nan, nan],\n        [nan, nan, 5., 1., 5., nan, 2., nan, 2., 2.],\n        [4., nan, 5., 1., nan, 2., 5., nan, nan, nan],\n        [nan, nan, 2., nan, 4., 2., 3., nan, 3., 5.],\n        [4., nan, nan, nan, nan, nan, 4., nan, 3., 2.],\n        [3., 3., nan, nan, 5., 2., nan, nan, nan, 4.],\n        [5., nan, nan, nan, 3., nan, nan, nan, 1., nan],\n        [nan, nan, 4., nan, nan, nan, nan, 4., nan, 4.],\n        [5., 2., 2., 2., 5., nan, nan, 3., nan, nan],\n        [2., nan, nan, 2., 1., 1., nan, 3., 2., 5.],\n        [5., nan, nan, nan, 4., 5., nan, nan, 2., 2.],\n        [nan, nan, 3., 1., nan, nan, 2., nan, nan, 2.],\n        [nan, 2., 3., nan, 4., 3., 2., 1., nan, 1.],\n        [4., nan, 4., 3., 2., nan, 2., nan, 2., 5.],\n        [nan, 1., 3., 5., 5., 4., nan, nan, 3., nan],\n        [5., nan, 5., nan, nan, 3., nan, 5., nan, 1.],\n        [nan, nan, 5., nan, nan, 2., 3., nan, nan, nan],\n        [nan, nan, 4., 4., 1., 4., nan, nan, nan, 5.],\n        [nan, nan, 1., nan, nan, 4., nan, nan, nan, 1.],\n        [nan, nan, nan, nan, 1., nan, 3., 3., nan, nan],\n        [nan, nan, 1., 1., nan, 5., 5., nan, 1., 5.],\n        [nan, nan, nan, 3., nan, 4., 1., nan, nan, nan],\n        [nan, nan, 3., 2., nan, nan, nan, nan, 2., 4.],\n        [1., 5., nan, 5., nan, 4., 4., nan, nan, 3.],\n        [nan, nan, nan, 2., nan, 5., 5., 2., nan, nan],\n        [nan, 5., 5., 2., nan, 3., 5., nan, nan, nan],\n        [1., 4., nan, nan, nan, nan, nan, 1., 5., nan],\n        [nan, nan, nan, nan, nan, 5., 2., 5., 5., 1.],\n        [5., nan, 2., 3., nan, 4., nan, nan, 5., 3.],\n        [3., nan, nan, nan, 5., 2., nan, nan, nan, nan],\n        [nan, nan, nan, nan, 1., 1., nan, 1., 3., nan],\n        [4., 2., 4., 4., 1., nan, 2., nan, 4., 4.],\n        [1., 1., nan, nan, nan, nan, 2., nan, 5., nan],\n        [2., 5., nan, 5., 3., 5., nan, nan, nan, nan],\n        [3., nan, nan, 1., nan, 3., 4., 5., 3., 4.],\n        [5., 2., nan, nan, 3., 1., 2., nan, 1., nan],\n        [1., nan, nan, nan, 1., nan, 2., nan, nan, 2.],\n        [1., 5., 1., 5., 5., 2., nan, 1., nan, 3.],\n        [nan, nan, nan, nan, 1., 4., 1., 2., nan, nan],\n        [2., nan, 4., nan, 5., nan, 2., nan, nan, 5.],\n        [nan, nan, 5., nan, nan, nan, nan, nan, nan, nan],\n        [3., 5., nan, nan, nan, nan, 5., nan, 4., 4.],\n        [2., 2., nan, nan, 3., 5., nan, nan, 4., nan],\n        [2., nan, 3., nan, 5., nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, 2., 5., 4., 3., nan, 3.],\n        [nan, 3., nan, nan, nan, 3., 3., 5., nan, nan],\n        [nan, 4., nan, nan, nan, 5., nan, nan, nan, nan],\n        [2., 5., nan, nan, nan, 3., 2., 4., nan, nan],\n        [1., 4., nan, 4., 5., 3., nan, nan, 1., nan],\n        [4., nan, nan, 5., 4., 1., 5., nan, 4., nan]])\n\n\n\nA, B, loss = factorize(D, 2)\nloss\n\ntensor(nan, grad_fn=<LinalgVectorNormBackward0>)\n\n\n\nmask = ~torch.isnan(D)\nmask\n\ntensor([[False, False, False,  True,  True,  True, False,  True, False,  True],\n        [False, False,  True, False,  True, False, False,  True,  True,  True],\n        [False,  True,  True, False, False, False,  True,  True, False, False],\n        [ True,  True,  True, False, False, False,  True,  True,  True, False],\n        [False, False,  True,  True,  True, False, False,  True,  True, False],\n        [False,  True, False,  True,  True,  True, False,  True, False, False],\n        [False, False, False, False,  True, False, False, False, False,  True],\n        [ True,  True, False, False,  True, False,  True, False, False,  True],\n        [ True,  True,  True, False,  True, False,  True,  True, False, False],\n        [False,  True,  True, False,  True, False, False, False,  True,  True],\n        [False, False, False, False,  True,  True, False,  True, False,  True],\n        [False, False,  True,  True, False, False, False, False, False,  True],\n        [False,  True,  True,  True,  True, False, False,  True, False, False],\n        [ True, False, False,  True,  True,  True,  True, False, False, False],\n        [False, False, False, False,  True,  True, False,  True,  True,  True],\n        [ True, False, False,  True, False, False,  True,  True, False, False],\n        [ True, False, False, False, False, False,  True, False,  True, False],\n        [ True, False,  True, False, False, False,  True, False, False,  True],\n        [False,  True,  True, False, False,  True,  True,  True, False,  True],\n        [False, False,  True, False,  True,  True, False,  True, False, False],\n        [False,  True, False,  True,  True, False,  True,  True,  True,  True],\n        [ True, False, False,  True, False,  True,  True,  True, False, False],\n        [False,  True, False,  True,  True, False, False,  True,  True, False],\n        [False, False, False,  True,  True, False, False, False, False,  True],\n        [False,  True,  True, False,  True, False, False,  True, False, False],\n        [False, False,  True, False,  True, False, False,  True,  True,  True],\n        [False, False, False,  True,  True, False, False, False, False, False],\n        [ True,  True,  True, False, False, False, False,  True,  True, False],\n        [False,  True, False, False,  True,  True, False,  True,  True, False],\n        [ True, False, False, False,  True, False,  True, False,  True, False],\n        [False, False,  True, False,  True, False,  True, False,  True, False],\n        [False, False,  True,  True,  True, False,  True, False, False, False],\n        [False,  True, False,  True,  True, False, False,  True, False, False],\n        [False, False, False, False,  True, False,  True,  True, False, False],\n        [ True, False, False,  True, False,  True,  True,  True,  True, False],\n        [ True,  True,  True, False, False, False, False,  True, False,  True],\n        [False,  True, False,  True, False, False,  True,  True, False, False],\n        [ True,  True, False,  True,  True,  True,  True, False, False, False],\n        [False,  True, False,  True, False, False,  True,  True,  True,  True],\n        [ True, False,  True,  True,  True, False, False, False,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True, False],\n        [ True, False, False,  True,  True, False,  True,  True,  True,  True],\n        [ True, False, False,  True,  True,  True, False, False, False, False],\n        [False,  True, False, False,  True,  True, False, False,  True, False],\n        [ True,  True, False,  True,  True,  True, False,  True,  True, False],\n        [False,  True, False, False,  True, False, False, False,  True,  True],\n        [ True, False,  True,  True,  True, False,  True, False,  True,  True],\n        [ True,  True,  True,  True,  True,  True,  True, False,  True, False],\n        [ True, False,  True,  True, False,  True,  True, False, False,  True],\n        [ True,  True,  True, False,  True, False, False, False, False,  True],\n        [False,  True,  True, False, False, False,  True,  True, False, False],\n        [False, False,  True,  True,  True, False,  True, False,  True,  True],\n        [ True, False,  True,  True, False,  True,  True, False, False, False],\n        [False, False,  True, False,  True,  True,  True, False,  True,  True],\n        [ True, False, False, False, False, False,  True, False,  True,  True],\n        [ True,  True, False, False,  True,  True, False, False, False,  True],\n        [ True, False, False, False,  True, False, False, False,  True, False],\n        [False, False,  True, False, False, False, False,  True, False,  True],\n        [ True,  True,  True,  True,  True, False, False,  True, False, False],\n        [ True, False, False,  True,  True,  True, False,  True,  True,  True],\n        [ True, False, False, False,  True,  True, False, False,  True,  True],\n        [False, False,  True,  True, False, False,  True, False, False,  True],\n        [False,  True,  True, False,  True,  True,  True,  True, False,  True],\n        [ True, False,  True,  True,  True, False,  True, False,  True,  True],\n        [False,  True,  True,  True,  True,  True, False, False,  True, False],\n        [ True, False,  True, False, False,  True, False,  True, False,  True],\n        [False, False,  True, False, False,  True,  True, False, False, False],\n        [False, False,  True,  True,  True,  True, False, False, False,  True],\n        [False, False,  True, False, False,  True, False, False, False,  True],\n        [False, False, False, False,  True, False,  True,  True, False, False],\n        [False, False,  True,  True, False,  True,  True, False,  True,  True],\n        [False, False, False,  True, False,  True,  True, False, False, False],\n        [False, False,  True,  True, False, False, False, False,  True,  True],\n        [ True,  True, False,  True, False,  True,  True, False, False,  True],\n        [False, False, False,  True, False,  True,  True,  True, False, False],\n        [False,  True,  True,  True, False,  True,  True, False, False, False],\n        [ True,  True, False, False, False, False, False,  True,  True, False],\n        [False, False, False, False, False,  True,  True,  True,  True,  True],\n        [ True, False,  True,  True, False,  True, False, False,  True,  True],\n        [ True, False, False, False,  True,  True, False, False, False, False],\n        [False, False, False, False,  True,  True, False,  True,  True, False],\n        [ True,  True,  True,  True,  True, False,  True, False,  True,  True],\n        [ True,  True, False, False, False, False,  True, False,  True, False],\n        [ True,  True, False,  True,  True,  True, False, False, False, False],\n        [ True, False, False,  True, False,  True,  True,  True,  True,  True],\n        [ True,  True, False, False,  True,  True,  True, False,  True, False],\n        [ True, False, False, False,  True, False,  True, False, False,  True],\n        [ True,  True,  True,  True,  True,  True, False,  True, False,  True],\n        [False, False, False, False,  True,  True,  True,  True, False, False],\n        [ True, False,  True, False,  True, False,  True, False, False,  True],\n        [False, False,  True, False, False, False, False, False, False, False],\n        [ True,  True, False, False, False, False,  True, False,  True,  True],\n        [ True,  True, False, False,  True,  True, False, False,  True, False],\n        [ True, False,  True, False,  True, False, False, False, False, False],\n        [False, False, False, False,  True,  True,  True,  True, False,  True],\n        [False,  True, False, False, False,  True,  True,  True, False, False],\n        [False,  True, False, False, False,  True, False, False, False, False],\n        [ True,  True, False, False, False,  True,  True,  True, False, False],\n        [ True,  True, False,  True,  True,  True, False, False,  True, False],\n        [ True, False, False,  True,  True,  True,  True, False,  True, False]])\n\n\n\nmask.sum()\n\ntensor(522)\n\n\n\nA = torch.randn(D.shape[0], k, requires_grad=True)\nB = torch.randn(D.shape[1], k, requires_grad=True)\n\ndiff_matrix = torch.mm(A, B.t())-D\ndiff_matrix.shape\n\ntorch.Size([100, 10])\n\n\n\n# Mask the matrix\ndiff_matrix[mask].shape\n\ntorch.Size([522])\n\n\n\n# Modify the loss function to ignore NaN values\n\ndef factorize(D, k):\n    \"\"\"Factorize the matrix D into A and B\"\"\"\n    # Randomly initialize A and B\n    A = torch.randn(D.shape[0], k, requires_grad=True)\n    B = torch.randn(D.shape[1], k, requires_grad=True)\n    \n    # Optimizer\n    optimizer = optim.Adam([A, B], lr=0.01)\n    \n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        diff_matrix = torch.mm(A, B.t())-D\n        diff_vector = diff_matrix[mask]\n        loss = torch.norm(diff_vector)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return A, B, loss\n\n\nA, B, loss = factorize(D, 5)\nloss\n\ntensor(8.3795, grad_fn=<LinalgVectorNormBackward0>)\n\n\n\ntorch.mm(A, B.t())\n\ntensor([[ 1.0002e+00, -6.1860e-01,  1.6844e+00,  9.9991e-01, -2.2450e+00,\n         -4.0025e-01, -9.3873e-01,  4.5819e+00,  8.8804e-01,  2.8812e-01],\n        [ 3.4045e+00,  4.3689e+00,  3.4459e+00,  7.1197e+00,  2.2094e+00,\n          3.5827e+00,  6.9298e-01,  3.9435e+00,  3.7188e+00,  3.8646e+00],\n        [ 1.8525e+00,  3.5934e+00,  2.3619e+00,  6.9679e+00,  2.2771e+00,\n          3.0910e+00,  6.2187e-01,  7.6504e-01,  2.4307e+00,  4.5831e+00],\n        [ 2.5042e+00,  2.7107e+00,  2.6498e+00,  1.0070e+00,  3.5360e+00,\n          4.0030e+00,  2.0012e+00,  2.9972e+00,  2.6560e+00,  3.0001e+00],\n        [-2.0035e+00, -7.5789e-01,  4.0035e+00, -2.6083e+00,  2.0150e+00,\n         -3.2294e+00,  3.9778e+00,  1.5164e+00,  2.0601e+00,  1.9979e+00],\n        [ 3.3313e+00,  5.0005e+00,  2.9979e+00,  1.9324e+00,  6.2719e+00,\n          5.0010e+00,  3.4474e+00,  1.9747e+00,  3.9212e+00,  3.0002e+00],\n        [ 2.4918e+00, -8.6924e-01,  2.9974e+00,  3.9993e+00, -3.6196e+00,\n          3.3461e+00, -2.6180e+00,  7.8752e+00,  1.0047e+00,  4.7184e+00],\n        [ 4.0137e+00,  5.0133e+00, -3.1814e-01,  5.7423e-01,  1.9839e+00,\n         -1.9341e+00,  1.0108e+00,  2.9967e+00,  3.5993e+00, -8.8937e+00],\n        [ 1.8884e+00,  3.0018e+00,  1.9986e+00,  3.6214e+00,  1.3445e+00,\n         -8.3828e-02,  9.5663e-01,  2.3653e+00,  2.7723e+00, -3.0290e-01],\n        [ 3.0752e+00,  9.4665e-01,  4.2899e+00, -4.5431e+00,  2.8226e+00,\n          2.9628e+00,  3.0004e+00,  8.1094e+00,  3.4849e+00,  1.0188e+00],\n        [ 2.9752e+00,  3.0103e+00, -1.8524e+00,  5.1349e-01,  9.8893e-01,\n          1.0100e+00, -6.7139e-01,  6.4309e-01,  1.0123e+00, -5.1665e+00],\n        [ 2.4924e+00,  1.6909e+00,  6.5299e+00,  2.1961e+00,  1.5871e+00,\n          1.9187e+00,  2.5060e+00,  8.5766e+00,  4.5608e+00,  4.9586e+00],\n        [ 3.7808e+00,  5.4975e+00,  4.4300e+00,  7.1002e+00,  3.5630e+00,\n          2.9987e+00,  1.9959e+00,  4.5226e+00,  5.0039e+00,  3.2930e+00],\n        [ 4.1509e+00,  2.9077e+00,  4.4567e+00,  1.6394e+00,  1.6263e+00,\n          2.7120e+00,  1.4758e+00,  8.4248e+00,  4.3598e+00,  1.2115e+00],\n        [ 1.3388e+00,  2.8519e+00,  3.6300e+00, -2.3746e-01,  5.0226e+00,\n          2.0026e+00,  3.9996e+00,  1.9982e+00,  3.3924e+00,  2.7614e+00],\n        [ 2.0062e+00,  3.4421e+00,  3.3045e+00,  3.3703e+00,  4.1875e+00,\n          3.7794e+00,  2.4669e+00,  1.8015e+00,  3.0830e+00,  4.6786e+00],\n        [ 1.9932e+00,  5.4306e+00,  2.6564e+00,  3.9896e+00,  2.9713e+00,\n         -4.4742e+00,  3.0714e+00,  1.9850e+00,  5.0565e+00, -5.1447e+00],\n        [ 9.7504e-01,  5.0209e+00,  5.1434e+00,  7.9740e+00,  4.0114e+00,\n         -8.0306e-01,  3.5290e+00,  1.2679e+00,  4.9996e+00,  4.0012e+00],\n        [ 2.9039e+00,  4.2371e+00,  5.1783e+00,  1.4304e+01, -6.2558e-01,\n          2.9950e+00, -1.1585e+00,  4.9986e+00,  4.0041e+00,  8.1423e+00],\n        [ 2.8765e+00,  3.6266e+00,  3.5521e+00,  3.5769e+00,  3.3101e+00,\n          3.4339e+00,  1.9543e+00,  3.7925e+00,  3.5855e+00,  3.3947e+00],\n        [ 3.1678e+00,  4.4892e+00,  4.0685e+00,  4.2476e+00,  3.7637e+00,\n          2.5998e+00,  2.4894e+00,  4.1890e+00,  4.4521e+00,  2.5758e+00],\n        [ 4.6850e+00,  5.1159e+00, -1.6040e+00,  2.1407e+00,  2.4447e+00,\n          2.7884e+00, -4.1521e-01,  1.3369e+00,  2.1819e+00, -5.2731e+00],\n        [ 4.3464e+00,  4.2727e+00,  1.2831e+00,  9.1247e-01,  4.8015e+00,\n          6.7635e+00,  1.5292e+00,  2.7263e+00,  2.6805e+00,  1.8472e+00],\n        [ 5.0499e+00,  3.7718e+00,  3.2828e+00,  1.8760e+00,  2.4281e+00,\n          4.7329e+00,  9.9891e-01,  7.4524e+00,  4.0032e+00,  1.0007e+00],\n        [ 3.1683e+00, -4.8069e+00,  4.1563e+00, -2.2212e+01,  3.1175e+00,\n          4.8951e+00,  4.9131e+00,  1.3957e+01,  1.7452e+00, -2.0775e+00],\n        [ 4.9331e+00,  9.8587e+00,  2.0322e+00,  6.3511e+00,  1.1053e+00,\n         -1.3336e+01,  2.8553e+00,  5.8145e+00,  8.9757e+00, -1.9562e+01],\n        [ 9.5013e-01,  2.2910e+00,  4.5609e+00,  2.4319e+00,  3.0765e+00,\n          8.9028e-01,  3.0615e+00,  3.1779e+00,  3.5110e+00,  4.0518e+00],\n        [ 1.5621e+00,  3.5200e-01,  2.2187e+00, -6.1495e-01,  2.0550e+00,\n          5.0016e+00,  1.0059e+00,  3.0014e+00,  9.9065e-01,  5.0016e+00],\n        [ 1.0132e+00,  9.9005e-01,  2.9984e+00,  3.0045e+00,  2.0013e+00,\n          4.6599e+00,  9.4810e-01,  1.9943e+00,  1.3488e+00,  7.2230e+00],\n        [ 1.0710e+00,  3.9048e+00,  7.0985e+00,  9.8046e+00,  2.7144e+00,\n          9.9892e-01,  2.8371e+00,  3.9444e+00,  5.1068e+00,  8.5642e+00],\n        [ 2.9798e+00,  6.6238e+00,  3.9980e+00,  1.9979e+00,  4.8557e+00,\n         -4.9116e+00,  4.9889e+00,  4.0151e+00,  6.8644e+00, -6.7965e+00],\n        [ 2.6444e+00,  5.0009e+00,  1.9984e+00,  8.4371e+00,  3.1761e+00,\n          4.0003e+00,  5.9719e-01,  1.1233e-01,  2.8453e+00,  4.3426e+00],\n        [ 4.9523e+00,  5.0456e+00,  9.8821e-01,  1.1904e+00,  2.4383e+00,\n          1.0191e+00,  1.0017e+00,  5.0290e+00,  4.0249e+00, -5.5541e+00],\n        [ 1.2343e+00,  2.6635e+00,  6.2834e+00,  2.4751e+00,  2.9971e+00,\n         -8.9346e-01,  3.9848e+00,  5.6039e+00,  5.0170e+00,  3.1984e+00],\n        [ 7.3295e+00,  4.9896e+00,  3.0382e+00,  2.0068e+00,  4.8380e+00,\n          1.1694e+01,  7.3273e-01,  7.9437e+00,  3.8515e+00,  4.9996e+00],\n        [-8.1543e-01,  3.9929e+00,  1.9948e+00,  2.0048e+00,  7.6522e+00,\n          2.0035e+00,  5.0092e+00, -5.3936e+00,  2.2828e+00,  4.6745e+00],\n        [ 2.4622e+00,  5.9762e+00,  5.4107e+00,  9.1516e+00,  4.0038e+00,\n          6.4204e-01,  3.0174e+00,  2.9985e+00,  5.6616e+00,  3.9994e+00],\n        [ 5.2110e+00,  5.8611e+00,  2.4634e+00,  5.0137e+00,  3.8820e+00,\n          5.0464e+00,  1.1397e+00,  4.4904e+00,  4.2950e+00,  9.7386e-01],\n        [ 4.5880e-01,  1.0057e+00,  1.2943e+00, -5.0455e+00,  1.2572e+00,\n         -5.5712e+00,  2.9910e+00,  2.9993e+00,  2.6805e+00, -7.6303e+00],\n        [ 1.7154e+00,  2.7432e+00,  2.2357e+00,  2.1176e+00,  4.0797e+00,\n          4.5664e+00,  2.0245e+00,  7.0967e-01,  2.0460e+00,  4.6899e+00],\n        [ 2.9886e+00,  1.6741e+00,  9.7626e-01, -4.9624e+00,  3.2646e+00,\n          2.9619e+00,  2.0996e+00,  4.0057e+00,  2.0244e+00, -2.1930e+00],\n        [-2.9729e+00,  3.1319e-01,  3.7236e+00,  3.3254e+00,  1.0136e+00,\n         -5.0190e+00,  2.9824e+00, -1.2713e+00,  1.9980e+00,  2.9977e+00],\n        [ 1.5646e+00,  2.5216e+00,  3.5026e+00,  1.0177e+00,  4.0812e+00,\n          2.9980e+00,  2.9738e+00,  2.3976e+00,  2.9533e+00,  3.9814e+00],\n        [ 1.1377e+00, -7.9534e-02,  3.6606e+00, -1.7493e+00,  1.0072e+00,\n          1.0560e+00,  1.8801e+00,  5.4316e+00,  2.2340e+00,  2.2851e+00],\n        [ 2.7395e+00,  4.2341e+00,  5.3765e+00,  8.3817e+00,  2.5022e+00,\n          3.1207e+00,  1.6137e+00,  4.6686e+00,  4.5147e+00,  6.4826e+00],\n        [ 5.0039e+00,  5.2292e+00,  5.3792e+00,  4.9980e+00,  3.9950e+00,\n          5.0192e+00,  2.3338e+00,  7.4932e+00,  5.5430e+00,  4.0016e+00],\n        [ 4.1034e+00,  4.9254e+00,  2.9669e+00,  3.8318e+00,  4.4845e+00,\n          5.2453e+00,  1.9372e+00,  3.6188e+00,  3.8836e+00,  2.9346e+00],\n        [ 2.7524e+00,  1.9983e+00,  3.0011e+00, -3.2032e+00,  3.8101e+00,\n          3.0059e+00,  3.0024e+00,  4.9954e+00,  3.0806e+00,  5.4398e-01],\n        [ 1.7170e+00,  3.1087e+00,  4.7616e+00,  3.3576e+00,  2.0605e+00,\n         -1.3493e+00,  2.7543e+00,  4.8920e+00,  4.5341e+00,  9.0290e-01],\n        [ 4.0465e+00,  2.5043e+00,  9.9836e-01,  1.0001e+00,  9.9855e-01,\n          4.0019e+00, -3.8940e-01,  5.0006e+00,  2.0672e+00, -4.7615e-01],\n        [ 4.3963e+00,  4.6748e+00, -2.2637e-01,  1.9547e+01, -5.7685e+00,\n          2.9123e+00, -7.2256e+00,  2.8164e+00,  1.2482e+00,  3.0466e+00],\n        [ 1.8831e+00,  3.2841e+00,  4.0538e+00,  4.7681e+00,  2.8000e+00,\n          1.9749e+00,  2.0825e+00,  3.1375e+00,  3.5871e+00,  4.1210e+00],\n        [ 5.2524e+00,  2.9996e+00,  4.9944e+00,  2.0011e+00,  1.5771e+00,\n          5.0016e+00,  1.0100e+00,  1.0275e+01,  4.5709e+00,  2.8167e+00],\n        [ 4.0196e+00,  9.7914e-01,  7.6036e+00, -8.5030e+00,  5.0027e+00,\n          3.6480e+00,  5.8612e+00,  1.2776e+01,  5.7694e+00,  1.9990e+00],\n        [ 6.9952e-01,  4.8144e-01,  4.6375e+00, -1.1169e+00,  1.9977e+00,\n          2.8582e-01,  2.9994e+00,  5.0373e+00,  3.0041e+00,  2.9873e+00],\n        [ 3.0164e+00,  2.7781e+00,  1.4268e+00,  1.0004e+00,  3.1666e+00,\n          4.9979e+00,  1.0501e+00,  2.4987e+00,  1.9833e+00,  2.2796e+00],\n        [ 5.7559e+00,  4.9924e+00,  9.6477e-01,  7.2380e-01,  5.0034e+00,\n          8.2238e+00,  1.1047e+00,  4.0024e+00,  2.9928e+00,  9.9998e-01],\n        [ 1.9086e+00,  4.3765e+00,  3.6471e+00, -1.8230e-02,  7.1279e+00,\n          3.0536e+00,  5.0289e+00,  1.0045e+00,  4.0867e+00,  2.8153e+00],\n        [ 8.0839e-01,  3.2813e+00,  2.8462e+00,  4.1885e+00,  4.5646e+00,\n          3.3629e+00,  2.6508e+00, -7.9362e-01,  2.4387e+00,  5.7568e+00],\n        [ 2.1391e+00,  2.7862e+00,  2.0157e+00,  3.7075e+00,  1.4357e+00,\n          1.5206e+00,  6.5361e-01,  2.5565e+00,  2.4614e+00,  1.2083e+00],\n        [ 3.8541e+00,  3.7919e+00,  1.1576e+00, -4.1922e+00,  4.8565e+00,\n          2.1025e+00,  3.1566e+00,  3.8267e+00,  3.4599e+00, -4.4144e+00],\n        [ 4.4749e+00,  2.6216e+00,  5.2107e+00,  8.0413e-01,  2.6066e+00,\n          5.2284e+00,  1.9287e+00,  9.1641e+00,  4.3544e+00,  3.9514e+00],\n        [-6.2355e-01,  2.9661e+00,  1.4762e+00, -1.5889e+00,  7.0237e+00,\n          1.1637e+00,  5.0129e+00, -3.9864e+00,  1.9879e+00,  1.9999e+00],\n        [-2.0532e+00,  9.4436e-01,  1.0788e+00, -1.5112e+00,  5.2766e+00,\n          1.0112e+00,  3.9423e+00, -4.9127e+00,  4.3571e-01,  3.9691e+00],\n        [ 2.5509e+00,  3.9951e+00,  4.4605e+00,  7.9626e+00,  1.9209e+00,\n          2.3589e+00,  1.1325e+00,  3.9963e+00,  4.0115e+00,  4.9989e+00],\n        [ 4.4588e+00,  4.1568e+00,  6.3874e-01,  4.7680e+00,  1.7098e+00,\n          5.1372e+00, -8.1905e-01,  3.2126e+00,  2.2777e+00,  8.1924e-01],\n        [ 4.0590e+00,  4.8974e+00,  2.2936e+00,  5.0399e+00,  3.7705e+00,\n          5.3666e+00,  1.0653e+00,  2.9172e+00,  3.3719e+00,  2.9769e+00],\n        [ 1.9058e+00,  2.3902e+00,  1.9088e+00, -2.7564e+00,  5.0644e+00,\n          3.5114e+00,  3.3203e+00,  1.5425e+00,  2.3176e+00,  1.2948e+00],\n        [ 4.5764e+00,  2.9666e+00,  5.4040e+00, -3.4611e+00,  4.9879e+00,\n          4.9381e+00,  4.0789e+00,  9.1713e+00,  5.0415e+00,  2.0002e+00],\n        [ 8.1217e-01,  9.9974e-01,  3.0004e+00,  2.8712e+00,  6.2371e-01,\n          1.0238e+00,  8.6280e-01,  2.9506e+00,  1.9404e+00,  3.5027e+00],\n        [ 8.6320e-01,  5.2823e-01,  6.2913e+00,  5.0601e+00,  1.5222e+00,\n          4.8221e+00,  1.6089e+00,  5.2303e+00,  2.7037e+00,  1.1437e+01],\n        [ 2.4092e+00,  2.3466e+00,  3.1749e+00,  4.7389e+00,  1.4796e+00,\n          4.0645e+00,  4.3717e-01,  3.7633e+00,  2.4378e+00,  5.1463e+00],\n        [ 2.9657e+00,  1.9680e+00,  4.9938e+00,  2.0152e+00,  5.0274e+00,\n          1.0372e+01,  2.3406e+00,  4.5921e+00,  2.5287e+00,  1.1962e+01],\n        [ 9.9360e-01,  2.8139e+00,  4.4274e+00,  4.0010e+00,  4.0218e+00,\n          2.9971e+00,  2.9780e+00,  1.7462e+00,  3.2108e+00,  6.5915e+00],\n        [ 2.0912e+00,  3.0938e+00,  4.2010e+00,  4.0979e+00,  3.1748e+00,\n          3.1397e+00,  2.1953e+00,  3.4810e+00,  3.4843e+00,  5.0422e+00],\n        [ 9.0708e-01,  2.1028e+00,  3.8487e+00,  4.9650e+00,  1.5964e+00,\n          1.2887e+00,  1.4710e+00,  2.5548e+00,  2.7485e+00,  4.8925e+00],\n        [ 3.9220e+00,  3.0327e+00,  9.9923e-01,  4.9932e+00,  4.2979e-02,\n          3.9924e+00, -1.4696e+00,  4.1899e+00,  1.9725e+00,  1.0117e+00],\n        [-5.1639e-01,  2.2394e+00,  3.5183e+00,  4.6464e+00,  1.2360e+00,\n         -3.5196e+00,  2.3021e+00,  9.0593e-01,  3.1064e+00,  1.1263e+00],\n        [ 1.6037e+00,  5.1307e+00, -8.8829e-01,  3.0027e+00,  4.9017e+00,\n          1.0025e+00,  2.0058e+00, -3.8608e+00,  1.9919e+00, -2.1546e+00],\n        [ 2.9648e+00,  2.4034e+00,  1.2029e+00, -7.7679e+00,  4.9274e+00,\n          1.0164e+00,  3.9814e+00,  3.9833e+00,  3.0510e+00, -5.3062e+00],\n        [ 9.9679e-01,  4.1864e+00,  4.2270e+00, -4.6136e+00,  8.7020e+00,\n         -2.4003e-01,  7.5830e+00,  1.0008e+00,  5.0024e+00, -6.6820e-01],\n        [ 2.2728e+00,  3.6244e+00,  4.1272e+00,  2.2280e+00,  5.0801e+00,\n          4.0399e+00,  3.3965e+00,  2.7825e+00,  3.7393e+00,  4.8698e+00],\n        [ 1.6791e+00,  3.3230e+00,  1.9598e+00,  1.0041e-01,  4.8143e+00,\n          2.2202e+00,  3.1183e+00,  5.4728e-01,  2.7286e+00,  1.0427e+00],\n        [ 3.4507e+00,  1.9917e+00,  4.0081e+00,  4.7036e+00,  3.0002e+00,\n          1.0496e+01,  2.8058e-01,  4.6939e+00,  1.8953e+00,  1.1558e+01],\n        [ 1.5679e+00,  2.5501e+00,  2.5804e+00,  3.3099e+00,  1.7224e+00,\n          7.7095e-01,  1.3122e+00,  2.4241e+00,  2.6655e+00,  1.5801e+00],\n        [ 7.8105e-01,  1.4697e+00,  4.6599e+00,  3.0228e+00,  1.8672e+00,\n          1.1596e+00,  2.2261e+00,  3.7987e+00,  3.0326e+00,  5.1036e+00],\n        [ 3.7187e+00,  3.0509e+00,  2.2057e+00,  7.4038e-01,  2.7456e+00,\n          4.1141e+00,  1.2110e+00,  4.7131e+00,  2.9046e+00,  9.1481e-01],\n        [ 1.7544e+00,  4.5809e+00,  3.3347e+00,  4.7582e+00,  4.2718e+00,\n          9.5751e-01,  2.9658e+00,  1.0462e+00,  3.9560e+00,  2.1825e+00],\n        [ 2.8734e+00,  4.2801e+00,  4.8296e+00,  2.8850e+00,  4.4548e+00,\n          2.1835e+00,  3.4971e+00,  4.6453e+00,  4.8704e+00,  2.7518e+00],\n        [ 1.9752e+00,  3.5589e+00,  2.0709e+00,  2.9926e+00,  3.5946e+00,\n          2.4313e+00,  1.9568e+00,  9.7326e-01,  2.7168e+00,  1.9824e+00],\n        [ 1.1651e+00,  2.4674e+00,  2.7450e+00,  4.1450e+00,  2.2433e+00,\n          1.9527e+00,  1.4024e+00,  1.3823e+00,  2.3369e+00,  3.8748e+00],\n        [ 3.2635e+00,  4.4776e+00,  1.8077e+00,  2.3946e+00,  4.9019e+00,\n          4.8625e+00,  2.1158e+00,  1.4241e+00,  2.9863e+00,  2.1902e+00],\n        [ 3.7890e+00,  7.9927e+00,  1.9485e+00,  2.8839e+00,  7.8015e+00,\n          1.0679e+00,  4.6564e+00, -6.0809e-02,  5.5080e+00, -3.2856e+00],\n        [-4.0094e+00,  9.9961e-01,  3.7105e+00,  6.7202e+00,  1.0018e+00,\n         -6.3401e+00,  2.9366e+00, -3.7277e+00,  1.9999e+00,  3.9998e+00],\n        [ 4.9758e+00,  5.6941e+00,  1.3000e+00,  3.4613e+00,  3.1975e+00,\n          2.6950e+00,  9.7137e-01,  3.9934e+00,  4.0431e+00, -2.8305e+00],\n        [-1.2464e+00, -6.5883e+00,  1.1624e+00, -2.1746e+01,  4.2800e+00,\n          5.0436e+00,  4.9288e+00,  3.9156e+00, -2.0051e+00,  1.9364e+00],\n        [ 1.7446e+00,  2.9185e+00,  1.7926e+00,  9.9024e-01,  3.9916e+00,\n          2.9798e+00,  2.2638e+00,  7.4184e-01,  2.2586e+00,  2.0701e+00],\n        [ 3.9972e+00,  6.4872e+00,  2.9988e+00,  1.2683e+01,  8.3564e-01,\n          1.0029e+00, -5.6647e-01,  3.3638e+00,  4.6541e+00,  1.3542e+00],\n        [ 3.6938e+00,  5.4303e+00,  4.3394e+00,  1.9919e+00,  5.0434e+00,\n          9.9244e-01,  3.9476e+00,  5.0027e+00,  5.6507e+00, -7.1759e-01],\n        [ 3.2622e+00,  5.0049e+00,  9.9177e-01,  3.6759e+00,  5.8119e+00,\n          6.9085e+00,  1.8074e+00, -7.1690e-01,  2.3350e+00,  4.0045e+00]],\n       grad_fn=<MmBackward0>)\n\n\n\n# Now use matrix factorization to predict the ratings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Create a class for the model\n\nclass MatrixFactorization(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors=20):\n        super().__init__()\n        self.user_factors = nn.Embedding(n_users, n_factors)\n        self.movie_factors = nn.Embedding(n_movies, n_factors)\n\n    def forward(self, user, movie):\n        return (self.user_factors(user) * self.movie_factors(movie)).sum(1)      \n\n\nmodel = MatrixFactorization(n_users, n_movies, 2)\nmodel\n\nMatrixFactorization(\n  (user_factors): Embedding(100, 2)\n  (movie_factors): Embedding(10, 2)\n)\n\n\n\nmodel(torch.tensor([0]), torch.tensor([2]))\n\ntensor([0.7200], grad_fn=<SumBackward1>)\n\n\n\nD[0, 2]\n\ntensor(4.)\n\n\n\ntype(D)\n\ntorch.Tensor\n\n\n\nmask = ~torch.isnan(D)\n\n# Get the indices of the non-NaN values\ni, j = torch.where(mask)\n\n# Get the values of the non-NaN values\nv = D[mask]\n\n# Store in PyTorch tensors\nusers = i.to(torch.int64)\nmovies = j.to(torch.int64)\nratings = v.to(torch.float32)\n\n\npd.DataFrame({'user': users, 'movie': movies, 'rating': ratings})\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n    \n  \n  \n    \n      0\n      0\n      3\n      4.0\n    \n    \n      1\n      0\n      4\n      4.0\n    \n    \n      2\n      0\n      5\n      2.0\n    \n    \n      3\n      0\n      7\n      3.0\n    \n    \n      4\n      0\n      9\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      475\n      99\n      3\n      5.0\n    \n    \n      476\n      99\n      4\n      4.0\n    \n    \n      477\n      99\n      5\n      1.0\n    \n    \n      478\n      99\n      6\n      5.0\n    \n    \n      479\n      99\n      8\n      4.0\n    \n  \n\n480 rows × 3 columns\n\n\n\n\n# Fit the Matrix Factorization model\nmodel = MatrixFactorization(n_users, n_movies, 4)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor i in range(1000):\n    # Compute the loss\n    pred = model(users, movies)\n    loss = F.mse_loss(pred, ratings)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 100 == 0:\n        print(loss.item())\n\n14.76720142364502\n5.302879810333252\n1.1853851079940796\n0.8891506195068359\n0.7703914642333984\n0.6836224794387817\n0.6128221154212952\n0.5464457869529724\n0.4790089726448059\n0.4133472144603729\n\n\n\nmodel(users, movies)\n\ntensor([3.5504, 4.5726, 2.0893, 2.4884, 1.2936, 2.9384, 2.1470, 2.1019, 3.5093,\n        4.2946, 4.8612, 4.9779, 0.7723, 2.4426, 4.9019, 2.7154, 4.6238, 4.5157,\n        3.9727, 3.1138, 4.8520, 4.0885, 3.1867, 3.5673, 4.2483, 4.7854, 1.5278,\n        1.8672, 4.9511, 3.6045, 4.0314, 3.9938, 4.2236, 2.8767, 3.1728, 1.5280,\n        2.2486, 1.5754, 4.0558, 4.6967, 2.4291, 2.3517, 2.8293, 1.9220, 0.9125,\n        2.4512, 1.9802, 1.6365, 0.8372, 3.9049, 5.2756, 4.9593, 3.9328, 2.9037,\n        3.1141, 5.3814, 2.1344, 2.6688, 3.4172, 2.3595, 5.3566, 2.2460, 2.8877,\n        1.1667, 2.3674, 4.3707, 4.4671, 1.6232, 2.4872, 0.9337, 0.9816, 5.0356,\n        3.9339, 2.0467, 4.0377, 0.9808, 3.9774, 0.9072, 4.0687, 3.5003, 3.4486,\n        2.3463, 1.1872, 3.4819, 2.4824, 1.8702, 2.5937, 3.0414, 3.1106, 1.0834,\n        0.7252, 4.9046, 1.6202, 2.4190, 1.8594, 3.1568, 1.8595, 2.2732, 2.4453,\n        4.7729, 4.4842, 0.7743, 3.3701, 4.0804, 1.6852, 2.0471, 3.1755, 1.9141,\n        5.1160, 2.9357, 3.9531, 0.9427, 4.9578, 2.9373, 2.1836, 1.9240, 4.4972,\n        1.4154, 4.5825, 3.6304, 4.9335, 2.0749, 1.7073, 4.5244, 0.6666, 2.7636,\n        3.1754, 4.6945, 2.2122, 4.2737, 3.8038, 2.1844, 1.1224, 4.6357, 1.5525,\n        1.7303, 5.2041, 2.9134, 1.4150, 2.5948, 3.0763, 4.0576, 1.0618, 4.8325,\n        2.1248, 1.1957, 0.8336, 2.7950, 1.9650, 0.9979, 3.0467, 3.4920, 1.8927,\n        3.9347, 4.4920, 3.7236, 2.5417, 3.5688, 1.8157, 5.0774, 3.3736, 4.3310,\n        4.1089, 3.1310, 3.1309, 3.6488, 1.5450, 4.5918, 3.0997, 3.6214, 3.4861,\n        1.5617, 1.8908, 5.0735, 1.0031, 1.2261, 3.8872, 1.8437, 1.5890, 4.8111,\n        4.6179, 4.7851, 3.6651, 3.3427, 3.9875, 2.0088, 3.9784, 3.9879, 4.6589,\n        1.5484, 1.6383, 2.2056, 2.2131, 2.1536, 2.7818, 3.1034, 1.9198, 5.0519,\n        3.9291, 3.1162, 1.7061, 4.9571, 2.2661, 1.9295, 1.0611, 5.2397, 4.2658,\n        2.6563, 3.1615, 4.7674, 3.9496, 1.2654, 2.2939, 4.4160, 4.0951, 5.0589,\n        2.1283, 4.8196, 0.9184, 1.1921, 0.8478, 1.4660, 2.9261, 0.6669, 2.8499,\n        3.0275, 2.2024, 1.6987, 2.3080, 1.1716, 5.1204, 3.2879, 4.8894, 2.8906,\n        2.7831, 3.9280, 4.1422, 4.8829, 1.7140, 3.2438, 3.3947, 4.9017, 2.0458,\n        2.7026, 4.9444, 1.6571, 4.5633, 2.0250, 1.5407, 2.2006, 4.2746, 5.2480,\n        1.4201, 1.7007, 4.3778, 2.2966, 3.9923, 1.9333, 3.4057, 4.0327, 3.5390,\n        3.9271, 2.9201, 2.7027, 3.3297, 3.0168, 3.0481, 5.1035, 1.8360, 3.9833,\n        5.0582, 2.9292, 1.0382, 4.0508, 3.8930, 4.0067, 4.8149, 2.3026, 1.7069,\n        3.4267, 3.1120, 3.6164, 2.3082, 1.9987, 0.6232, 1.3492, 2.3994, 2.7838,\n        4.6602, 5.0888, 3.9673, 4.9551, 2.0347, 1.9640, 2.9836, 0.9779, 2.0205,\n        2.0087, 1.7446, 2.7367, 3.7122, 3.4894, 1.6120, 1.1975, 1.4710, 3.9296,\n        3.9017, 2.2767, 2.3583, 3.4504, 2.5856, 3.6031, 1.2024, 3.0122, 5.1747,\n        4.5718, 3.8806, 3.2524, 5.3072, 4.7787, 3.7848, 3.5267, 1.6829, 5.0166,\n        1.9989, 2.9831, 4.7302, 3.1548, 2.7528, 3.1149, 4.3910, 1.0141, 3.9930,\n        0.9875, 0.9372, 3.0393, 3.0338, 1.8012, 1.3336, 4.6768, 3.6524, 2.4870,\n        4.3179, 3.0068, 3.9975, 1.0024, 3.1920, 2.1036, 2.4352, 3.3581, 1.4066,\n        4.5507, 5.2796, 4.4308, 3.1042, 3.2778, 1.9613, 4.6138, 4.9244, 2.5226,\n        4.9642, 4.7832, 1.7949, 3.0818, 5.2887, 1.0500, 3.8949, 1.2380, 4.7658,\n        5.2319, 1.2119, 4.4511, 4.9884, 2.0136, 5.0272, 1.8745, 3.6636, 3.9354,\n        3.5262, 3.8342, 3.0293, 5.0180, 1.9718, 1.0241, 0.8656, 1.3043, 2.8483,\n        3.4284, 2.0108, 3.6046, 2.7003, 2.3305, 3.7959, 2.9962, 3.9723, 1.0880,\n        0.9551, 1.8982, 5.0531, 2.0814, 4.6891, 4.0077, 4.0894, 5.0302, 3.6051,\n        1.6802, 3.5425, 4.1980, 3.5684, 2.3596, 4.1367, 4.4500, 2.7960, 1.9353,\n        0.4775, 2.1907, 2.0642, 0.9823, 1.0218, 1.8203, 2.1463, 1.0953, 3.8434,\n        0.4024, 4.7627, 4.9139, 3.1105, 2.5272, 2.4698, 1.0022, 4.0023, 1.0069,\n        1.9934, 2.0410, 4.0398, 5.1972, 3.4594, 3.5575, 5.0009, 2.9782, 4.9950,\n        4.4440, 4.0633, 4.4900, 2.1110, 2.1829, 3.1008, 4.4881, 3.7592, 1.9632,\n        3.0318, 5.0554, 1.9397, 5.0368, 3.6756, 2.9673, 3.3422, 3.5383, 3.0946,\n        3.8896, 3.3948, 4.0218, 4.9802, 2.3230, 5.1215, 3.2226, 2.2078, 3.0848,\n        0.5148, 4.5484, 3.6065, 4.3127, 2.7101, 2.3162, 4.0013, 4.6726, 3.9981,\n        0.9850, 4.5959, 4.6673], grad_fn=<SumBackward1>)\n\n\n\n# Now, let's predict the ratings for our df dataframe\n\nD = torch.from_numpy(df.values)\nD.shape\n\ntorch.Size([45, 10])\n\n\n\nmask = ~torch.isnan(D)\n\n# Get the indices of the non-NaN values\ni, j = torch.where(mask)\n\n# Get the values of the non-NaN values\nv = D[mask]\n\n# Store in PyTorch tensors\nusers = i.to(torch.int64)\nmovies = j.to(torch.int64)\nratings = v.to(torch.float32)\n\n\npd.DataFrame({'user': users, 'movie': movies, 'rating': ratings})\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n    \n  \n  \n    \n      0\n      0\n      0\n      4.0\n    \n    \n      1\n      0\n      1\n      5.0\n    \n    \n      2\n      0\n      2\n      4.0\n    \n    \n      3\n      0\n      3\n      4.0\n    \n    \n      4\n      0\n      4\n      5.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      371\n      44\n      3\n      4.0\n    \n    \n      372\n      44\n      4\n      4.0\n    \n    \n      373\n      44\n      5\n      4.0\n    \n    \n      374\n      44\n      6\n      4.0\n    \n    \n      375\n      44\n      7\n      5.0\n    \n  \n\n376 rows × 3 columns\n\n\n\n\n# Fit the Matrix Factorization model\nn_users = D.shape[0]\nn_movies = D.shape[1]\nmodel = MatrixFactorization(n_users, n_movies, 4)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor i in range(1000):\n    # Compute the loss\n    pred = model(users, movies)\n    loss = F.mse_loss(pred, ratings)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 100 == 0:\n        print(loss.item())\n\n21.0886173248291\n4.061213493347168\n0.7133367657661438\n0.4743851125240326\n0.38834860920906067\n0.3376193344593048\n0.3069866895675659\n0.2862914800643921\n0.27032995223999023\n0.2577725350856781\n\n\n\n# Now, let us predict the ratings for any user and movie from df for which we already have the ratings\n\nusername = 'Dhruv'\nmovie = 'The Dark Knight'\n\n# Get the user and movie indices\nuser_idx = df.index.get_loc(username)\nmovie_idx = df.columns.get_loc(movie)\n\n# Predict the rating\npred = model(torch.tensor([user_idx]), torch.tensor([movie_idx]))\npred.item(), df.loc[username, movie]\n\n(5.302472114562988, 5.0)\n\n\n\ndf.loc[username]\n\nSholay                      NaN\nSwades (We The People)      NaN\nThe Matrix (I)              5.0\nInterstellar                5.0\nDangal                      3.0\nTaare Zameen Par            NaN\nShawshank Redemption        5.0\nThe Dark Knight             5.0\nNotting Hill                4.0\nUri: The Surgical Strike    5.0\nName: Dhruv, dtype: float64\n\n\n\n# Now, let us predict the ratings for any user and movie from df for which we do not have the ratings\n\nusername = 'Dhruv'\nmovie = 'Sholay'\n\n# Get the user and movie indices\nuser_idx = df.index.get_loc(username)\nmovie_idx = df.columns.get_loc(movie)\n\n# Predict the rating\npred = model(torch.tensor([user_idx]), torch.tensor([movie_idx]))\npred, df.loc[username, movie]\n\n(tensor([4.1941], grad_fn=<SumBackward1>), nan)\n\n\n\n# Complete the matrix\nwith torch.no_grad():\n    completed_matrix = pd.DataFrame(model.user_factors.weight @ model.movie_factors.weight.t(), index=df.index, columns=df.columns)\n    # round to nearest integer\n    completed_matrix = completed_matrix.round()\n\n\ncompleted_matrix.head()\n\n\n\n\n\n  \n    \n      \n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n    \n      Your name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Nipun\n      4.0\n      4.0\n      4.0\n      5.0\n      5.0\n      5.0\n      4.0\n      4.0\n      4.0\n      5.0\n    \n    \n      Gautam Vashishtha\n      3.0\n      3.0\n      4.0\n      5.0\n      2.0\n      2.0\n      5.0\n      5.0\n      4.0\n      3.0\n    \n    \n      Eshan Gujarathi\n      4.0\n      4.0\n      5.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Sai Krishna Avula\n      4.0\n      4.0\n      3.0\n      4.0\n      4.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n    \n      Ankit Yadav\n      4.0\n      3.0\n      3.0\n      4.0\n      3.0\n      4.0\n      5.0\n      3.0\n      3.0\n      3.0\n    \n  \n\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Sholay\n      Swades (We The People)\n      The Matrix (I)\n      Interstellar\n      Dangal\n      Taare Zameen Par\n      Shawshank Redemption\n      The Dark Knight\n      Notting Hill\n      Uri: The Surgical Strike\n    \n    \n      Your name\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Nipun\n      4.0\n      5.0\n      4.0\n      4.0\n      5.0\n      5.0\n      4.0\n      5.0\n      4.0\n      5.0\n    \n    \n      Gautam Vashishtha\n      3.0\n      4.0\n      4.0\n      5.0\n      3.0\n      1.0\n      5.0\n      5.0\n      4.0\n      3.0\n    \n    \n      Eshan Gujarathi\n      4.0\n      NaN\n      5.0\n      5.0\n      4.0\n      5.0\n      5.0\n      5.0\n      NaN\n      4.0\n    \n    \n      Sai Krishna Avula\n      5.0\n      3.0\n      3.0\n      4.0\n      4.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0\n    \n    \n      Ankit Yadav\n      3.0\n      3.0\n      2.0\n      5.0\n      2.0\n      5.0\n      5.0\n      3.0\n      3.0\n      4.0"
  },
  {
    "objectID": "notebooks/posts/split.html",
    "href": "notebooks/posts/split.html",
    "title": "Dataset splitting for machine learning",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n# Create a dataset with 3 features and 1000 samples for a classification problem\n\ndf = pd.DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'])\ndf['D'] = np.random.randint(0, 2, 1000)\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.112981\n      -0.033889\n      0.348708\n      1\n    \n    \n      1\n      1.821601\n      1.267423\n      0.274915\n      1\n    \n    \n      2\n      0.148040\n      -1.412712\n      -0.271345\n      1\n    \n    \n      3\n      0.004766\n      -1.209944\n      0.122512\n      1\n    \n    \n      4\n      0.854442\n      -0.559497\n      -0.605376\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      0.161444\n      -0.969567\n      -2.079664\n      0\n    \n    \n      996\n      1.615938\n      0.219218\n      -0.322223\n      0\n    \n    \n      997\n      0.501970\n      -0.874313\n      1.571102\n      1\n    \n    \n      998\n      0.386063\n      -1.481215\n      -1.974313\n      1\n    \n    \n      999\n      0.468528\n      1.060850\n      0.612252\n      1\n    \n  \n\n1000 rows × 4 columns\n\n\n\n\nX, y = df[['A', 'B', 'C']], df['D']\n\n\ntrain_X, test_X = X[:800], X[800:]\ntrain_y, test_y = y[:800], y[800:]\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndt = DecisionTreeClassifier(max_depth=2)\ndt.fit(train_X, train_y)\n\nDecisionTreeClassifier(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2)\n\n\n\nfrom sklearn import tree\ntree.plot_tree(dt)\n\n[Text(0.5, 0.8333333333333334, 'X[0] <= -0.17\\ngini = 0.5\\nsamples = 800\\nvalue = [400, 400]'),\n Text(0.25, 0.5, 'X[0] <= -0.307\\ngini = 0.495\\nsamples = 348\\nvalue = [157, 191]'),\n Text(0.125, 0.16666666666666666, 'gini = 0.499\\nsamples = 316\\nvalue = [153, 163]'),\n Text(0.375, 0.16666666666666666, 'gini = 0.219\\nsamples = 32\\nvalue = [4, 28]'),\n Text(0.75, 0.5, 'X[2] <= 1.656\\ngini = 0.497\\nsamples = 452\\nvalue = [243, 209]'),\n Text(0.625, 0.16666666666666666, 'gini = 0.495\\nsamples = 434\\nvalue = [239, 195]'),\n Text(0.875, 0.16666666666666666, 'gini = 0.346\\nsamples = 18\\nvalue = [4, 14]')]\n\n\n\n\n\n\ny_hat = dt.predict(test_X)\npd.DataFrame({\"y_true\": test_y, \"y_hat\": y_hat})\n\n\n\n\n\n  \n    \n      \n      y_true\n      y_hat\n    \n  \n  \n    \n      800\n      0\n      1\n    \n    \n      801\n      0\n      0\n    \n    \n      802\n      1\n      1\n    \n    \n      803\n      1\n      1\n    \n    \n      804\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      995\n      0\n      1\n    \n    \n      996\n      0\n      0\n    \n    \n      997\n      1\n      0\n    \n    \n      998\n      1\n      1\n    \n    \n      999\n      1\n      1\n    \n  \n\n200 rows × 2 columns\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain_scores = {}\ntest_scores = {}\n\nfor i in range(1, 20):\n    dt = DecisionTreeClassifier(max_depth=i)\n    dt.fit(train_X, train_y)\n    train_scores[i] = dt.score(train_X, train_y)\n    \n    test_scores[i] = dt.score(test_X, test_y)\n\n\nscores_df = pd.DataFrame({'train': train_scores, 'test': test_scores})\nscores_df.plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "notebooks/posts/hyperparameter-1.html",
    "href": "notebooks/posts/hyperparameter-1.html",
    "title": "Grid Search",
    "section": "",
    "text": "Hyperparameter Tuning\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nDataset creation\n\n# Create a DataFrame for classification containing four real features and one binary target\n\ndf = pd.DataFrame({\n    'feature1': np.random.randint(0, 100, 100),\n    'feature2': np.random.randint(0, 100, 100),\n    'feature3': np.random.randint(0, 100, 100),\n    'feature4': np.random.randint(0, 100, 100),\n    'target': np.random.randint(0, 2, 100)\n})\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      feature1\n      feature2\n      feature3\n      feature4\n      target\n    \n  \n  \n    \n      0\n      29\n      14\n      66\n      83\n      1\n    \n    \n      1\n      68\n      70\n      87\n      72\n      1\n    \n    \n      2\n      42\n      5\n      40\n      67\n      1\n    \n    \n      3\n      2\n      54\n      79\n      0\n      1\n    \n    \n      4\n      81\n      36\n      35\n      75\n      0\n    \n  \n\n\n\n\n\ntrain_df = df[:50]\nvalidation_df = df[50:80]\n\n\ndt = DecisionTreeClassifier()\ndt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\ndt\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\ndt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n0.5333333333333333\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=2)\ndt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\ndt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n0.5666666666666667\n\n\n\nhyperparams = {'criterion': ['gini', 'entropy'],\n               'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n               'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n\n\nout = {}\nfor c in hyperparams['criterion']:\n    for d in hyperparams['max_depth']:\n        for s in hyperparams['min_samples_split']:\n            dt = DecisionTreeClassifier(criterion=c, max_depth=d, min_samples_split=s)\n            dt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\n            out[(c, d, s)] = dt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n\nhp_ser = pd.Series(out)\nhp_ser.sort_values(ascending=False)\n\nentropy  10  10    0.766667\n         7   7     0.766667\n         9   8     0.766667\n         8   10    0.766667\n             9     0.766667\n                     ...   \ngini     10  5     0.500000\n         8   3     0.500000\n         7   4     0.500000\n             3     0.500000\n         5   2     0.500000\nLength: 162, dtype: float64\n\n\n\nhp_ser.idxmax()\n\n('entropy', 4, 6)\n\n\n\nbest_dt = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)\n\n\nbest_dt.fit(df[:80][['feature1', 'feature2', 'feature3', 'feature4']], df[:80]['target'])\n\nDecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)\n\n\n\nbest_dt.score(df[80:][['feature1', 'feature2', 'feature3', 'feature4']], df[80:]['target'])\n\n0.45\n\n\n\n\nWithout using multiple nested loops\n\nprint(hyperparams.items(), len(hyperparams.items()))\n\ndict_items([('criterion', ['gini', 'entropy']), ('max_depth', [2, 3, 4, 5, 6, 7, 8, 9, 10]), ('min_samples_split', [2, 3, 4, 5, 6, 7, 8, 9, 10])]) 3\n\n\n\ndef print_vec(x, y, z):\n    print(f\"[{x} \\n{y} \\n{z}]\")\nprint_vec(*hyperparams.items())\n\n[('criterion', ['gini', 'entropy']) \n('max_depth', [2, 3, 4, 5, 6, 7, 8, 9, 10]) \n('min_samples_split', [2, 3, 4, 5, 6, 7, 8, 9, 10])]\n\n\n\nlist(zip(*hyperparams.items()))\n\n[('criterion', 'max_depth', 'min_samples_split'),\n (['gini', 'entropy'],\n  [2, 3, 4, 5, 6, 7, 8, 9, 10],\n  [2, 3, 4, 5, 6, 7, 8, 9, 10])]\n\n\n\nkeys, values = zip(*hyperparams.items())\n\n\nkeys\n\n('criterion', 'max_depth', 'min_samples_split')\n\n\n\nvalues\n\n(['gini', 'entropy'],\n [2, 3, 4, 5, 6, 7, 8, 9, 10],\n [2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n\nimport itertools\nlist(itertools.product(*values))[::10]\n\n[('gini', 2, 2),\n ('gini', 3, 3),\n ('gini', 4, 4),\n ('gini', 5, 5),\n ('gini', 6, 6),\n ('gini', 7, 7),\n ('gini', 8, 8),\n ('gini', 9, 9),\n ('gini', 10, 10),\n ('entropy', 3, 2),\n ('entropy', 4, 3),\n ('entropy', 5, 4),\n ('entropy', 6, 5),\n ('entropy', 7, 6),\n ('entropy', 8, 7),\n ('entropy', 9, 8),\n ('entropy', 10, 9)]\n\n\n\nv = next(itertools.product(*values))\nprint(v)\n\n('gini', 2, 2)\n\n\n\nprint_vec(*zip(keys, v))\n\n[('criterion', 'gini') \n('max_depth', 2) \n('min_samples_split', 2)]\n\n\n\ndef print_dict(**kwargs):\n    print(kwargs)\n\nprint_dict(**(dict(zip(keys, v))))\n\n{'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2}\n\n\n\nout = {}\nfor v in itertools.product(*values):\n    params = dict(zip(keys, v))\n    dt= DecisionTreeClassifier(**params)\n    dt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\n    out[(params['criterion'], params['max_depth'], params['min_samples_split'])] = dt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n    \n\n\npd.Series(out).sort_values(ascending=False)\n\nentropy  10  10    0.766667\n         7   7     0.766667\n         9   8     0.766667\n         8   10    0.766667\n             9     0.766667\n                     ...   \n         3   3     0.500000\n             4     0.500000\n             5     0.500000\ngini     10  5     0.500000\n             3     0.500000\nLength: 162, dtype: float64"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebook",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nParametric v/s Non-Parametric\n\n\nNipun Batra\n\n\n\n\nApr 14, 2023\n\n\nMovie Recommendation using KNN and Matrix Factorization\n\n\nNipun Batra\n\n\n\n\nApr 4, 2023\n\n\nAutodiff\n\n\nNipun Batra\n\n\n\n\nApr 4, 2023\n\n\n1d CNN\n\n\nNipun Batra\n\n\n\n\nApr 4, 2023\n\n\nKNN LSH\n\n\nNipun Batra\n\n\n\n\nApr 3, 2023\n\n\nCNN\n\n\nNipun Batra\n\n\n\n\nMar 31, 2023\n\n\nGenerating names using MLPs\n\n\nNipun Batra\n\n\n\n\nMar 1, 2023\n\n\nAutoDiff in JAX and PyTorch\n\n\nNipun Batra\n\n\n\n\nFeb 28, 2023\n\n\nLogistic Regression\n\n\nNipun Batra\n\n\n\n\nFeb 28, 2023\n\n\nNeural Network\n\n\nNipun Batra\n\n\n\n\nFeb 16, 2023\n\n\nGradient Descent\n\n\nNipun Batra\n\n\n\n\nFeb 14, 2023\n\n\nTaylor Series\n\n\nNipun Batra\n\n\n\n\nFeb 10, 2023\n\n\nConditioning and Linear Regression\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nLinear Regression: Geometric Perspective\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nMaths and JAX\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nCNN Edge 2d\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nMaths and JAX: Low Rank\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nSome Python Utilities\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nDataset splitting for machine learning\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nGrid Search\n\n\nNipun Batra\n\n\n\n\nJan 12, 2023\n\n\nDT Regression\n\n\nNipun Batra\n\n\n\n\nJan 12, 2023\n\n\nPandas tips\n\n\nNipun Batra\n\n\n\n\nJan 10, 2023\n\n\nMisc tips\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exams/q4.html",
    "href": "exams/q4.html",
    "title": "Quiz 4 (25 March)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\n\nConsider the figure above, where we fit the model \\(p(y=1 \\mid \\mathbf{x}, \\bm{{\\theta}})=\\sigma\\left(\\theta_0+\\theta_1 x_1+\\theta_2 x_2\\right)\\) by maximum likelihood, i.e., we minimize \\(J_a({\\theta})=-\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)\\) where \\(\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)\\) is the log likelihood on the training set. In the questions below, when multiple decision boundaries are possible, you should choose the one which minimizes the number of classification errors on the training dataset.\n\n\nSketch a decision boundary for the model. How many classification errors does your method make? (1 mark)\n\n\nNow, we regularize only the \\(\\theta_0\\) parameter, i.e., we minimize: \\(J_b({\\theta})=-\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)+\\lambda \\theta_0^2\\). Suppose \\(\\lambda\\) is a very large number, so we regularize \\(\\theta_0\\) all the way to 0, but all other parameters are unregularized. Sketch a possible decision boundary. How many classification errors does your method make? (1 mark)\n\n\nRepeat part (b), but we now instead regularize the \\(\\theta_1\\) parameter. (1 mark)\n\n\nRepeat part (b), but we now instead regularize the \\(\\theta_2\\) parameter. (1 mark)\n\n\nProve that softmax is equivalent to sigmoid when there are only two classes. (1 mark)\n\\(y = \\sigma(z)\\), where \\(\\sigma\\) is the sigmoid function. We also know that \\(z = f(a)\\). Find \\(\\dfrac{\\partial y}{\\partial a}\\). (1 mark)\nLet us consider a \\(K\\)-class logistic regression problem. For some example, \\(x\\), we get our outputs before the application of softmax as: \\(z_1=x^T\\theta_1\\), \\(\\cdots , z_k=x^T\\theta_k\\), \\(\\cdots ,z_K=x^T\\theta_K\\). We denote the vector of outputs as \\(\\vec{z} = \\left[\\begin{array}{@{}c@{}}  z_{1} \\\\  z_{2} \\\\  \\vdots \\\\  z_{K}  \\end{array} \\right]\\)\n\nWe will try to now use the cross entropy loss function to train our model. One of the terms in the cross entropy loss function is: \\(\\log\\left(\\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\\right)\\) which we refer to as \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\). However, we find that \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) cannot be computed directly for several cases. When \\(z_k\\) is a large number (e.g. 5000), a computer is unable to compute \\(e^{z_k}\\) as an overflow occurs (\\(e^{z_k}\\) = inf). When \\(z_k\\) is a large negative number (e.g. -5000), \\(e^{z_k}\\) = 0.0.\n\n\nWhat problem occurs in computing \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) when all elements of \\(\\vec{z}\\) are large (in magnitude) negative numbers (e.g. all \\(z_i < -6000\\))? (1 mark)\n\n\nModify the \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) expression using some trick so that we are able to compute it for any \\(z_k\\) and \\(\\vec{z}\\). You need to show the steps/simplifications you make. Show that this trick solves both the above problems (overflow and the problem you find in part (a) of this question) (2 marks)\n\n\n\nWe use a new type of coin for coin toss experiments. For this coin, the probability of heads goes down exponentially with the draw. Assuming the probaility of heads for the first draw (\\(i=1\\)) is \\(\\theta\\) and for the \\(i\\)th draw is \\(\\theta_i = \\dfrac{\\theta}{2^{i-1}}\\). What is the maximum likelihood estimate for \\(\\theta\\) for obtaining the draws as: T, H, H. Assume that each draw is independent of the others. Ofcourse, the identical assumption can not be made. (1 mark)"
  },
  {
    "objectID": "exams/q2.html",
    "href": "exams/q2.html",
    "title": "Quiz 2 (8 Feb)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\nIn bootstrap sampling, we sample with replacement from the original dataset. Let us assume that the original dataset of size \\(N\\) has all distinct elements. As an example if \\(N=8\\), we may have the dataset as \\(\\{1, 2,3, 4, 5, 6, 7, 8\\}\\). A bootstrap sample (or round) is also of size \\(N\\) and can contain some elements more than once. For example, a bootstrap sample may be \\(\\{8, 8, 3, 4, 5, 1, 8, 5\\}\\). The unique elements in this sample are: \\(\\{1, 3, 4, 5, 8\\}\\). This sample has 5 unique elements. Show that on average the number of unique elements in a bootstrap sample is \\(63.2\\%\\) of \\(N\\). [1.5 marks]\nWe studied the ADABoost classification algorithm for binary classification. We wrote the final prediction as: \\(\\mathrm{SIGN}(\\sum{\\alpha_i}h_i(x))\\) where \\(\\alpha_i\\) is the weight of the classifier \\(h_i\\) and \\(h_i(x)\\) is the prediction of the classifier \\(h_i\\) on the input \\(x\\). We also noted that each prediction \\(h_i(x)\\) is either \\(+1\\) or \\(-1\\).\nExtend ADABoost to multi-class classification where we have \\(K\\) classes and each classifier predicts one of the \\(K\\) classes (a number from \\(\\{1 \\cdots K\\}\\)). As an example, if we have \\(m=4\\) members in the ensemble, we may have something like \\(h_1(x) = 1\\), \\(h_2(x) = 2\\), \\(h_3(x) = 3\\) and \\(h_4(x) = 2\\). Now, write the formula for prediction for multi-class classification using the ensemble of classifiers, i.e. for any input \\(x\\), which class amongst \\(\\{1 \\cdots K\\}\\)) will be predicted as a function of \\(\\alpha_i\\)s and \\(h_i(x)\\)? Note: do not use the concept of one-vs-one or one-vs-all here. [2 marks]\nWhich hyperparameter can you vary to control the bias-variance tradeoff (or complexity) for decision trees? Draw the bias variance tradeoff curve for decision trees using this hyperparameter. Explain your answer. [1.5 mark]\nThe normal equation for linear regression is given as: \\(\\hat{\\theta} = (X^TX)^{-1}X^Ty\\). Instead of computing the normal equation directly, let us use the SVD decomposition of X. We decompose X as \\(X = U\\Sigma V^T\\).\n\n\nRewrite the normal equation using the reduced SVD decomposition of X, that is write \\(\\hat{\\theta}\\) in terms of \\(U\\), \\(\\Sigma\\) and \\(V\\) and \\(y\\).\n\n\nFor this question, let us assume that \\(X\\) is of size \\(n \\times m\\) and \\(y\\) is of size \\(n \\times 1\\). Let us also assume that the number of features \\(m\\) is significantly less than the number of samples \\(n\\).\n\n\nOnce you have written \\(\\hat{\\theta}\\) in terms of \\(U\\), \\(\\Sigma\\) and \\(V\\) and \\(y\\), find the time complexity of computing \\(\\hat{\\theta}\\) using the reduced form of SVD decomposition of \\(X\\). [4 marks]\n\n\nWe provide some background on the SVD decomposition of a matrix \\(X\\) below: The reduced form of SVD decomposition of \\(X\\) is given as \\(X = U\\Sigma V^T\\) where \\(U\\) is of size \\(n \\times m\\), \\(\\Sigma\\) is of size \\(m \\times m\\) and \\(V\\) is of size \\(m \\times m\\). The columns of \\(U\\) are called the left singular vectors of \\(X\\) and the columns of \\(V\\) are called the right singular vectors of \\(X\\). The singular matrix \\(\\Sigma\\) is a diagonal matrix: it has zeros everywhere except on the diagonal. The diagonal elements of \\(\\Sigma\\) are the singular values of \\(X\\). The singular values of \\(X\\) are always non-negative and are arranged in decreasing order. The singular values of \\(X\\) are also called the eigenvalues of \\(X^TX\\).\n\n\nFurther, for reduced SVD, \\(U^TU = I\\) and \\(V^TV = I\\) and \\(VV^T = I\\) where \\(I\\) is the identity matrix.\n\n\nWe also provide some background on the time complexity of matrix multiplication below: Let \\(A\\) be of size \\(n \\times m\\), \\(B\\) be of size \\(m \\times p\\) and \\(C\\) be of size \\(n \\times p\\). The time complexity of computing \\(C = AB\\) is \\(O(nmp)\\). Further, the time complexity of inverse of a \\(n \\times n\\) matrix \\(A\\) is \\(O(n^3)\\). The time complexity of computing SVD of the above \\(n \\times m\\) matrix \\(X\\) is \\(O(nm^2)\\). You should factor this time complexity in your answer for computing \\(\\hat{\\theta}\\).\n\n\nBONUS: Solve the above problem (computing \\(\\hat{\\theta}\\) and its time complexity) with the full version of SVD, what changes will you need to make? The full version of SVD is given as \\(X = U\\Sigma V^T\\) where \\(U\\) is of size \\(n \\times n\\), \\(\\Sigma\\) is of size \\(n \\times m\\) and \\(V\\) is of size \\(m \\times m\\). \\(U\\) and \\(V\\) are orthogonal matrices. [2 marks]\n\n\nLet us assume \\(K\\) members in an ensemble. For simplicity let us assume that each member in the ensemble has the same probability of error \\(p<0.5\\). We saw in the class that the probability of error (given by the binomial expansion) reduces as we increase the number of members in the ensemble. But, empirically adding more members in an ensemble may not always reduce the error. Why? [1 mark]"
  },
  {
    "objectID": "exams/q5.html",
    "href": "exams/q5.html",
    "title": "Quiz 5 (6 April)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\nAssume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work? Or, in other words show that without using a non-linear activation, we get a linear model. You can show this for a single datapoint \\((1 X D)\\) instead of showing this for \\(N\\) datapoints.(1 mark)\nFor this task, we are trying to use a neural network to predict a non-negative real number. However, in general, the output of a neural network is not constrained to be non-negative. Thus, we try to solve this problem by learning \\(f(\\hat{y})\\) as the output of the neural network, and we then apply \\(f^{-1}(f(\\hat{y}))\\) to finally get a \\(\\hat{y}\\) which is guaranteed to be non-negative? Can you give an example of what \\(f\\) and thus its inverse can be? Please note: you are not allowed to use ReLU activation. Further, we want our function f and its inverse to be continuous and differentiable everywhere.(1 mark)\nDraw a simple perceptron with two inputs and one output for the binary AND problem. Remember that the simple perceptron works on binary input and produces binary outputs. Show that your model works for all four inputs ((1, 1), (1, 0), (0, 0) and (0, 1))(1 mark)\nShow that the following network using ReLu activation works for the XOR problem. (1 mark; 0.25 marks for each of the four possible inputs (0,0), (0,1), (1,0), (1,1)) \nAssume we are solving the 3-class classification problem. We have two models (M1 and M2) from which we get two different output probabilities: \\(\\hat{y}_1 = (0.2, 0.7, 0.1)\\), \\(\\hat{y}_1 = (0.2, 0.5, 0.3)\\). The true class is \\(y = 0\\). Which of M1 or M2 has a higher cross-entropy losses? (1 mark)\nWe have the following code\n\nclass M_A(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        z1 = self.fc1(x)\n        a1 = F.relu(z1)\n        z2 = self.fc2(a1)\n        a2 = F.relu(z2)\n        z3 = self.fc3(a2) # logits\n        return z3\n\nclass M_B(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        z1 = self.fc1(x)\n        z2 = self.fc2(z1)\n        z3 = self.fc3(z2) \n        return z3\nBoth models have the same number of parameters despite some differences. Explain? (1 mark)\n\nLet us learn a model to predict the next character given a context of k previous characters. We have a total v characters in our vocabulary (e.g., a, b, …, z, _) Let us use an embedding size of e, i.e. each character in our vocabulary is represented as e length vector. Now, let us use a simple MLP with one hidden layer with h hidden units. What is the total number of parameters in this model? (1 mark)\nAssume the above setup in Q7. We observe that our embedding has “overfitted”, i.e. the embedding vectors are very far from the origin. Can you suggest an additional loss term that can help us to bring the embeddings closer to the origin and thus act as a regularisation? (1 mark)\nAs discussed in the lecture, chatGPT like systems are similar to the above model. But, we know that chatGPT can produce many different outputs for the same input. Let us use a simplification and consider next character prediction problem. Our model outputs a probability distribution over the next character. However, if we just pick up the most probable class/character, we should always get the same output given the same input. But, how do chatGPT like systems generate different outputs? (1 mark)\nAssume a MLP input of 4 dimensions, 2 real outputs, first hidden layers of 3 units, second hidden layer of 2 units, and a final output layer of 2 units. What is the total number of parameters in this model? (1 mark)"
  },
  {
    "objectID": "exams/prereq.html",
    "href": "exams/prereq.html",
    "title": "Prerequsite test",
    "section": "",
    "text": "Instructions\n\nThis test is open book, open internet, open notes. You can use any resources you want to solve the problems.\nYou should be typing your answers in a Jupyter notebook.\nThe submission would be a link to a public GitHub repository containing the notebook. Fill this form to submit your solution.\nA random subset of students may have a viva post the exam. The viva would be based on the notebook and the solutions you have provided.\nThe test is open till 6th January 2022 9 PM. You can submit your solutions anytime before that.\nThis problem has to be solved individually. You cannot collaborate with anyone else.\nThe code should be written using Python.\nSome questions may require you to answer in text. You can use markdown cells to write your answers. Some questions may require you to write code. You can use code cells to write your code. Some questions may require you to write mathematical expressions. You can use LaTeX to write your expressions. You can write such LaTeX expressions in markdown cells.\nFor any other questions, please ask on the General channel on Slack.\n\n\n\n\nQuestions\n\nHow many multiplications and additions do you need to perform a matrix multiplication between a (n, k) and (k, m) matrix? Explain.\nWrite Python code to multiply the above two matrices. Solve using list of lists and then use numpy. Compare the timing of both solutions. Which one is faster? Why?\nFinding the highest element in a list requires one pass of the array. Finding the second highest element requires 2 passes of the the array. Using this method, what is the time complexity of finding the median of the array? Can you suggest a better method? Can you implement both these methods in Python and compare against numpy.median routine in terms of time?\nWhat is the gradient of the following function with respect to x and y? \\[\nx^2y+y^3\\sin(x)\n\\]\nUse JAX to confirm the gradient evaluated by your method matches the analytical solution corresponding to a few random values of x and y\nUse sympy to confirm that you obtain the same gradient analytically.\nCreate a Python nested dictionary to represent hierarchical information. We want to store record of students and their marks. Something like:\n\n2022\n\nBranch 1\n\nRoll Number: 1, Name: N, Marks:\n\nMaths: 100, English: 70 …\n\n\nBranch 2\n\n2023\n\nBranch 1\nBranch 2\n\n2024\n\nBranch 1\nBranch 2\n\n2025\n\nBranch 1\nBranch 2\n\n\nStore the same information using Python classes. We have an overall database which is a list of year objects. Each year contains a list of branches. Each branch contains a list of students. Each student has some properties like name, roll number and has marks in some subjects.\nUsing matplotlib plot the following functions on the domain: x = 0.5 to 100.0 in steps of 0.5.\n\n\\(y = x\\)\n\\(y = x^2\\)\n\\(y = \\frac{x^3}{100}\\)\n\\(y = \\sin(x)\\)\n\\(y = \\frac{\\sin(x)}{x}\\)\n\\(y = \\log(x)\\)\n\\(y = e^x\\)\n\nUsing numpy generate a matrix of size 20X5 containing random numbers drawn uniformly from the range of 1 to 2. Using Pandas create a dataframe out of this matrix. Name the columns of the dataframe as “a”, “b”, “c”, “d”, “e”. Find the column with the highest standard deviation. Find the row with the lowest mean.\nAdd a new column to the dataframe called “f” which is the sum of the columns “a”, “b”, “c”, “d”, “e”. Create another column called “g”. The value in the column “g” should be “LT8” if the value in the column “f” is less than 8 and “GT8” otherwise. Find the number of rows in the dataframe where the value in the column “g” is “LT8”. Find the standard deviation of the column “f” for the rows where the value in the column “g” is “LT8” and “GT8” respectively.\nWrite a small piece of code to explain broadcasting in numpy.\nWrite a function to compute the argmin of a numpy array. The function should take a numpy array as input and return the index of the minimum element. You can use the np.argmin function to verify your solution."
  },
  {
    "objectID": "exams/q3.html",
    "href": "exams/q3.html",
    "title": "Quiz 3 (27 Feb)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\nMany evaluation metrics decompose over the training examples. For example, the loss function for linear regression (proportional to mean squared error) is given as: \\[L(\\theta) = \\frac{1}{2N}\\sum_{i=1}^N (y_i - \\sum_{d=1}^D \\theta_d x_i^d)^2\\] where \\(N\\) is the number of training examples, \\(x_i\\) is the \\(i^{th}\\) training example and \\(y_i\\) is the corresponding label. Mention any evaluation metric/loss function in machine learning that does not decompose over the training examples. [1 mark]\nWe saw the figure showing SGD convergence. \n\n\n2A) Prove that SGD is an unbiased estimator. [1 mark]\n2B) It seems that the SGD algorithm is not converging to the global minimum. Why do you think this is the case? [1 mark]\n2C). Why is it generally a good idea to use a small learning rate for SGD? [1 mark]\n2D) It seems that while the SGD algorithm is not converging, but it seems to be very quickly moving close to the global minimum. Why is SGD good initially when the loss is high? To help you answer this question, we pose a series of questions. Consider a simplification of linear regression. Our data is 1d. Our model is \\(y=\\theta x\\). Consider a dataset of \\(N\\) examples. Obtain the closed form solution for \\(\\theta\\) in terms of the scalars \\(x_i\\) and \\(y_i\\) for \\(i=1, \\cdots, N\\). [1 marks]\n2E) Consider \\(N=3\\) and a datset of the form \\(x_1=1, x_2=2, x_3=3, y_1=1, y_2=2.2, y_3=2.8\\). Plot the approximate contour plot of the loss function \\(L(\\theta)\\) for \\(\\theta \\in [-1, 3]\\). [1 marks].\n2F) Plot the loss v/s parameters (\\(\\theta\\)) corresponding to the loss for each training input. [1 marks]\n2G) Now, answer why SGD works initially, when the loss is high. [1 marks]\n\n\nIn an above question, we proved that the SGD estimator is an unbiased estimator. We have also previously discussed that we typically have a bias-variance tradeoff in our models. In the recent assignment question, we have plotted the bias and variance for different complexity trees. In this question, you have to derive the mean squared error in terms of three terms: bias, variance and irreducible noise.\n\n\nLet us assume our data is generated from a `true’ function \\(f(x)\\) and we have some additional zero mean normally distributed noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\n\\[y = f(x) + \\epsilon\\]\nWe can use some model such as a decision tree or linear regression to approximate \\(f(x)\\). We now consider a single training example \\((x_0, y_0)\\). We can define the mean squared error as:\n\\[MSE = \\mathbb{E}[(y_0 - \\hat{f}(x_0)^2)]\\]\nwhere \\(y\\) is the true label and \\(\\hat{f}(x_0)\\) is the predicted label. The expectation is over all possible training sets that could have been generated.\nTo keep the notation simple, we refer \\(f(x_0)\\) as \\(f\\) and \\(\\hat{f}(x_0)\\) as \\(\\hat{f}\\). Thus, we can write \\[MSE = \\mathbb{E}[(f - \\hat{f})^2]\\] We also define the bias as the difference between the true function and the predicted function, evaluated at the training example: \\[bias = \\mathbb{E}[\\hat{f}] - f\\] or, \\[bias = \\overline{f} - f\\] where \\(\\overline{f}\\) is the average/expectation of the predicted function over all possible training sets.\nWe define the variance as: \\[variance = VAR(\\hat{f})\\] or,\n\\[variance = \\mathbb{E}[(\\hat{f} - \\overline{f})^2]\\]\nWe define irreducible noise as the variance of the noise term \\(\\epsilon\\):\n\\[irreducible = VAR(\\epsilon)\\] or,\n\\[irreducible = \\sigma^2\\]\nUsing the above definitions, show that the mean squared error can be written as:\n\\[MSE = bias^2 + variance + irreducible\\]\n[2 marks]"
  },
  {
    "objectID": "exams/assignment-5.html",
    "href": "exams/assignment-5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "General instructions\n\nFeel free to use any framework of your choice: PyTorch, JAX (+Flax/Equinox), Tensorflow (+Keras)\nWe are not providing any code template. However, it is recommended you adhere to high code quality standards.\nFeel free to make use of ChatGPT, CoPilot, etc. like tools. Cite where you used them. However, you should still be able to explain your code during the viva. Also, you need to be careful with the hallucinations of these tools!\nAs before, this assignment is in a group of two students. You do not need to submit the assignment but can show your private repo to the TA during the viva.\n\n\n\nIn this question, you have to compare the performance on a binary classification task of the following models:\n\nVGG (1 block)\nVGG (3 blocks)\nVGG (3 blocks) with data augmentation\nTransfer learning using VGG16 or VGG19\n\nRefer this article You do not need to write your own code. You can reuse the code from the post. Or, you could roll out your own implemenation. Either way, you should be able to explain your code during the viva.\nYou need to create the dataset on your own based on your first names. For instance if the first name of the team members are: Siya and Raghav, they can choose a dataset of their liking based on any names, place, animal or thing. As examples:\n\nSeoul v/s Riyadh\nSnake v/s Rat\nSquirrel v/s Rabbit\nSambhar v/s Roti\n\nYou can refer to resource 1 or resource 2 or plainly download 100 images of both classes (total 200 images). Of these 100 images of each class, we will use 80 for training and 20 for testing. You get 1 mark for dataset creation [1 mark]\nCreate a table with models as rows and the following columns [2 marks (0.5 marks for each model)]\n\nTraining time\nTraining loss\nTraining accuracy\nTesting accuracy\nNumber of model parameters\n\nWe will now be using Tensorboard for visualizing network performance. You are suggested to refer to:\n\nPyTorch + Tensorboard\nTensorflow + Tensorboard\n\nUse Tensorboard to log the following and present screenshots/images [1 mark]\nScalars\n\nTraining loss v/s iterations (and not epochs)\nTraining accuracy v/s iterations (and not epochs)\nTesting accuracy v/s iterations (and not epochs)\n\nImages\n\nShow all images from the test set and their predictions\n\nNow you have to present various insights. For instance, you should discuss the following: [2 marks (0.5 marks for each question)]\n\nAre the results as expected? Why or why not?\nDoes data augmentation help? Why or why not?\nDoes it matter how many epochs you fine tune the model? Why or why not?\nAre there any particular images that the model is confused about? Why or why not?\n\nNow, create a MLP model with comparable number of parameters as VGG16 and compare your performance with the other models in the table. You can choose the distribution of number of neurons and number of layers. What can you conclude? [1 mark]"
  },
  {
    "objectID": "exams/q1.html",
    "href": "exams/q1.html",
    "title": "Quiz 1 (18 Jan)",
    "section": "",
    "text": "Instructions\n\nTotal Time: 30 mins\n\n\n\nRemember the entropy discussion we had in the lecture. We saw that for the Tennis example, the maximum entropy is 1.0. What is the maximum entropy an Imagenet classification problem, where we have 1024 classes? [1 mark]\nGiven the following dataset, what attribute/feature would the decision tree algorithm choose to split the data on for the first iteration? Why? [1 mark]\n\n\n\n\n\n\n\n\n\n\n\nSample #\nTomato radius\nTomato weight\nTomato color\nTomato quality\n\n\n\n\n1\n1\n1\n1\nGood\n\n\n2\n1\n1\n2\nGood\n\n\n3\n1\n2\n1\nBad\n\n\n4\n1\n2\n2\nBad\n\n\n5\n2\n1\n1\nGood\n\n\n6\n2\n2\n2\nGood\n\n\n\n\nIn the lectures we saw that np.std(x) and pd.Series(x).std() are different. Why? [1 mark]\nQuoting Wikipedia:\n\n\nPruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.\n\n\nPre-pruning procedures prevent a complete induction of the training set by replacing a stop () criterion in the induction algorithm (e.g. max. Tree depth or information gain (Attr)> minGain). Pre-pruning methods are do not induce an entire set, but rather trees remain small from the start.\n\nCreate a decision tree for the following classification problem. Explain why the pre-pruning using information gain approach can be limiting? [2 marks]\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n\nVisualize the decision tree for the following regression problem, where the ground truth is the function \\(y = x + 2\\). Use \\(x = \\{1, 2, \\cdots 4\\}\\) as the training dataset. Also visualize the learnt function [2 marks]\nCreate an example ground truth and prediction where the mean absolute error is 100 and mean error is 0. [0.5 marks]\nCreate one confusion matrix for 100 total examples where the precision is 0.8, recall is 0.5. [1 mark]\nShow visualisation of 1d regression problem for continuous inputs showing a good fit, a high bias and a high variance fit. [1.5 mark]"
  },
  {
    "objectID": "grading.html",
    "href": "grading.html",
    "title": "Grading Policy",
    "section": "",
    "text": "Quizzes: 60%\n\n10% each\nBest 6 out of 8\n\n\n\n\nAssignments: 40%\n\nVariable weight (e.g. some assignments would be 5%, some 10%, etc.)\nSome assignments would involve:\n\nMaking pull requests to public repositories\nWriting Hugging Face Spaces like demos\n\n\n\n\n\nBonus: up to 6%\n\nMaking a non-trivial pull request to a well-starred public repo (4%)\nGetting the PR accepted (2%)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Lecture #\nDate\nTopic\n\n\n\n\n1\n3 Jan\nIntroduction and Logistics [Slides]\n\n\nNone\n4 Jan\nPre-requisites quiz released\n\n\n2\n5 Jan\nConvention, Metrics, Classification, Regression [Slides]\n\n\n3\n10 Jan\nDecision Trees - 1[Slides][Notebook]\n\n\n4\n12 Jan\nDecision Trees - 2[Slides][Notebook]\n\n\n5\n17 Jan\nBias and Variance[Slides][Notebook on Python utils][Notebook on Grid Search]\n\n\nNone\n18 Jan\nQuiz 1\n\n\n6\n19 Jan\nBias, Variance 2, Cross Validation[Slides]\n\n\n7\n24 Jan\nEnsemble Methods[Slides]\n\n\n8\n31 Jan\nEnsemble Methods[Slides], Weighted samples in decision trees[Slides], Maths for ML-1 [Slides] [Notebook-1] [Notebook 2], [Streamlit app] Linear Regression [Slides]\n\n\n9\n2 Feb\nLinear Regression [Slides], Contour Plots [Slides], Geometric View of Linear Regression [Slides]\n\n\n10\n9 Feb\nLinear Regression II [Slides]\n\n\n11\n14 Feb\nGradient Descent [Slides], Taylor’s Series, Notebook on Taylor’s series, Reference on relationship between Taylor’s series and GD, Reference 2\n\n\n12\n16 Feb\nGradient Descent [Slides] Notebook\n\n\n13\n21 Feb\nGradient Descent continued, [Ridge Regression], [Streamlit demo], [Additional reading on SGD being an unbiased estimator]\n\n\n14\n23 Feb\nRidge regression, LASSO, [Interactive article on Optimization algorithms]\n\n\n15\n28 Feb\nLogistic regression [Slides], [Notebook] (best run locally to render interactive visualisations)\n\n\n16\n2 Mar\nLogistic regression [Slides]\n\n\n17\n14 Mar\nLogistic regression [Slides]\n\n\n18\n16 Mar\nMLP [Slides]\n\n\n19\n21 Mar\nMLP [Slides], Notebook\n\n\n20\n28 Mar\nMLP [Slides]\n\n\n21\n30 Mar\nNext work prediction [Slides], Notebook\n\n\n22\n4 Apr\nConvolutional Neural Networks [Slides], 1d CNN slides, Notebook 1, Notebook 2, Notebook 3, Equivariance v/s Invariance, Reference1, Reference2, Notebook\n\n\n23\n6 Apr\nAutograd [Slides], Notebook on Autodiff, Reference on chain rule Naive Bayes [Slides]\n\n\n24\n11 Apr\nSVM-II\n\n\n25\n13 Apr\nSVM-III\n\n\n26\n18 Apr\nNaive Bayes + KNN\n\n\n27\n20 Apr\nKNN + Unsupervised\n\n\n28\n25 Apr\nUnsupervised + RL?"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Prerequisites\n\nWhat are the course prerequisites?\n\nThe course has no “formal” prerequisites like all courses at IITGn. However, it is assumed that you have a basic understanding of:\n\nprogramming (Python), and data structures (ES242 equivalent)\nprobability and statistics\nlinear algebra\ncalculus\n\n\nHow can I prepare for the prerequisite exam?\n\nYour UG course material should be sufficient to prepare for the exam.\nYou may additionally refer to the “prerequisite” reading section on the course homepage.\n\nWhat happens if I do not clear the prerequisite exam?\n\nYou will not be allowed to take the course.\n\nWhat do you mean by “clear” the exam?\n\nLike all courses at IITGn, the instructor will decide the cut-off for the exam. You will have to score above the cut-off to clear the exam. No cut-off will be revealed to the students apriori.\n\n\n\n\n\nQuizzes\n\nWhat happens if I miss a quiz due to any reason?\n\nThe quiz will be marked as 0.\nThe provision of best 6 out of 8 quizzes is designed keeping in mind such scenarios.\n\nWill the quizzes and end-semester exam be open book? Will I be allowed to carry notes?\n\nNo, the exams and quizzes will be closed book. You are not permitted to carry notes.\n\nHow soon can I expect to receive my answer sheets back?\n\nYou should expect to receive answer sheets back in 4-5 working days.\n\nIs there an end-semester exam or mid-semester exam?\n\nNo, there is no end-semester exam or mid-semester exam. However, some of the quizzes may be held in the exam slot. In total we will have 8 equal weightage quizzes. The best 6 out of them will be considered for the final grade.\n\nWill the quizzes be MCQs or subjective?\n\nThe quizzes may contain both the MCQs and subjective questions.\n\n\n\n\n\nAssignments\n\nWhat happens if I miss an assignment due to any reason?\n\nThere will no extensions for assignments.\n\nI have a doubt in the assignment. Whom should I write to?\n\nAsk on the slack General channel. If you don’t get a response within 2 days, write to the course instructor.\n\nI do not know Python. Can I code assignments in some other language?\n\nUnfortunately, no. You have to stick to Python.\n\nHow will you evaluate the assignment?\n\nThe assignments would be followed by a viva. The TAs would first check the code and compare against the submission. Any change from the submitted code is not allowed and any instance of the same would culminate in a warning. The TAs would run the code and ask a few questions. About 75% of these questions would be based on the assignment in question and about 25% would be based on the theory behind the concepts covered in the assignment.\nThe grade breakup would be: i) code runs correctly and solves the problem [50% marks]; ii) questions based on the assignment and student understanding of code [25% marks]; iii) code quality [12.5% marks]; iv) questions based on the theory behind the concepts covered in the assignments [12.5% marks]\n\nIs the assignment individual or group?\n\nThe assignment is TBA. In case of group, all team members get the same grade for the assignment.\n\n\n\n\n\nAttendance\n\nAttendance policy\n\nAttendance is not mandatory, but highly encouraged. Marks often correlated with attendance.\n\n\n\n\n\nProjects\n\nIs there a project component in the course?\n\nNo. There is no project component in the course.\n\n\n\n\n\nBonus\n\nCan I choose a PR of my choice.\n\nYes, you can choose a PR of your choice from a list of PRs decided by the teaching staff. The teaching staff hold the right to decide whether a PR is suitable or not."
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2023\n\n\nPrerequsite test\n\n\n\n\n\n\nJan 18, 2023\n\n\nQuiz 1 (18 Jan)\n\n\n\n\n\n\nFeb 8, 2023\n\n\nQuiz 2 (8 Feb)\n\n\n\n\n\n\nFeb 27, 2023\n\n\nQuiz 3 (27 Feb)\n\n\n\n\n\n\nMar 25, 2023\n\n\nQuiz 4 (25 March)\n\n\n\n\n\n\nMar 26, 2023\n\n\nAssignment 5\n\n\n\n\n\n\nApr 6, 2023\n\n\nQuiz 5 (6 April)\n\n\n\n\n\n\n\n\nNo matching items"
  }
]