[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Lecture #\nDate\nTopic\n\n\n\n\n1\n3 Jan\nIntroduction and Logistics [Slides]\n\n\nNone\n4 Jan\nPre-requisites quiz released\n\n\n2\n5 Jan\nConvention, Metrics, Classification, Regression [Slides]\n\n\n3\n10 Jan\nDecision Trees - 1[Slides][Notebook]\n\n\n4\n12 Jan\nDecision Trees - 2[Slides][Notebook]\n\n\n5\n17 Jan\nBias and Variance[Slides][Notebook on Python utils][Notebook on Grid Search]\n\n\nNone\n18 Jan\nQuiz 1\n\n\n6\n19 Jan\nBias, Variance 2, Cross Validation[Slides]\n\n\n7\n24 Jan\nEnsemble Methods[Slides]\n\n\n8\n31 Jan\nEnsemble Methods[Slides], Weighted samples in decision trees[Slides], Maths for ML-1 [Slides] [Notebook-1] [Notebook 2], [Streamlit app] Linear Regression [Slides]\n\n\n9\n2 Feb\nLinear Regression [Slides], Contour Plots [Slides], Geometric View of Linear Regression [Slides]\n\n\n10\n9 Feb\nLinear Regression II [Slides]\n\n\n11\n14 Feb\nGradient Descent [Slides], Taylor’s Series, Notebook on Taylor’s series, Reference on relationship between Taylor’s series and GD, Reference 2\n\n\n12\n16 Feb\nGradient Descent [Slides] Notebook\n\n\n13\n21 Feb\nGradient Descent continued, [Ridge Regression], [Streamlit demo], [Additional reading on SGD being an unbiased estimator]\n\n\n14\n23 Feb\nRidge regression, LASSO, [Interactive article on Optimization algorithms]\n\n\n15\n28 Feb\nLogistic regression [Slides], [Notebook] (best run locally to render interactive visualisations)\n\n\n16\n2 Mar\nLogistic regression [Slides]\n\n\n17\n14 Mar\nLogistic regression [Slides]\n\n\n18\n16 Mar\nMLP [Slides]\n\n\n19\n21 Mar\nMLP [Slides], Notebook\n\n\n20\n28 Mar\nMLP [Slides]\n\n\n21\n30 Mar\nNext work prediction [Slides], Notebook\n\n\n22\n4 Apr\nConvolutional Neural Networks [Slides], Equivariance v/s Invariance, Reference1, Reference2, Notebook\n\n\n23\n6 Apr\nAutograd + Constrained Optimization and SVM-I?\n\n\n24\n11 Apr\nSVM-II\n\n\n25\n13 Apr\nSVM-III\n\n\n26\n18 Apr\nNaive Bayes + KNN\n\n\n27\n20 Apr\nKNN + Unsupervised\n\n\n28\n25 Apr\nUnsupervised + RL?"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Prerequisites\n\nWhat are the course prerequisites?\n\nThe course has no “formal” prerequisites like all courses at IITGn. However, it is assumed that you have a basic understanding of:\n\nprogramming (Python), and data structures (ES242 equivalent)\nprobability and statistics\nlinear algebra\ncalculus\n\n\nHow can I prepare for the prerequisite exam?\n\nYour UG course material should be sufficient to prepare for the exam.\nYou may additionally refer to the “prerequisite” reading section on the course homepage.\n\nWhat happens if I do not clear the prerequisite exam?\n\nYou will not be allowed to take the course.\n\nWhat do you mean by “clear” the exam?\n\nLike all courses at IITGn, the instructor will decide the cut-off for the exam. You will have to score above the cut-off to clear the exam. No cut-off will be revealed to the students apriori.\n\n\n\n\n\nQuizzes\n\nWhat happens if I miss a quiz due to any reason?\n\nThe quiz will be marked as 0.\nThe provision of best 6 out of 8 quizzes is designed keeping in mind such scenarios.\n\nWill the quizzes and end-semester exam be open book? Will I be allowed to carry notes?\n\nNo, the exams and quizzes will be closed book. You are not permitted to carry notes.\n\nHow soon can I expect to receive my answer sheets back?\n\nYou should expect to receive answer sheets back in 4-5 working days.\n\nIs there an end-semester exam or mid-semester exam?\n\nNo, there is no end-semester exam or mid-semester exam. However, some of the quizzes may be held in the exam slot. In total we will have 8 equal weightage quizzes. The best 6 out of them will be considered for the final grade.\n\nWill the quizzes be MCQs or subjective?\n\nThe quizzes may contain both the MCQs and subjective questions.\n\n\n\n\n\nAssignments\n\nWhat happens if I miss an assignment due to any reason?\n\nThere will no extensions for assignments.\n\nI have a doubt in the assignment. Whom should I write to?\n\nAsk on the slack General channel. If you don’t get a response within 2 days, write to the course instructor.\n\nI do not know Python. Can I code assignments in some other language?\n\nUnfortunately, no. You have to stick to Python.\n\nHow will you evaluate the assignment?\n\nThe assignments would be followed by a viva. The TAs would first check the code and compare against the submission. Any change from the submitted code is not allowed and any instance of the same would culminate in a warning. The TAs would run the code and ask a few questions. About 75% of these questions would be based on the assignment in question and about 25% would be based on the theory behind the concepts covered in the assignment.\nThe grade breakup would be: i) code runs correctly and solves the problem [50% marks]; ii) questions based on the assignment and student understanding of code [25% marks]; iii) code quality [12.5% marks]; iv) questions based on the theory behind the concepts covered in the assignments [12.5% marks]\n\nIs the assignment individual or group?\n\nThe assignment is TBA. In case of group, all team members get the same grade for the assignment.\n\n\n\n\n\nAttendance\n\nAttendance policy\n\nAttendance is not mandatory, but highly encouraged. Marks often correlated with attendance.\n\n\n\n\n\nProjects\n\nIs there a project component in the course?\n\nNo. There is no project component in the course.\n\n\n\n\n\nBonus\n\nCan I choose a PR of my choice.\n\nYes, you can choose a PR of your choice from a list of PRs decided by the teaching staff. The teaching staff hold the right to decide whether a PR is suitable or not."
  },
  {
    "objectID": "notebooks/posts/split.html",
    "href": "notebooks/posts/split.html",
    "title": "Dataset splitting for machine learning",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n# Create a dataset with 3 features and 1000 samples for a classification problem\n\ndf = pd.DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'])\ndf['D'] = np.random.randint(0, 2, 1000)\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      0.112981\n      -0.033889\n      0.348708\n      1\n    \n    \n      1\n      1.821601\n      1.267423\n      0.274915\n      1\n    \n    \n      2\n      0.148040\n      -1.412712\n      -0.271345\n      1\n    \n    \n      3\n      0.004766\n      -1.209944\n      0.122512\n      1\n    \n    \n      4\n      0.854442\n      -0.559497\n      -0.605376\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      0.161444\n      -0.969567\n      -2.079664\n      0\n    \n    \n      996\n      1.615938\n      0.219218\n      -0.322223\n      0\n    \n    \n      997\n      0.501970\n      -0.874313\n      1.571102\n      1\n    \n    \n      998\n      0.386063\n      -1.481215\n      -1.974313\n      1\n    \n    \n      999\n      0.468528\n      1.060850\n      0.612252\n      1\n    \n  \n\n1000 rows × 4 columns\n\n\n\n\nX, y = df[['A', 'B', 'C']], df['D']\n\n\ntrain_X, test_X = X[:800], X[800:]\ntrain_y, test_y = y[:800], y[800:]\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndt = DecisionTreeClassifier(max_depth=2)\ndt.fit(train_X, train_y)\n\nDecisionTreeClassifier(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=2)\n\n\n\nfrom sklearn import tree\ntree.plot_tree(dt)\n\n[Text(0.5, 0.8333333333333334, 'X[0] <= -0.17\\ngini = 0.5\\nsamples = 800\\nvalue = [400, 400]'),\n Text(0.25, 0.5, 'X[0] <= -0.307\\ngini = 0.495\\nsamples = 348\\nvalue = [157, 191]'),\n Text(0.125, 0.16666666666666666, 'gini = 0.499\\nsamples = 316\\nvalue = [153, 163]'),\n Text(0.375, 0.16666666666666666, 'gini = 0.219\\nsamples = 32\\nvalue = [4, 28]'),\n Text(0.75, 0.5, 'X[2] <= 1.656\\ngini = 0.497\\nsamples = 452\\nvalue = [243, 209]'),\n Text(0.625, 0.16666666666666666, 'gini = 0.495\\nsamples = 434\\nvalue = [239, 195]'),\n Text(0.875, 0.16666666666666666, 'gini = 0.346\\nsamples = 18\\nvalue = [4, 14]')]\n\n\n\n\n\n\ny_hat = dt.predict(test_X)\npd.DataFrame({\"y_true\": test_y, \"y_hat\": y_hat})\n\n\n\n\n\n  \n    \n      \n      y_true\n      y_hat\n    \n  \n  \n    \n      800\n      0\n      1\n    \n    \n      801\n      0\n      0\n    \n    \n      802\n      1\n      1\n    \n    \n      803\n      1\n      1\n    \n    \n      804\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      995\n      0\n      1\n    \n    \n      996\n      0\n      0\n    \n    \n      997\n      1\n      0\n    \n    \n      998\n      1\n      1\n    \n    \n      999\n      1\n      1\n    \n  \n\n200 rows × 2 columns\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain_scores = {}\ntest_scores = {}\n\nfor i in range(1, 20):\n    dt = DecisionTreeClassifier(max_depth=i)\n    dt.fit(train_X, train_y)\n    train_scores[i] = dt.score(train_X, train_y)\n    \n    test_scores[i] = dt.score(test_X, test_y)\n\n\nscores_df = pd.DataFrame({'train': train_scores, 'test': test_scores})\nscores_df.plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "notebooks/posts/cnn-edge.html",
    "href": "notebooks/posts/cnn-edge.html",
    "title": "CNN Edge 2d",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\nimport seaborn as sns\n%matplotlib inline\n\n\n# Create a tensor of size 6x6 with first three columns as 1 and rest as 0\nx = torch.zeros(6, 6)\nx[:, :3] = 1\nprint(x)\n\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])\n\n\n\nx.shape\n\ntorch.Size([6, 6])\n\n\n\n# Plot the tensor with equal aspect ratio\nplt.figure(figsize=(6, 6))\nsns.heatmap(x, cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\n# Create a 3x3 kernel with first column as 1, second as 0 and third as -1\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\nprint(k)\n\ntensor([[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]])\n\n\n\n# Apply the kernel to the image\n\n\ny = F.conv2d(x.view(1, 1, 6, 6), k.view(1, 1, 3, 3))\nprint(y)\n\n# Create figure of size of y\nplt.figure(figsize=(y.shape[2], y.shape[3]))\nsns.heatmap(y[0, 0], cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\ntensor([[[[0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.]]]])\n\n\n<AxesSubplot: >\n\n\n\n\n\n\nim = plt.imread('lm.jpeg')\nplt.imshow(im)\n\n<matplotlib.image.AxesImage at 0x151220670>\n\n\n\n\n\n\n# Crop to left 180 X 180 pixels\n\nim = im[:180, :180]\nplt.imshow(im, cmap='gray')\n\n<matplotlib.image.AxesImage at 0x1512b3cd0>\n\n\n\n\n\n\n# Convert to grayscale\nim = im.mean(axis=2)\nplt.imshow(im, cmap='gray')\n\n<matplotlib.image.AxesImage at 0x15133dac0>\n\n\n\n\n\n\nim.shape\n\n(180, 180)\n\n\n\n# Detect edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\n\n\n\n# plot the result\n#plt.figure(figsize=(y.shape[2], y.shape[3]))\nplt.imshow(y[0, 0], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x197097430>\n\n\n\n\n\n\n# Detect horizontal edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float().T\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\nplt.imshow(y[0, 0], cmap='gray')\n\n<matplotlib.image.AxesImage at 0x197105730>"
  },
  {
    "objectID": "notebooks/posts/projection.html",
    "href": "notebooks/posts/projection.html",
    "title": "Linear Regression: Geometric Perspective",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Two three-dimensional vectors\nv1 = np.array([1, 1, 1])\nv2 = np.array([2, -2, 2])\n\n# y-vector\ny = np.array([2.5, -0.8, 1.2])\n\n\n# plot the vectors in 3D\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2')\nax.quiver(0, 0, 0, y[0], y[1], y[2], color='g', label='y')\n\nax.set_xlim(0, 3)\nax.set_ylim(0, 4)\nax.set_zlim(0, 3)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.legend()\n\nax.view_init(elev=45, azim=60)\n\n\n\n\n\ntheta = np.linalg.lstsq(np.array([v1, v2]).T, y, rcond=None)[0]\ntheta\n\narray([0.525 , 0.6625])\n\n\n\n# Projection of y onto the plane spanned by v1 and v2\ny_proj = np.dot(np.array([v1, v2]).T, theta)\ny_proj\n\narray([ 1.85, -0.8 ,  1.85])\n\n\n\n# Plot the x=z plane filled with color black\nfig, ax = plt.subplots(figsize=(8, 8))\n# 3d projection\nax = fig.add_subplot(111, projection='3d')\nxx, zz = np.meshgrid(np.linspace(-1, 4, 100), np.linspace(-1, 4, 100))\nyy = np.zeros_like(xx)\nax.plot_surface(xx, yy, zz, alpha=0.2, color='k')\n\n\n# plot the vectors in 3D\nax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1')\nax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2')\nax.quiver(0, 0, 0, y[0], y[1], y[2], color='g', label='y')\n\n\n# Limit the view to the x-z plane\n\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\n# Set the view angle so that we can see the x-z plane appear at a 45 degree angle\n#ax.view_init(azim=70, elev=200)\nax.view_init(elev=60, azim=-80, roll=90)\nax.view_init(120, -120, -120)\n#ax.view_init(roll=45)\n#ax.view_init(elev=30, azim=45, roll=15)\nax.set_ylim(-4, 4)\nax.set_xlim(0, 4)\nax.set_zlim(0, 4)\n\n\n# Plot the projection of y onto the plane spanned by v1 and v2\nax.quiver(0, 0, 0, y_proj[0], y_proj[1], y_proj[2], color='k', label='Projection of y onto\\n the plane spanned by v1 and v2')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x111d52730>"
  },
  {
    "objectID": "notebooks/posts/dt-reg.html",
    "href": "notebooks/posts/dt-reg.html",
    "title": "DT Regression",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"mins-played.csv\")\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Day\n      Outlook\n      Temp\n      Humidity\n      Wind\n      Minutes Played\n    \n  \n  \n    \n      0\n      D1\n      Sunny\n      Hot\n      High\n      Weak\n      20\n    \n    \n      1\n      D2\n      Sunny\n      Hot\n      High\n      Strong\n      24\n    \n    \n      2\n      D3\n      Overcast\n      Hot\n      High\n      Weak\n      40\n    \n    \n      3\n      D4\n      Rain\n      Mild\n      High\n      Weak\n      50\n    \n    \n      4\n      D5\n      Rain\n      Cool\n      Normal\n      Weak\n      60\n    \n    \n      5\n      D6\n      Rain\n      Cool\n      Normal\n      Strong\n      10\n    \n    \n      6\n      D7\n      Overcast\n      Cool\n      Normal\n      Strong\n      4\n    \n    \n      7\n      D8\n      Sunny\n      Mild\n      High\n      Weak\n      10\n    \n    \n      8\n      D9\n      Sunny\n      Cool\n      Normal\n      Weak\n      60\n    \n    \n      9\n      D10\n      Rain\n      Mild\n      Normal\n      Weak\n      40\n    \n    \n      10\n      D11\n      Sunny\n      Mild\n      High\n      Strong\n      45\n    \n    \n      11\n      D12\n      Overcast\n      Mild\n      High\n      Strong\n      40\n    \n    \n      12\n      D13\n      Overcast\n      Hot\n      Normal\n      Weak\n      35\n    \n    \n      13\n      D14\n      Rain\n      Mild\n      High\n      Strong\n      20\n    \n  \n\n\n\n\n\ndf[\"Minutes Played\"].std()\n\n18.3111087402348\n\n\n\nimport numpy as np\n# np.std(df[\"Minutes Played\"].values)\n\n\ndf.query(\"Wind=='Weak'\")[\"Minutes Played\"].std()*len(df.query(\"Wind=='Weak'\"))/len(df)\n\n10.180585192846463\n\n\n\ndf.query(\"Wind=='Strong'\")[\"Minutes Played\"].std()*len(df.query(\"Wind=='Strong'\"))/len(df)\n\n6.933944897151599\n\n\n\nout = {}\nfor temp in df[\"Temp\"].unique():\n    print(temp)\n    out[temp] = df.query(\"Temp==@temp\")[\"Minutes Played\"].std()*len(df.query(\"Temp==@temp\"))/len(df)\n    print(out[temp])\n    print()\n\nHot\n2.6636888135137133\n\nMild\n6.696785704762413\n\nCool\n8.770699519880226\n\n\n\n\ndf[\"Minutes Played\"].std() - pd.Series(out).sum()\n\n0.17993470207844808"
  },
  {
    "objectID": "notebooks/posts/autodiff-jax-torch.html",
    "href": "notebooks/posts/autodiff-jax-torch.html",
    "title": "AutoDiff in JAX and PyTorch",
    "section": "",
    "text": "import jax.numpy as jnp\nimport jax\n\nimport torch\nprint(torch.__version__)\nprint(jax.__version__)\n\nArray(1., dtype=float32, weak_type=True)\n\n\n\ndef f(x):\n    return jnp.sin(x)\n\nArray(1., dtype=float32, weak_type=True)\n\n\n\nz = torch.tensor(0.0, requires_grad=True)\ntorch.sin(z).backward()\nprint(jax.grad(f)(0.0), z.grad)\n\ntensor(1.)\n\n\n\ndef f(x):\n    return jnp.abs(x)\n\n\nz1 = torch.tensor(0.0001, requires_grad=True)\ntorch.abs(z1).backward()\n\nz2 = torch.tensor(-0.0001, requires_grad=True)\ntorch.abs(z2).backward()\n\nz3 = torch.tensor(0.0, requires_grad=True)\ntorch.abs(z3).backward()\n\nprint(jax.grad(f)(0.0), z1.grad, z2.grad, z3.grad)\n\n1.0 tensor(1.) tensor(-1.) tensor(0.)\n\n\n\n# Use functorch\n\nimport functorch\n\nImportError: dlopen(/Users/nipun/miniconda3/lib/python3.9/site-packages/functorch/_C.cpython-39-darwin.so, 0x0002): Symbol not found: __ZN2at4_ops10as_strided4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEES8_NS5_8optionalIS7_EE\n  Referenced from: <12715304-4308-3E9B-A374-E4ADB3345E65> /Users/nipun/miniconda3/lib/python3.9/site-packages/functorch/_C.cpython-39-darwin.so\n  Expected in:     <22ECBAD5-EEDD-3C80-9B5A-0564B60B6811> /Users/nipun/miniconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n\n\n\n\n\n'1.12.1'"
  },
  {
    "objectID": "notebooks/posts/taylor.html",
    "href": "notebooks/posts/taylor.html",
    "title": "Taylor Series",
    "section": "",
    "text": "import jax.numpy as jnp\nfrom jax import random, jit, vmap, grad, jacfwd, jacrev, hessian, value_and_grad\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Define the function to be approximated\n\ndef f(x):\n    return jnp.sin(x)\n\n\n# Plot the function\n\nx = jnp.linspace(-jnp.pi, jnp.pi, 100)\nplt.plot(x, f(x))\n\n\n\n\n\n# First order Taylor approximation for f(x) at x = 0\n\ndef taylor1(f, x, x0=0.):\n    return f(x0) + grad(f)(x0) * (x - x0)\n\n\n# Plot the Taylor approximation\n\nplt.plot(x, f(x), label='f(x)')\nplt.plot(x, taylor1(f, x), label='Taylor approximation')\n\n\n\n\n\n# factorial function in JAX\n\ndef factorial(n):\n    return jnp.prod(jnp.arange(1, n + 1))\n\n\n# Find the nth order Taylor approximation for f(x) at x = 0\n\ndef taylor(f, x, n, x0=0.):\n    grads = {0:f}\n    output = f(x0)\n    for i in range(1, n+1):\n        grads[i] = grad(grads[i-1])\n        output += grads[i](x0) * (x - x0)**i / factorial(i)\n    return output\n\n\nplt.plot(x, f(x), label='f(x)', lw=5)\nplt.plot(x, taylor(f, x, 1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(f, x, 3), label='Taylor approximation, n=3')\nplt.plot(x, taylor(f, x, 5), label='Taylor approximation, n=5')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1aea5ea90>\n\n\n\n\n\n\nx = jnp.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\nplt.plot(x, g(x), label='g(x)', lw=4, alpha=0.5)\nplt.plot(x, taylor(g, x, 1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(g, x, 2), label='Taylor approximation, n=3', ls='--')\n\n\n\n\n\nplt.plot(x, g(x), label='g(x)', lw=4, alpha=0.5)\nplt.plot(x, taylor(g, x, 1, 4.1), label='Taylor approximation, n=1')\nplt.plot(x, taylor(g, x, 2, 4.1), label='Taylor approximation, n=3', ls='--')\nplt.ylim((-2, 20))\n\n(-2.0, 20.0)"
  },
  {
    "objectID": "notebooks/posts/hyperparameter-1.html",
    "href": "notebooks/posts/hyperparameter-1.html",
    "title": "Grid Search",
    "section": "",
    "text": "Hyperparameter Tuning\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nDataset creation\n\n# Create a DataFrame for classification containing four real features and one binary target\n\ndf = pd.DataFrame({\n    'feature1': np.random.randint(0, 100, 100),\n    'feature2': np.random.randint(0, 100, 100),\n    'feature3': np.random.randint(0, 100, 100),\n    'feature4': np.random.randint(0, 100, 100),\n    'target': np.random.randint(0, 2, 100)\n})\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      feature1\n      feature2\n      feature3\n      feature4\n      target\n    \n  \n  \n    \n      0\n      29\n      14\n      66\n      83\n      1\n    \n    \n      1\n      68\n      70\n      87\n      72\n      1\n    \n    \n      2\n      42\n      5\n      40\n      67\n      1\n    \n    \n      3\n      2\n      54\n      79\n      0\n      1\n    \n    \n      4\n      81\n      36\n      35\n      75\n      0\n    \n  \n\n\n\n\n\ntrain_df = df[:50]\nvalidation_df = df[50:80]\n\n\ndt = DecisionTreeClassifier()\ndt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\ndt\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\ndt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n0.5333333333333333\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=2)\ndt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\ndt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n0.5666666666666667\n\n\n\nhyperparams = {'criterion': ['gini', 'entropy'],\n               'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n               'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n\n\nout = {}\nfor c in hyperparams['criterion']:\n    for d in hyperparams['max_depth']:\n        for s in hyperparams['min_samples_split']:\n            dt = DecisionTreeClassifier(criterion=c, max_depth=d, min_samples_split=s)\n            dt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\n            out[(c, d, s)] = dt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n\n\nhp_ser = pd.Series(out)\nhp_ser.sort_values(ascending=False)\n\nentropy  10  10    0.766667\n         7   7     0.766667\n         9   8     0.766667\n         8   10    0.766667\n             9     0.766667\n                     ...   \ngini     10  5     0.500000\n         8   3     0.500000\n         7   4     0.500000\n             3     0.500000\n         5   2     0.500000\nLength: 162, dtype: float64\n\n\n\nhp_ser.idxmax()\n\n('entropy', 4, 6)\n\n\n\nbest_dt = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)\n\n\nbest_dt.fit(df[:80][['feature1', 'feature2', 'feature3', 'feature4']], df[:80]['target'])\n\nDecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6)\n\n\n\nbest_dt.score(df[80:][['feature1', 'feature2', 'feature3', 'feature4']], df[80:]['target'])\n\n0.45\n\n\n\n\nWithout using multiple nested loops\n\nprint(hyperparams.items(), len(hyperparams.items()))\n\ndict_items([('criterion', ['gini', 'entropy']), ('max_depth', [2, 3, 4, 5, 6, 7, 8, 9, 10]), ('min_samples_split', [2, 3, 4, 5, 6, 7, 8, 9, 10])]) 3\n\n\n\ndef print_vec(x, y, z):\n    print(f\"[{x} \\n{y} \\n{z}]\")\nprint_vec(*hyperparams.items())\n\n[('criterion', ['gini', 'entropy']) \n('max_depth', [2, 3, 4, 5, 6, 7, 8, 9, 10]) \n('min_samples_split', [2, 3, 4, 5, 6, 7, 8, 9, 10])]\n\n\n\nlist(zip(*hyperparams.items()))\n\n[('criterion', 'max_depth', 'min_samples_split'),\n (['gini', 'entropy'],\n  [2, 3, 4, 5, 6, 7, 8, 9, 10],\n  [2, 3, 4, 5, 6, 7, 8, 9, 10])]\n\n\n\nkeys, values = zip(*hyperparams.items())\n\n\nkeys\n\n('criterion', 'max_depth', 'min_samples_split')\n\n\n\nvalues\n\n(['gini', 'entropy'],\n [2, 3, 4, 5, 6, 7, 8, 9, 10],\n [2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n\nimport itertools\nlist(itertools.product(*values))[::10]\n\n[('gini', 2, 2),\n ('gini', 3, 3),\n ('gini', 4, 4),\n ('gini', 5, 5),\n ('gini', 6, 6),\n ('gini', 7, 7),\n ('gini', 8, 8),\n ('gini', 9, 9),\n ('gini', 10, 10),\n ('entropy', 3, 2),\n ('entropy', 4, 3),\n ('entropy', 5, 4),\n ('entropy', 6, 5),\n ('entropy', 7, 6),\n ('entropy', 8, 7),\n ('entropy', 9, 8),\n ('entropy', 10, 9)]\n\n\n\nv = next(itertools.product(*values))\nprint(v)\n\n('gini', 2, 2)\n\n\n\nprint_vec(*zip(keys, v))\n\n[('criterion', 'gini') \n('max_depth', 2) \n('min_samples_split', 2)]\n\n\n\ndef print_dict(**kwargs):\n    print(kwargs)\n\nprint_dict(**(dict(zip(keys, v))))\n\n{'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2}\n\n\n\nout = {}\nfor v in itertools.product(*values):\n    params = dict(zip(keys, v))\n    dt= DecisionTreeClassifier(**params)\n    dt.fit(train_df[['feature1', 'feature2', 'feature3', 'feature4']], train_df['target'])\n    out[(params['criterion'], params['max_depth'], params['min_samples_split'])] = dt.score(validation_df[['feature1', 'feature2', 'feature3', 'feature4']], validation_df['target'])\n    \n\n\npd.Series(out).sort_values(ascending=False)\n\nentropy  10  10    0.766667\n         7   7     0.766667\n         9   8     0.766667\n         8   10    0.766667\n             9     0.766667\n                     ...   \n         3   3     0.500000\n             4     0.500000\n             5     0.500000\ngini     10  5     0.500000\n             3     0.500000\nLength: 162, dtype: float64"
  },
  {
    "objectID": "notebooks/posts/gd.html",
    "href": "notebooks/posts/gd.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "import jax.numpy as jnp\nfrom jax import random, jit, vmap, grad, jacfwd, jacrev, hessian, value_and_grad\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Simple 2D quadratic function\ndef f(theta_0, theta_1):\n    return theta_0**2 + theta_1**2\n\n\n# Plot surface and contour plots for f using jax.vmap\ndef create_plot(f):\n    theta_0 = jnp.linspace(-2, 2, 100)\n    theta_1 = jnp.linspace(-2, 2, 100)\n    theta_0, theta_1 = jnp.meshgrid(theta_0, theta_1)\n    f_vmap = jnp.vectorize(f, signature='(),()->()')\n    f_vals = f_vmap(theta_0, theta_1)\n\n    # Create a figure with 2 subplots (3d surface and 2d contour)\n    fig = plt.figure(figsize=(12, 4))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122)\n\n    # Plot surface and contour plots\n    temp = ax1.plot_surface(theta_0, theta_1, f_vals, cmap='viridis')\n\n    # Filled contour plot and marked level set values using clabel\n    # Set 20 levels between min and max of f_vals\n    levels = jnp.linspace(0.5, int(jnp.max(f_vals))+0.5, 11)\n\n    contours = ax2.contour(theta_0, theta_1, f_vals, levels=levels, cmap='viridis')\n    ax2.clabel(contours, inline=True, fontsize=8)\n\n    # Fill using imshow\n    ax2.imshow(f_vals, extent=[-2, 2, -2, 2], origin='lower', cmap='viridis', alpha=0.5)\n  \n    # Find the global minimum of f using jax.scipy.optimize.minimize\n    from jax.scipy.optimize import minimize\n    def f_min(theta):\n        return f(theta[0], theta[1])\n    res = minimize(f_min, jnp.array([0., 0.]), method='BFGS')\n    theta_min = res.x\n    f_min = res.fun\n    print(f'Global minimum: {f_min} at {theta_min}')\n    # Plot the global minimum\n    ax2.scatter(theta_min[0], theta_min[1], marker='x', color='red', s=100)\n\n    \n\n    ax2.set_aspect('equal')\n\n    # Add labels\n    ax1.set_xlabel(r'$\\theta_0$')\n    ax1.set_ylabel(r'$\\theta_1$')\n    ax1.set_zlabel(r'$f(\\theta_0, \\theta_1)$')\n    ax2.set_xlabel(r'$\\theta_0$')\n    ax2.set_ylabel(r'$\\theta_1$')\n\n    # Add colorbar\n    fig.colorbar(temp, ax=ax1, shrink=0.5, aspect=5)\n\n    # Tight layout\n    plt.tight_layout()\n\n\ncreate_plot(f)\n\nGlobal minimum: 0.0 at [0. 0.]\n\n\n\n\n\n\n# Gradient of f at a given point\ndef grad_f(theta_0, theta_1):\n    return grad(f, argnums=(0, 1))(theta_0, theta_1)\n\n\ngrad_f(2., 1.)\n\n(Array(4., dtype=float32, weak_type=True),\n Array(2., dtype=float32, weak_type=True))\n\n\n\ntheta = jnp.array([2., 1.])\ntheta\n\nArray([2., 1.], dtype=float32)\n\n\n\nf(*theta)\n\nArray(5., dtype=float32)\n\n\n\njnp.array(grad_f(*theta))\n\nArray([4., 2.], dtype=float32)\n\n\n\nlr = 0.1\ntheta = theta- lr * jnp.array(grad_f(*theta))\ntheta\n\nArray([1.6, 0.8], dtype=float32)\n\n\n\nf(*theta)\n\nArray(3.2000003, dtype=float32)\n\n\n\n# Gradient descent loop\n\n# Initial parameters\ntheta = jnp.array([2., 1.])\n\n# Store parameters and function values for plotting\ntheta_vals = [theta]\nf_vals = [f(*theta)]\n\nfor i in range(10):\n    theta = theta - lr * jnp.array(grad_f(*theta))\n    theta_vals.append(theta)\n    f_vals.append(f(*theta))\n    print(f'Iteration {i}: theta = {theta}, f = {f(*theta)}')\n\ntheta_vals = jnp.array(theta_vals)\nf_vals = jnp.array(f_vals)\n\nIteration 0: theta = [1.6 0.8], f = 3.200000286102295\nIteration 1: theta = [1.28 0.64], f = 2.047999858856201\nIteration 2: theta = [1.0239999  0.51199996], f = 1.3107198476791382\nIteration 3: theta = [0.8191999  0.40959996], f = 0.8388606309890747\nIteration 4: theta = [0.6553599  0.32767996], f = 0.5368707776069641\nIteration 5: theta = [0.52428794 0.26214397], f = 0.34359729290008545\nIteration 6: theta = [0.41943035 0.20971517], f = 0.21990226209163666\nIteration 7: theta = [0.3355443  0.16777214], f = 0.14073745906352997\nIteration 8: theta = [0.26843542 0.13421771], f = 0.09007196873426437\nIteration 9: theta = [0.21474834 0.10737417], f = 0.05764605849981308\n\n\n\n# Plot the cost vs iterations\nplt.plot(f_vals)\n\n\n\n\n\n# Simple dataset for linear regression\n\nX = jnp.array([[1.], [2.], [3.]])\ny = jnp.array([1., 2.2, 2.8])\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlr.coef_, lr.intercept_\n\n(array([0.9000001], dtype=float32), 0.19999981)\n\n\n\n# Cost function for linear regression using jax.vmap\ndef cost(theta_0, theta_1):\n    y_hat = (theta_0 + theta_1 * X).flatten()\n    #print(y_hat, y, y-y_hat, (y-y_hat)**2)\n    return jnp.mean((y_hat- y)**2)\n    \n# Plot surface and contour plots for cost function\n#create_plot(cost)\n\n\ncost(2.0, 2.0)\n\nArray(16.826666, dtype=float32)\n\n\n\n(3**2 + 3.8**2 + 5.2**2)/3.\n\n16.826666666666668\n\n\n\n# Gradient of cost function at a given point\ndef grad_cost(theta_0, theta_1):\n    return jnp.array(grad(cost, argnums=(0, 1))(theta_0, theta_1))\n\ngrad_cost(2.0, 2.0)\n\nArray([ 8.      , 17.466667], dtype=float32)\n\n\n\ndef grad_cost_manual(theta_0, theta_1):\n    y_hat = (theta_0 + theta_1 * X).flatten()\n    return jnp.array([2*jnp.mean(y_hat - y), 2*jnp.mean((y_hat - y) * X.flatten())])\n\n\ngrad_cost_manual(2.0, 2.0)\n\nArray([ 8.      , 17.466667], dtype=float32)\n\n\n\n# Plotting cost surface and contours for three points in X individually\n\ndef cost_i(theta_0, theta_1, i = 1):\n    y_hat = theta_0 + theta_1 * X[i-1:i]\n    return jnp.mean((y_hat- y[i-1:i])**2)\n\n\n(cost_i(2.0, 2.0, 1) + cost_i(2.0, 2.0, 2) + cost_i(2.0, 2.0, 3))/3.0\n\nArray(16.826666, dtype=float32)\n\n\n\nfrom functools import partial\n\n\n# Plot surface and contour plots for cost function\nfor i in range(1, 4):\n    cost_i_p = partial(cost_i, i=i)\n    create_plot(cost_i_p)\n\nGlobal minimum: 0.0 at [0.5 0.5]\nGlobal minimum: 0.0 at [0.44000003 0.88000005]\nGlobal minimum: 0.0 at [0.28000003 0.84      ]\n\n\n\n\n\n\n\n\n\n\n\n\ngrad_cost_1 = grad(cost_i, argnums=(0, 1))\ngrad_cost_1(2.0, 2.0)\n\n(Array(6., dtype=float32, weak_type=True),\n Array(6., dtype=float32, weak_type=True))\n\n\n\njnp.array(grad_cost_1(2.0, 2.0, 1)), jnp.array(grad_cost_1(2.0, 2.0, 2)), jnp.array(grad_cost_1(2.0, 2.0, 3))\n\n(Array([6., 6.], dtype=float32),\n Array([ 7.6, 15.2], dtype=float32),\n Array([10.4     , 31.199999], dtype=float32))"
  },
  {
    "objectID": "notebooks/posts/mle.html",
    "href": "notebooks/posts/mle.html",
    "title": "ES654",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndata = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n\n\nn_h = np.sum(data)\nn_t = len(data) - n_h\n    \n\n\nn_h, n_t\n\n(8, 2)\n\n\n\ndef likelihood(theta):\n    return theta**n_h * (1-theta)**n_t\n\n\ndef log_likelihood(theta):\n    return n_h * np.log(theta) + n_t * np.log(1-theta)\n\n\nlikelihood(0.1)\n\n8.100000000000005e-09\n\n\n\nlikelihood(0.9)\n\n0.004304672099999999\n\n\n\nlikelihood(0.8)\n\n0.0067108864\n\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\nx_lin = np.linspace(0.01, 0.99, 100)\ny_lin = likelihood(x_lin)\ny_lin_ll = log_likelihood(x_lin)\nax[0].plot(x_lin, y_lin)\nax[1].plot(x_lin, y_lin_ll)\n\n\n\n\n\nimport torch\n\ntheta = 0.2\nbn = torch.distributions.Bernoulli(probs= theta)\n\ndata = []\nfor i in range(100):\n    data.append(bn.sample())\n\ntensor(59.)"
  },
  {
    "objectID": "notebooks/posts/log-sum-exp.html",
    "href": "notebooks/posts/log-sum-exp.html",
    "title": "ES654",
    "section": "",
    "text": "import torch\n\n\nxs = torch.linspace(0.01, 1, 1000)\nxs\n\ntensor([0.0100, 0.0110, 0.0120, 0.0130, 0.0140, 0.0150, 0.0159, 0.0169, 0.0179,\n        0.0189, 0.0199, 0.0209, 0.0219, 0.0229, 0.0239, 0.0249, 0.0259, 0.0268,\n        0.0278, 0.0288, 0.0298, 0.0308, 0.0318, 0.0328, 0.0338, 0.0348, 0.0358,\n        0.0368, 0.0377, 0.0387, 0.0397, 0.0407, 0.0417, 0.0427, 0.0437, 0.0447,\n        0.0457, 0.0467, 0.0477, 0.0486, 0.0496, 0.0506, 0.0516, 0.0526, 0.0536,\n        0.0546, 0.0556, 0.0566, 0.0576, 0.0586, 0.0595, 0.0605, 0.0615, 0.0625,\n        0.0635, 0.0645, 0.0655, 0.0665, 0.0675, 0.0685, 0.0695, 0.0705, 0.0714,\n        0.0724, 0.0734, 0.0744, 0.0754, 0.0764, 0.0774, 0.0784, 0.0794, 0.0804,\n        0.0814, 0.0823, 0.0833, 0.0843, 0.0853, 0.0863, 0.0873, 0.0883, 0.0893,\n        0.0903, 0.0913, 0.0923, 0.0932, 0.0942, 0.0952, 0.0962, 0.0972, 0.0982,\n        0.0992, 0.1002, 0.1012, 0.1022, 0.1032, 0.1041, 0.1051, 0.1061, 0.1071,\n        0.1081, 0.1091, 0.1101, 0.1111, 0.1121, 0.1131, 0.1141, 0.1150, 0.1160,\n        0.1170, 0.1180, 0.1190, 0.1200, 0.1210, 0.1220, 0.1230, 0.1240, 0.1250,\n        0.1259, 0.1269, 0.1279, 0.1289, 0.1299, 0.1309, 0.1319, 0.1329, 0.1339,\n        0.1349, 0.1359, 0.1368, 0.1378, 0.1388, 0.1398, 0.1408, 0.1418, 0.1428,\n        0.1438, 0.1448, 0.1458, 0.1468, 0.1477, 0.1487, 0.1497, 0.1507, 0.1517,\n        0.1527, 0.1537, 0.1547, 0.1557, 0.1567, 0.1577, 0.1586, 0.1596, 0.1606,\n        0.1616, 0.1626, 0.1636, 0.1646, 0.1656, 0.1666, 0.1676, 0.1686, 0.1695,\n        0.1705, 0.1715, 0.1725, 0.1735, 0.1745, 0.1755, 0.1765, 0.1775, 0.1785,\n        0.1795, 0.1805, 0.1814, 0.1824, 0.1834, 0.1844, 0.1854, 0.1864, 0.1874,\n        0.1884, 0.1894, 0.1904, 0.1914, 0.1923, 0.1933, 0.1943, 0.1953, 0.1963,\n        0.1973, 0.1983, 0.1993, 0.2003, 0.2013, 0.2023, 0.2032, 0.2042, 0.2052,\n        0.2062, 0.2072, 0.2082, 0.2092, 0.2102, 0.2112, 0.2122, 0.2132, 0.2141,\n        0.2151, 0.2161, 0.2171, 0.2181, 0.2191, 0.2201, 0.2211, 0.2221, 0.2231,\n        0.2241, 0.2250, 0.2260, 0.2270, 0.2280, 0.2290, 0.2300, 0.2310, 0.2320,\n        0.2330, 0.2340, 0.2350, 0.2359, 0.2369, 0.2379, 0.2389, 0.2399, 0.2409,\n        0.2419, 0.2429, 0.2439, 0.2449, 0.2459, 0.2468, 0.2478, 0.2488, 0.2498,\n        0.2508, 0.2518, 0.2528, 0.2538, 0.2548, 0.2558, 0.2568, 0.2577, 0.2587,\n        0.2597, 0.2607, 0.2617, 0.2627, 0.2637, 0.2647, 0.2657, 0.2667, 0.2677,\n        0.2686, 0.2696, 0.2706, 0.2716, 0.2726, 0.2736, 0.2746, 0.2756, 0.2766,\n        0.2776, 0.2786, 0.2795, 0.2805, 0.2815, 0.2825, 0.2835, 0.2845, 0.2855,\n        0.2865, 0.2875, 0.2885, 0.2895, 0.2905, 0.2914, 0.2924, 0.2934, 0.2944,\n        0.2954, 0.2964, 0.2974, 0.2984, 0.2994, 0.3004, 0.3014, 0.3023, 0.3033,\n        0.3043, 0.3053, 0.3063, 0.3073, 0.3083, 0.3093, 0.3103, 0.3113, 0.3123,\n        0.3132, 0.3142, 0.3152, 0.3162, 0.3172, 0.3182, 0.3192, 0.3202, 0.3212,\n        0.3222, 0.3232, 0.3241, 0.3251, 0.3261, 0.3271, 0.3281, 0.3291, 0.3301,\n        0.3311, 0.3321, 0.3331, 0.3341, 0.3350, 0.3360, 0.3370, 0.3380, 0.3390,\n        0.3400, 0.3410, 0.3420, 0.3430, 0.3440, 0.3450, 0.3459, 0.3469, 0.3479,\n        0.3489, 0.3499, 0.3509, 0.3519, 0.3529, 0.3539, 0.3549, 0.3559, 0.3568,\n        0.3578, 0.3588, 0.3598, 0.3608, 0.3618, 0.3628, 0.3638, 0.3648, 0.3658,\n        0.3668, 0.3677, 0.3687, 0.3697, 0.3707, 0.3717, 0.3727, 0.3737, 0.3747,\n        0.3757, 0.3767, 0.3777, 0.3786, 0.3796, 0.3806, 0.3816, 0.3826, 0.3836,\n        0.3846, 0.3856, 0.3866, 0.3876, 0.3886, 0.3895, 0.3905, 0.3915, 0.3925,\n        0.3935, 0.3945, 0.3955, 0.3965, 0.3975, 0.3985, 0.3995, 0.4005, 0.4014,\n        0.4024, 0.4034, 0.4044, 0.4054, 0.4064, 0.4074, 0.4084, 0.4094, 0.4104,\n        0.4114, 0.4123, 0.4133, 0.4143, 0.4153, 0.4163, 0.4173, 0.4183, 0.4193,\n        0.4203, 0.4213, 0.4223, 0.4232, 0.4242, 0.4252, 0.4262, 0.4272, 0.4282,\n        0.4292, 0.4302, 0.4312, 0.4322, 0.4332, 0.4341, 0.4351, 0.4361, 0.4371,\n        0.4381, 0.4391, 0.4401, 0.4411, 0.4421, 0.4431, 0.4441, 0.4450, 0.4460,\n        0.4470, 0.4480, 0.4490, 0.4500, 0.4510, 0.4520, 0.4530, 0.4540, 0.4550,\n        0.4559, 0.4569, 0.4579, 0.4589, 0.4599, 0.4609, 0.4619, 0.4629, 0.4639,\n        0.4649, 0.4659, 0.4668, 0.4678, 0.4688, 0.4698, 0.4708, 0.4718, 0.4728,\n        0.4738, 0.4748, 0.4758, 0.4768, 0.4777, 0.4787, 0.4797, 0.4807, 0.4817,\n        0.4827, 0.4837, 0.4847, 0.4857, 0.4867, 0.4877, 0.4886, 0.4896, 0.4906,\n        0.4916, 0.4926, 0.4936, 0.4946, 0.4956, 0.4966, 0.4976, 0.4986, 0.4995,\n        0.5005, 0.5015, 0.5025, 0.5035, 0.5045, 0.5055, 0.5065, 0.5075, 0.5085,\n        0.5095, 0.5105, 0.5114, 0.5124, 0.5134, 0.5144, 0.5154, 0.5164, 0.5174,\n        0.5184, 0.5194, 0.5204, 0.5214, 0.5223, 0.5233, 0.5243, 0.5253, 0.5263,\n        0.5273, 0.5283, 0.5293, 0.5303, 0.5313, 0.5323, 0.5332, 0.5342, 0.5352,\n        0.5362, 0.5372, 0.5382, 0.5392, 0.5402, 0.5412, 0.5422, 0.5432, 0.5441,\n        0.5451, 0.5461, 0.5471, 0.5481, 0.5491, 0.5501, 0.5511, 0.5521, 0.5531,\n        0.5541, 0.5550, 0.5560, 0.5570, 0.5580, 0.5590, 0.5600, 0.5610, 0.5620,\n        0.5630, 0.5640, 0.5650, 0.5659, 0.5669, 0.5679, 0.5689, 0.5699, 0.5709,\n        0.5719, 0.5729, 0.5739, 0.5749, 0.5759, 0.5768, 0.5778, 0.5788, 0.5798,\n        0.5808, 0.5818, 0.5828, 0.5838, 0.5848, 0.5858, 0.5868, 0.5877, 0.5887,\n        0.5897, 0.5907, 0.5917, 0.5927, 0.5937, 0.5947, 0.5957, 0.5967, 0.5977,\n        0.5986, 0.5996, 0.6006, 0.6016, 0.6026, 0.6036, 0.6046, 0.6056, 0.6066,\n        0.6076, 0.6086, 0.6095, 0.6105, 0.6115, 0.6125, 0.6135, 0.6145, 0.6155,\n        0.6165, 0.6175, 0.6185, 0.6195, 0.6205, 0.6214, 0.6224, 0.6234, 0.6244,\n        0.6254, 0.6264, 0.6274, 0.6284, 0.6294, 0.6304, 0.6314, 0.6323, 0.6333,\n        0.6343, 0.6353, 0.6363, 0.6373, 0.6383, 0.6393, 0.6403, 0.6413, 0.6423,\n        0.6432, 0.6442, 0.6452, 0.6462, 0.6472, 0.6482, 0.6492, 0.6502, 0.6512,\n        0.6522, 0.6532, 0.6541, 0.6551, 0.6561, 0.6571, 0.6581, 0.6591, 0.6601,\n        0.6611, 0.6621, 0.6631, 0.6641, 0.6650, 0.6660, 0.6670, 0.6680, 0.6690,\n        0.6700, 0.6710, 0.6720, 0.6730, 0.6740, 0.6750, 0.6759, 0.6769, 0.6779,\n        0.6789, 0.6799, 0.6809, 0.6819, 0.6829, 0.6839, 0.6849, 0.6859, 0.6868,\n        0.6878, 0.6888, 0.6898, 0.6908, 0.6918, 0.6928, 0.6938, 0.6948, 0.6958,\n        0.6968, 0.6977, 0.6987, 0.6997, 0.7007, 0.7017, 0.7027, 0.7037, 0.7047,\n        0.7057, 0.7067, 0.7077, 0.7086, 0.7096, 0.7106, 0.7116, 0.7126, 0.7136,\n        0.7146, 0.7156, 0.7166, 0.7176, 0.7186, 0.7195, 0.7205, 0.7215, 0.7225,\n        0.7235, 0.7245, 0.7255, 0.7265, 0.7275, 0.7285, 0.7295, 0.7305, 0.7314,\n        0.7324, 0.7334, 0.7344, 0.7354, 0.7364, 0.7374, 0.7384, 0.7394, 0.7404,\n        0.7414, 0.7423, 0.7433, 0.7443, 0.7453, 0.7463, 0.7473, 0.7483, 0.7493,\n        0.7503, 0.7513, 0.7523, 0.7532, 0.7542, 0.7552, 0.7562, 0.7572, 0.7582,\n        0.7592, 0.7602, 0.7612, 0.7622, 0.7632, 0.7641, 0.7651, 0.7661, 0.7671,\n        0.7681, 0.7691, 0.7701, 0.7711, 0.7721, 0.7731, 0.7741, 0.7750, 0.7760,\n        0.7770, 0.7780, 0.7790, 0.7800, 0.7810, 0.7820, 0.7830, 0.7840, 0.7850,\n        0.7859, 0.7869, 0.7879, 0.7889, 0.7899, 0.7909, 0.7919, 0.7929, 0.7939,\n        0.7949, 0.7959, 0.7968, 0.7978, 0.7988, 0.7998, 0.8008, 0.8018, 0.8028,\n        0.8038, 0.8048, 0.8058, 0.8068, 0.8077, 0.8087, 0.8097, 0.8107, 0.8117,\n        0.8127, 0.8137, 0.8147, 0.8157, 0.8167, 0.8177, 0.8186, 0.8196, 0.8206,\n        0.8216, 0.8226, 0.8236, 0.8246, 0.8256, 0.8266, 0.8276, 0.8286, 0.8295,\n        0.8305, 0.8315, 0.8325, 0.8335, 0.8345, 0.8355, 0.8365, 0.8375, 0.8385,\n        0.8395, 0.8405, 0.8414, 0.8424, 0.8434, 0.8444, 0.8454, 0.8464, 0.8474,\n        0.8484, 0.8494, 0.8504, 0.8514, 0.8523, 0.8533, 0.8543, 0.8553, 0.8563,\n        0.8573, 0.8583, 0.8593, 0.8603, 0.8613, 0.8623, 0.8632, 0.8642, 0.8652,\n        0.8662, 0.8672, 0.8682, 0.8692, 0.8702, 0.8712, 0.8722, 0.8732, 0.8741,\n        0.8751, 0.8761, 0.8771, 0.8781, 0.8791, 0.8801, 0.8811, 0.8821, 0.8831,\n        0.8841, 0.8850, 0.8860, 0.8870, 0.8880, 0.8890, 0.8900, 0.8910, 0.8920,\n        0.8930, 0.8940, 0.8950, 0.8959, 0.8969, 0.8979, 0.8989, 0.8999, 0.9009,\n        0.9019, 0.9029, 0.9039, 0.9049, 0.9059, 0.9068, 0.9078, 0.9088, 0.9098,\n        0.9108, 0.9118, 0.9128, 0.9138, 0.9148, 0.9158, 0.9168, 0.9177, 0.9187,\n        0.9197, 0.9207, 0.9217, 0.9227, 0.9237, 0.9247, 0.9257, 0.9267, 0.9277,\n        0.9286, 0.9296, 0.9306, 0.9316, 0.9326, 0.9336, 0.9346, 0.9356, 0.9366,\n        0.9376, 0.9386, 0.9395, 0.9405, 0.9415, 0.9425, 0.9435, 0.9445, 0.9455,\n        0.9465, 0.9475, 0.9485, 0.9495, 0.9505, 0.9514, 0.9524, 0.9534, 0.9544,\n        0.9554, 0.9564, 0.9574, 0.9584, 0.9594, 0.9604, 0.9614, 0.9623, 0.9633,\n        0.9643, 0.9653, 0.9663, 0.9673, 0.9683, 0.9693, 0.9703, 0.9713, 0.9723,\n        0.9732, 0.9742, 0.9752, 0.9762, 0.9772, 0.9782, 0.9792, 0.9802, 0.9812,\n        0.9822, 0.9832, 0.9841, 0.9851, 0.9861, 0.9871, 0.9881, 0.9891, 0.9901,\n        0.9911, 0.9921, 0.9931, 0.9941, 0.9950, 0.9960, 0.9970, 0.9980, 0.9990,\n        1.0000])\n\n\n\nxs.prod()\n\ntensor(0.)\n\n\n\nxs = torch.linspace(0.01, 1, 100)\nxs.prod()\n\ntensor(9.3326e-43)\n\n\n\nxs = torch.linspace(0.01, 1, 1000)\na = xs.log().sum()\n\nb = torch.linspace(0.01, 1, 10000).log().sum()\nprint(a, b)\n\ntensor(-954.8404) tensor(-9536.1816)\n\n\n\ntorch.linspace(0.01, 1, 1000).log().sum()\n\ntensor(-954.8404)\n\n\n\ntorch.linspace(0.01, 1, 10000).log().sum()\n\ntensor(-9536.1816)\n\n\n\n# Trivial example to test the binary cross entropy loss\n\ny_true = torch.tensor([0., 1., 1., 0.])\nlogits = torch.tensor([0.1, 0.9, 0.8, 0.2])\n\nloss = torch.nn.BCEWithLogitsLoss()\nloss(logits, y_true)\n\ntensor(0.5637)\n\n\n\n\ny_hat = torch.nn.Sigmoid()(logits)\ny_hat\n\ntensor([0.5250, 0.7109, 0.6900, 0.5498])\n\n\n\n# BCE\n-(y_true * torch.log(y_hat) + (1 - y_true) * torch.log(1 - y_hat)).mean()\n\ntensor(0.5637)\n\n\n\ndef our_bce(y_true, logits):\n    y_hat = torch.nn.Sigmoid()(logits)\n    return -(y_true * torch.log(y_hat) + (1 - y_true) * torch.log(1 - y_hat)).mean()\n\n\n# check that our implementation is correct\nassert our_bce(y_true, logits) == loss(logits, y_true)\n\n\n# Larger logits\nlogits = torch.tensor([100., 200., 300., 400.])\n\nassert our_bce(y_true, logits) == loss(logits, y_true)\n\n\nAssertionError: \n\n\n\ntorch.nn.Sigmoid()(logits)\n\ntensor([1., 1., 1., 1.])\n\n\n\ntorch.exp(logits)/(1 + torch.exp(logits))\n\ntensor([nan, nan, nan, nan])\n\n\n\ntorch.exp(-logits)\n\ntensor([3.7835e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n\n\n\n1/(1+torch.exp(-logits))\n\ntensor([1., 1., 1., 1.])"
  },
  {
    "objectID": "notebooks/posts/logistic.html",
    "href": "notebooks/posts/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "# Create linearly separable data in 2d\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n                            n_clusters_per_class=2, class_sep=1.5, random_state=42)\n\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')\n\n<matplotlib.collections.PathCollection at 0x12b5ed280>\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Without regularization\nclf = LogisticRegression(random_state=0, penalty='none').fit(X, y)\nclf.coef_, clf.intercept_\n\n(array([[26.23339925, -5.01002931]]), array([1.74951957]))\n\n\n\n# Create a surface plot of the decision boundary for any theta_0, theta_1, theta_2\n\ndef plot_decision_boundary(theta_0, theta_1, theta_2):\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = -(theta_0 + theta_1 * x_lin) / theta_2\n    plt.plot(x_lin, y_lin, 'k--', label='Decision boundary ($\\sigma(z) = 0.5$))', lw=5)\n    plt.xlim(-4, 4)\n    plt.ylim(-4, 4)\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n\n    # Plot the probability of class 1 contour\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n    plt.contourf(x1, x2, z, linestyles='dashed')\n    plt.colorbar()\n\n    # Plot the data\n    plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')\n    plt.legend(loc='best')\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary(theta_0, theta_1, theta_2)>\n\n\n\n# Create a 3d plot of the decision boundary for any theta_0, theta_1, theta_2\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = np.linspace(-4, 4, 100)\n    X_g, Y_g = np.meshgrid(x_lin, y_lin)\n    Z_g = -(theta_0 + theta_1 * X_g + theta_2 * Y_g)\n    #ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2)\n    ax.set_xlabel(r'$x_1$')\n    ax.set_ylabel(r'$x_2$')\n    ax.set_zlabel(r'$x_3$')\n    ax.set_title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n    \n    # Scatter plot of data (class 1 is Z = 1, class 0 is Z = 0)\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n    # Plot the 3d sigmoid function\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n    ax.plot_surface(x1, x2, z, alpha=0.2, color='green')\n    \n    # Rotate the plot so that the sigmoid function is visible\n    ax.view_init(azim, elev)\n\n    # Plot the decision plane\n    ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2, color='k')\n\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary_3d,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1),\n            azim=FloatSlider(min=-180, max=180, step=1, value=30),\n            elev=FloatSlider(min=-180, max=180, step=1, value=30))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30)>\n\n\n\n# Create two 3d plot any theta_0, theta_1, theta_2\n# First showing the decision boundary\n# Second showing the probability of class 1\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30):\n    fig = plt.figure(figsize=(10, 8))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122, projection='3d')\n\n    x_lin = np.linspace(-4, 4, 100)\n    y_lin = np.linspace(-4, 4, 100)\n    X_g, Y_g = np.meshgrid(x_lin, y_lin)\n    Z_g = -(theta_0 + theta_1 * X_g + theta_2 * Y_g)\n    #ax.plot_surface(X_g, Y_g, Z_g, alpha=0.2)\n    ax1.set_xlabel(r'$x_1$')\n    ax1.set_ylabel(r'$x_2$')\n    ax1.set_zlabel(r'$x_3$')\n    ax1.set_title(r'$\\theta_0 = {:.2f}, \\theta_1 = {:.2f}, \\theta_2 = {:.2f}$'.format(theta_0, theta_1, theta_2))\n\n    # Scatter plot of data (class 1 is Z = 1, class 0 is Z = 0)\n    ax1.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax1.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n    # Plot the 3d sigmoid function\n    x1, x2 = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-4, 4, 100))\n    z = 1 / (1 + np.exp(-(theta_0 + theta_1 * x1 + theta_2 * x2)))\n\n    # Plot the decision plane\n    ax1.plot_surface(X_g, Y_g, Z_g, alpha=0.2, color='k')\n\n    # Plot the probability of class 1\n    ax2.plot_surface(x1, x2, z, alpha=0.2, color='black')\n    ax2.scatter(X[y == 1, 0], X[y == 1, 1], 1, marker='o', c='b', s=25, edgecolor='k')\n    ax2.scatter(X[y == 0, 0], X[y == 0, 1], 0, marker='o', c='y', s=25, edgecolor='k')\n\n\n     # Rotate the plot so that the sigmoid function is visible\n    ax1.view_init(azim, elev)\n    ax2.view_init(azim, elev)\n\n\n# Create a slider widget to explore the decision boundary\n\nfrom ipywidgets import interact, FloatSlider\n\ninteract(plot_decision_boundary_3d,\n            theta_0=FloatSlider(min=-2, max=3, step=0.1, value=0.1),\n            theta_1=FloatSlider(min=-5, max=40, step=0.5, value=0.1),\n            theta_2=FloatSlider(min=-10, max=5, step=0.1, value=0.1),\n            azim=FloatSlider(min=-180, max=180, step=1, value=30),\n            elev=FloatSlider(min=-180, max=180, step=1, value=30))\n\n\n\n\n\n\n<function __main__.plot_decision_boundary_3d(theta_0, theta_1, theta_2, azim=30, elev=30)>"
  },
  {
    "objectID": "notebooks/posts/lowrank-matrix.html",
    "href": "notebooks/posts/lowrank-matrix.html",
    "title": "Maths and JAX: Low Rank",
    "section": "",
    "text": "Multiplying a matrix A with a vector x transforms x\n\n\n\n\nTransforming a vector via a low rank matrix in the shown examples leads to a line\n\nWe first study Goal 1. The interpretation of matrix vector product is borrowed from the excellent videos from the 3Blue1Brown channel. I’ll first set up the environment by importing a few relevant libraries.\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nfrom sympy import Matrix, MatrixSymbol, Eq, MatMul\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=0.75)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nsympy_A = MatrixSymbol(\"A\", 2, 2)\nsympy_x = MatrixSymbol(\"x\", 2, 1)\ny = MatrixSymbol(\"y\", 2, 1)\n\nEq(y, sympy_A*sympy_x, evaluate=False)\n\n\\(\\displaystyle y = A x\\)\n\n\nGiven a matrix A and a vector x, we are trying to get y=Ax. Let us first see the values for a specific instance in the 2d space.\n\nA = np.array([[2, 1], [1, 4]])\n\nx = np.array([1, 1])\nAx = A @ x\n\nEq(Matrix(Ax), MatMul(Matrix(A), Matrix(x)),evaluate=False)\n\n\\(\\displaystyle \\left[\\begin{matrix}3\\\\5\\end{matrix}\\right] = \\left[\\begin{matrix}2 & 1\\\\1 & 4\\end{matrix}\\right] \\left[\\begin{matrix}1\\\\1\\end{matrix}\\right]\\)\n\n\nHere, we have A = \\(\\left[\\begin{matrix}2 & 1\\\\1 & 4\\end{matrix}\\right]\\) and x = \\({\\text{[1 1]}}\\)\nNow some code to create arrows to represent arrows.\n\ndef plot_arrow(ax, x, color, label):\n    x_head, y_head = x[0], x[1]\n    x_tail = 0.0\n    y_tail = 0.0\n    dx = x_head - x_tail\n    dy = y_head - y_tail\n\n    arrow = mpatches.FancyArrowPatch(\n        (x_tail, y_tail), (x_head, y_head), mutation_scale=10, color=color, label=label\n    )\n\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6, 1), borderaxespad=0)\n\nNow some code to plot the vector corresponding to Ax\n\ndef plot_transform(A, x):\n    Ax = A @ x\n    fig, ax = plt.subplots()\n    plot_arrow(ax, x, \"k\", f\"Original (x) {x}\")\n    plot_arrow(ax, Ax, \"g\", f\"Transformed (Ax) {Ax}\")\n    plt.xlim((-5, 5))\n    plt.ylim((-5, 5))\n    plt.grid(alpha=0.1)\n    ax.set_aspect(\"equal\")\n    plt.title(f\"A = {A}\")\n    sns.despine(left=True, bottom=True)\n    plt.tight_layout()\n\n\nplot_transform(np.array([[1.0, 1.0], [1.0, -1.0]]), [1.0, 2.0])\nplt.savefig(\"Ax1.png\", dpi=100)\n\n\n\n\nIn the plot above, we can see that the vector [1, 2] is transformed to [3, -1] via the matrix A.\nLet us now write some code to create the rotation matrix and apply it on our input x\n\ndef rot(angle):\n    theta = np.radians(angle)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array(((c, -s), (s, c)))\n    return np.round(R, 2)\n\n\nx = np.array([1.0, 2.0])\nplot_transform(rot(90), x)\nplt.savefig(\"Ax2\", dpi=100)\n\n\n\n\nAs we can see above, creating the 90 degree rotation matrix indeed transforms our vector anticlockwise 90 degrees.\nNow let us talk about matrices A that are low rank. I am creating a simple low rank matrix where the second row is some constant times the first row.\n\ndef plot_lr(x, slope):\n    low_rank = np.array([1.0, 2.0])\n    low_rank = np.vstack((low_rank, slope * low_rank))\n    plot_transform(low_rank, x)\n    x_lin = np.linspace(-5, 5, 100)\n    y = x_lin * slope\n    plt.plot(x_lin, y, alpha=0.4, lw=5, label=f\"y = {slope}x\")\n    plt.legend(bbox_to_anchor=(1.2, 1), borderaxespad=0)\n\n\nplot_lr(x, 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-1.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([1.0, -1.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-2.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([0.5, -0.7], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-3.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([-1.0, 0.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-4.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\nTo summarize\n\nIn the above plots we can see that changing our x to any vector in the 2d space leads to us to transformed vector not covering the whole 2d space, but on line in the 2d space. One can easily take this learning to higher dimensional matrices A."
  },
  {
    "objectID": "notebooks/posts/1d-cnn.html",
    "href": "notebooks/posts/1d-cnn.html",
    "title": "1d CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# Simple 1d dataset\n\ny = torch.Tensor([1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1])\nx = torch.arange(0, len(y))\n\nplt.plot(x, y, 'o-')\n\n\n\n\n\n# Filter 1 (detect silence)\n\nwith torch.no_grad():\n    f1 = nn.Conv1d(1, 1, 3, padding=1)\n    f1.weight.data = torch.Tensor([[[-1, -1, -1]]])\n    f1.bias.data = torch.Tensor([0])\n    y1 = F.relu(f1(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y1, 'o-', label='filtered f1 (silence)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfcccb80>\n\n\n\n\n\n\n# Filter 2 (detect falling edge)\n\nwith torch.no_grad():\n    f2 = nn.Conv1d(1, 1, 3, padding=1)\n    f2.weight.data = torch.Tensor([[[1, 0, -1]]])\n    f2.bias.data = torch.Tensor([0])\n    y2 = F.relu(f2(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y2, 'o-', label='filtered f2 (falling edge)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfbc7250>\n\n\n\n\n\n\n# Filter 3 (detect rising edge)\n\nwith torch.no_grad():\n    f3 = nn.Conv1d(1, 1, 3, padding=1)\n    f3.weight.data = torch.Tensor([[[-1, 0, 1]]])\n    f3.bias.data = torch.Tensor([0])\n    y3 = F.relu(f3(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y3, 'o-', label='filtered f3 (rising edge)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfbb88e0>\n\n\n\n\n\n\n# Filter 4 (detect high amplitude)\n\nwith torch.no_grad():\n    f4 = nn.Conv1d(1, 1, 3, padding=1)\n    f4.weight.data = torch.Tensor([[[1, 1, 1]]])\n    f4.bias.data = torch.Tensor([0])\n    y4 = F.relu(f4(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original') \nplt.plot(x, y4, 'o-', label='filtered f4 (high amplitude)')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7efecfb2dfd0>"
  },
  {
    "objectID": "notebooks/posts/python-utils.html",
    "href": "notebooks/posts/python-utils.html",
    "title": "Some Python Utilities",
    "section": "",
    "text": "Function Argument Unpacking\nReference: https://www.youtube.com/watch?v=YWY4BZi_o28\n\ndef print_vec(x, y, z):\n    print(f\"[{x} \\n{y} \\n{z}]\")\n\n\nprint_vec(1, 2, 3)\n\n[1 \n2 \n3]\n\n\n\ntuple_vec = (1, 0, 1)\n#print_vec(tuple_vec)\nprint_vec(tuple_vec[0], tuple_vec[1], tuple_vec[2])\n\n[1 \n0 \n1]\n\n\n\nlist_vec = [1, 0, 1]\n\nprint_vec(tuple_vec[0], tuple_vec[1], tuple_vec[2])\nprint(\"*\"*20)\nprint_vec(*tuple_vec)\nprint(\"*\"*20)\n\nprint_vec(*list_vec)\n\n[1 \n0 \n1]\n********************\n[1 \n0 \n1]\n********************\n[1 \n0 \n1]\n\n\n\ndictionary_vec = {\"x\": 1, \"y\": 0, \"z\": 1}\nprint_vec(**dictionary_vec)\n\n[1 \n0 \n1]\n\n\n\ndictionary_vec = {\"a\": 1, \"b\": 0, \"c\":1}\nprint_vec(**dictionary_vec)\n\nTypeError: print_vec() got an unexpected keyword argument 'a'\n\n\n\nprint(*dictionary_vec)\n\na b c\n\n\n\n\nZip\n\nlist(zip([1, 2, 3], ['a', 'b', 'c'], [7, 8, 9]))\n\n[(1, 'a', 7), (2, 'b', 8), (3, 'c', 9)]\n\n\n\n\nItertools Product\n\nimport itertools\nlist(itertools.product([1, 2], ['a', 'b', 'c'], [7, 8, 9]))\n\n[(1, 'a', 7),\n (1, 'a', 8),\n (1, 'a', 9),\n (1, 'b', 7),\n (1, 'b', 8),\n (1, 'b', 9),\n (1, 'c', 7),\n (1, 'c', 8),\n (1, 'c', 9),\n (2, 'a', 7),\n (2, 'a', 8),\n (2, 'a', 9),\n (2, 'b', 7),\n (2, 'b', 8),\n (2, 'b', 9),\n (2, 'c', 7),\n (2, 'c', 8),\n (2, 'c', 9)]"
  },
  {
    "objectID": "notebooks/posts/nn.html",
    "href": "notebooks/posts/nn.html",
    "title": "Neural Network",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# MNIST dataset\n\nfrom torchvision import datasets, transforms\nimport torchvision\n\n# Split MNIST into train, validation, and test sets\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n\n# Split train_data into train and validation sets\nval_data = torch.utils.data.Subset(train_data, range(50000, 51000))\n\n# Reduce the size of the training set to 5,000\ntrain_data = torch.utils.data.Subset(train_data, range(0, 5000))\n\n\n# Create data loaders\nbatch_size = 64\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n\nimg, target = next(iter(train_loader))\nprint(img.shape)\nprint(target.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\n\nplt.imshow(img[0].numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\ntarget\n\ntensor([3, 4, 1, 8, 9, 3, 9, 8, 4, 8, 3, 0, 0, 7, 7, 1, 6, 6, 9, 7, 4, 3, 3, 4,\n        5, 7, 3, 2, 8, 4, 8, 2, 8, 3, 1, 4, 2, 1, 4, 8, 5, 3, 5, 1, 8, 7, 3, 7,\n        7, 2, 0, 3, 1, 3, 7, 0, 7, 1, 7, 6, 4, 1, 8, 0])\n\n\n\n# Store the labels and Images in TensorBoard\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Clean up any old runs\n!rm -rf runs\n\n# Default `log_dir` is \"runs\"\nwriter = SummaryWriter('runs/mnist')\n\n# Add images to tensorboard in the form of a grid in batches of 64\ndataiter = iter(DataLoader(train_data, batch_size=64, shuffle=True))\n\n# Add a slider in tensorboard to iterate through the batches\nfor i in range(10):\n    images, labels = next(dataiter)\n    images = torchvision.utils.make_grid(images)\n    # Add images with labels to tensorboard\n    writer.add_image(f'mnist_images_{i}', images, global_step=i)\n\n\n# Define model for 10-class MNIST classification\n\nclass MNISTClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, 10)\n        \n    def forward(self, x):\n        z1 = self.fc1(x)\n        a1 = F.relu(z1)\n        z2 = self.fc2(a1)\n        a2 = F.relu(z2)\n        z3 = self.fc3(a2) # logits\n        return z3\n\n\nmodel = MNISTClassifier()\n\n\n# Forward pass through the model\n\n\n\nsaved_img, saved_target = next(iter(train_loader))\nsaved_img = saved_img.view(saved_img.shape[0], -1)\nprint(saved_img.shape)\n\nplt.imshow(saved_img[0].reshape(28, 28), cmap='gray_r')\nplt.title(saved_target[0].item())\n\n\nwith torch.no_grad():\n    out = model(saved_img.view(saved_img.shape[0], -1))\n\n\n\nprint(out[0])\n\n# Softmax\nprobas = F.softmax(out, dim=1)\nprint(probas[0])\n\ntorch.Size([64, 784])\ntensor([-0.1859, -0.1648,  0.0156,  0.0309,  0.1785, -0.1567, -0.0472,  0.1143,\n        -0.0400, -0.0006])\ntensor([0.0846, 0.0864, 0.1035, 0.1051, 0.1218, 0.0871, 0.0972, 0.1143, 0.0979,\n        0.1019])\n\n\n\n\n\n\nprobas[0:1]\n\ntensor([[0.0846, 0.0864, 0.1035, 0.1051, 0.1218, 0.0871, 0.0972, 0.1143, 0.0979,\n         0.1019]])\n\n\n\n# Predicted label before training\n\npred = torch.argmax(probas[0:1], dim=1).item()\nprint(f\"Prediction:  {pred}, True label: {saved_target[0].item()}\")\n\nPrediction:  4, True label: 2\n\n\n\nmodel.fc1.weight.shape, model.fc1.bias.shape\n\n(torch.Size([64, 784]), torch.Size([64]))\n\n\n\n# Number of parameters in fc1\nprint(\"fc1\", model.fc1.weight.numel() + model.fc1.bias.numel())\n\n# Number of parameters in fc2\nprint(\"fc2\", model.fc2.weight.numel() + model.fc2.bias.numel())\n\n# Number of parameters in fc3\nprint(\"fc3\", model.fc3.weight.numel() + model.fc3.bias.numel())\n\nfc1 50240\nfc2 2080\nfc3 330\n\n\n\n# Get total number of parameters\nprint(\"Total number of parameters:\", sum(p.numel() for p in model.parameters()))\n\nTotal number of parameters: 52650\n\n\n\nlen(train_loader)\n\n79\n\n\n\n# Forward pass through the model and writing to tensorboard\n\nlr = 0.001\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    for batch_idx, (data_test, target) in enumerate(train_loader):\n        # Reshape data to input to the network\n        data_test = data_test.view(data_test.shape[0], -1)\n        # Forward pass\n        output = model(data_test)\n        loss = criterion(output, target)\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        iteration_num = epoch * len(train_loader) + batch_idx\n        \n        # Write to tensorboard\n        writer.add_scalar('Loss/train', loss.item(), iteration_num)\n        writer.add_scalar('Accuracy/train', (output.argmax(dim=1) == target).float().mean(), iteration_num)\n\n        # Find test loss and accuracy\n        runing_loss = 0.0\n        running_acc = 0.0\n        with torch.no_grad():\n            for data_test, target_test in val_loader:\n                data_test = data_test.view(data_test.shape[0], -1)\n                output_test = model(data_test)\n                loss_test = criterion(output_test, target_test)\n                runing_loss += loss_test.item()\n                running_acc += (output_test.argmax(dim=1) == target_test).float().mean()\n        writer.add_scalar('Loss/validation', runing_loss / len(val_loader), iteration_num)\n        writer.add_scalar('Accuracy/validation', running_acc / len(val_loader), iteration_num)\n\n        if batch_idx % 100 == 0:\n            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\nEpoch [1/1], Step [0/79], Loss: 2.3027\n\n\n\n# Prediction on the saved image\n\nwith torch.no_grad():\n    out = model(saved_img[0:1])\n    probas = F.softmax(out, dim=1)\n    pred = torch.argmax(probas, dim=1).item()\n    print(pred, saved_target[0].item())\n\n1 1\n\n\n\n# Create a HParam dictionary for batch size and learning rate for tensorboard\n\nb_sizes = [32, 64, 512]\nlrs = [0.001, 0.01, 0.1]\n\n\nnum_epochs  = 1\nfor epoch in range(num_epochs):\n    for b_size in b_sizes:\n        train_loader = DataLoader(train_data, batch_size=b_size, shuffle=True)\n        model = MNISTClassifier()\n        for lr in lrs:\n            print(f\"Batch size: {b_size}, Learning rate: {lr}\")\n            optimizer = optim.Adam(model.parameters(), lr=lr)\n            criterion = nn.CrossEntropyLoss()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                # Reshape data to input to the network\n                data = data.view(data.shape[0], -1)\n                # Forward pass\n                output = model(data)\n                loss = criterion(output, target)\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n\n                # Write loss as scalar to Tensorboard and hyperparameters to HParams\n                writer.add_scalar('training loss',\n                                loss.item(),\n                                epoch * len(train_loader) + batch_idx)\n                writer.add_hparams({'lr': lr, 'bsize': b_size},\n                                { 'hparam/loss': loss.item()})\n                \n\n    \n\nBatch size: 32, Learning rate: 0.001\nBatch size: 32, Learning rate: 0.01\nBatch size: 32, Learning rate: 0.1\nBatch size: 64, Learning rate: 0.001\nBatch size: 64, Learning rate: 0.01\nBatch size: 64, Learning rate: 0.1\nBatch size: 512, Learning rate: 0.001\nBatch size: 512, Learning rate: 0.01\nBatch size: 512, Learning rate: 0.1"
  },
  {
    "objectID": "notebooks/posts/cnn.html",
    "href": "notebooks/posts/cnn.html",
    "title": "CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# MNIST dataset\n\nfrom torchvision import datasets, transforms\nimport torchvision\n\n# Split MNIST into train, validation, and test sets\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n\n# Split train_data into train and validation sets\nval_data = torch.utils.data.Subset(train_data, range(50000, 51000))\n\n# Reduce the size of the training set to 5,000\ntrain_data = torch.utils.data.Subset(train_data, range(0, 5000))\n\n\n# Create data loaders\nbatch_size = 64\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n\nimg, target = next(iter(train_loader))\nprint(img.shape)\nprint(target.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\n\nplt.imshow(img[0].numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\nimg[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Create a simple LeNet like CNN\n\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5) \n        # 6 input image channel, 16 output channels, 5x5 square convolution\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x) # 28x28x1 -> 24x24x6\n        x = F.max_pool2d(F.relu(x), 2) # 24x24x6 -> 12x12x6\n        x = self.conv2(x) # 12x12x6 -> 8x8x16\n        x = F.max_pool2d(F.relu(x), 2) # 8x8x16 -> 4x4x16\n        x = x.view(-1, self.num_flat_features(x)) # 4x4x16 -> 256\n        x = self.fc1(x) # 256 -> 120\n        x = F.relu(x)\n        x = self.fc2(x) # 120 -> 84\n        x = F.relu(x)\n        x = self.fc3(x) # 84 -> 10\n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\n# Create a model\n\nmodel = LeNet5()\nprint(model)\n\nLeNet5(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n# Train the model\n\n# Define the loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nn_epochs = 10\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(n_epochs):\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    # Train the model\n    model.train()\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        \n    # Evaluate the model\n    model.eval()\n    for data, target in val_loader:\n        output = model(data)\n        loss = criterion(output, target)\n        val_loss += loss.item()*data.size(0)\n        \n    # Calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    val_loss = val_loss/len(val_loader.sampler)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    # Print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch+1, \n        train_loss,\n        val_loss\n        ))\n\nEpoch: 1    Training Loss: 1.437300     Validation Loss: 0.653900\nEpoch: 2    Training Loss: 0.424091     Validation Loss: 0.367598\nEpoch: 3    Training Loss: 0.303504     Validation Loss: 0.308797\nEpoch: 4    Training Loss: 0.219186     Validation Loss: 0.257062\nEpoch: 5    Training Loss: 0.195089     Validation Loss: 0.214157\nEpoch: 6    Training Loss: 0.153489     Validation Loss: 0.190220\nEpoch: 7    Training Loss: 0.130065     Validation Loss: 0.189110\nEpoch: 8    Training Loss: 0.114033     Validation Loss: 0.173153\nEpoch: 9    Training Loss: 0.103402     Validation Loss: 0.167645\nEpoch: 10   Training Loss: 0.089715     Validation Loss: 0.156438\n\n\n\n# Plot the training and validation loss\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(val_losses, label='Validation loss')\n\n\n\n\n\n# Test the model\n\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for data, target in test_loader:\n        output = model(data)\n        _, predicted = torch.max(output.data, 1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n    print('Test Accuracy: {}%'.format(100 * correct / total))\n\nTest Accuracy: 96.1%\n\n\n\n# Now, let us take an image and walk it through the model\n\ntest_img = train_data[1][0].unsqueeze(0)\n\n\nplt.imshow(test_img.numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\n# Get weights and biases from the first convolutional layer\n\nweights = model.conv1.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False, annot=True)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n# Get output from model's first conv1 layer\n\nconv1 = F.relu(model.conv1(test_img))\n\n# For plotting bring all the images to the same scale\nc1 = conv1 - conv1.min()\nc1 = c1 / conv1.max()\n\nprint(c1.shape)\nprint(\"1 image, 6 channels, 24x24 pixels\")\n\ntorch.Size([1, 6, 24, 24])\n1 image, 6 channels, 24x24 pixels\n\n\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\n\n\n\n\n# Get output from model after max pooling\n\npool1 = F.max_pool2d(conv1, 2)\n\n# For plotting bring all the images to the same scale\np1 = pool1 - pool1.min()\np1 = p1 / pool1.max()\n\nprint(p1.shape)\nprint(\"1 image, 6 channels, 12x12 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 6, 12, 12])\n1 image, 6 channels, 12x12 pixels\n\n\n\n\n\n\n# Visualize the filters in the second convolutional layer\n\nweights = model.conv2.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n# Get output from model's second conv2 layer\n\nconv2 = F.relu(model.conv2(pool1))\n\n# For plotting bring all the images to the same scale\nc2 = conv2 - conv2.min()\nc2 = c2 / conv2.max()\n\nprint(c2.shape)\nprint(\"1 image, 16 channels, 8x8 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 8, 8])\n1 image, 16 channels, 8x8 pixels\n\n\n\n\n\n\n# Get output from model after max pooling\n\npool2 = F.max_pool2d(conv2, 2)\n\n# For plotting bring all the images to the same scale\np2 = pool2 - pool2.min()\np2 = p2 / pool2.max()\n\nprint(p2.shape)\nprint(\"1 image, 16 channels, 4x4 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 4, 4])\n1 image, 16 channels, 4x4 pixels\n\n\n\n\n\n\n# Flatten the output of the second convolutional layer\n\nflat = pool2.view(pool2.size(0), -1)\nprint(flat.shape)\n\ntorch.Size([1, 256])\n\n\n\n# Repeat the above process as a function to visualize the convolution outputs for any image for any layer\ndef scale_img(img):\n    \"\"\"\n    Scale the image to the same scale\n    \"\"\"\n    img = img - img.min()\n    img = img / img.max()\n    return img\n\ndef visualize_conv_output(model, img):\n    \"\"\"\n    Visualize the output of a convolutional layer\n    \"\"\"\n    # Get output from model's first conv1 layer\n    conv1 = F.relu(model.conv1(img))\n\n    # For plotting bring all the images to the same scale\n    c1 = scale_img(conv1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 1', fontsize=16)\n    \n    # Get output from model after max pooling\n    pool1 = F.max_pool2d(conv1, 2)\n\n    # For plotting bring all the images to the same scale\n    p1 = scale_img(pool1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 1', fontsize=16)\n\n    # Get output from model's second conv2 layer\n    conv2 = F.relu(model.conv2(pool1))\n\n    # For plotting bring all the images to the same scale\n    c2 = scale_img(conv2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    \n    for i in range(16):\n        sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 2', fontsize=16)\n\n    # Get output from model after max pooling\n    pool2 = F.max_pool2d(conv2, 2)\n\n    # For plotting bring all the images to the same scale\n    p2 = scale_img(pool2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    for i in range(16):\n        sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 2', fontsize=16)\n\n\nvisualize_conv_output(model, train_data[2][0].unsqueeze(0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_conv_output(model, train_data[4][0].unsqueeze(0))"
  },
  {
    "objectID": "notebooks/posts/pivot-cross.html",
    "href": "notebooks/posts/pivot-cross.html",
    "title": "Pandas tips",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\nDataset creation\n\n# Create a DataFrame for whether or not a person plays tennis. \n# It has discrete features and 14 rows.\n\ndf = pd.DataFrame({\n    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n})\n\n\n\nCounting values of outcome variable (useful for calculating entropy)\n\nser = df['PlayTennis'].value_counts()\nser\n\nYes    9\nNo     5\nName: PlayTennis, dtype: int64\n\n\n\n\nUsing cross tab to quickly capture the relationship between two variables\n\ndf_out = pd.crosstab(df['Outlook'], df['PlayTennis'])\ndf_out\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\ndf_out.index, df_out.columns\n\n(Index(['Overcast', 'Rain', 'Sunny'], dtype='object', name='Outlook'),\n Index(['No', 'Yes'], dtype='object', name='PlayTennis'))\n\n\n\ne = df_out.sum(axis=1)\ne\n\nOutlook\nOvercast    4\nRain        5\nSunny       5\ndtype: int64\n\n\n\n# Find the fraction of each row that is a 'Yes' and 'No' for PlayTennis\ndf_out['Yes'] = df_out['Yes'] / e\ndf_out['No'] = df_out['No'] / e\ndf_out\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0.0\n      1.0\n    \n    \n      Rain\n      0.4\n      0.6\n    \n    \n      Sunny\n      0.6\n      0.4\n    \n  \n\n\n\n\n\n\nMore efficient cross tabulation (using normalize)\n\npd.crosstab(df['Outlook'], df['PlayTennis'], normalize='index')\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0.0\n      1.0\n    \n    \n      Rain\n      0.4\n      0.6\n    \n    \n      Sunny\n      0.6\n      0.4\n    \n  \n\n\n\n\n\n\nUsing pd.groupby()\n\ndf.groupby([\"Outlook\"]).groups\n\n{'Overcast': [2, 6, 11, 12], 'Rain': [3, 4, 5, 9, 13], 'Sunny': [0, 1, 7, 8, 10]}\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")\n\n\n\n\n\n  \n    \n      \n      Outlook\n      Temperature\n      Humidity\n      Wind\n      PlayTennis\n    \n  \n  \n    \n      0\n      Sunny\n      Hot\n      High\n      Weak\n      No\n    \n    \n      1\n      Sunny\n      Hot\n      High\n      Strong\n      No\n    \n    \n      7\n      Sunny\n      Mild\n      High\n      Weak\n      No\n    \n    \n      8\n      Sunny\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      10\n      Sunny\n      Mild\n      Normal\n      Strong\n      Yes\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")[\"PlayTennis\"]\n\n0      No\n1      No\n7      No\n8     Yes\n10    Yes\nName: PlayTennis, dtype: object\n\n\n\ndf.groupby([\"Outlook\"]).get_group(\"Sunny\")[\"PlayTennis\"].value_counts()\n\nNo     3\nYes    2\nName: PlayTennis, dtype: int64\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).groups\n\n{('Overcast', 'Yes'): [2, 6, 11, 12], ('Rain', 'No'): [5, 13], ('Rain', 'Yes'): [3, 4, 9], ('Sunny', 'No'): [0, 1, 7], ('Sunny', 'Yes'): [8, 10]}\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).get_group((\"Sunny\", \"Yes\"))\n\n\n\n\n\n  \n    \n      \n      Outlook\n      Temperature\n      Humidity\n      Wind\n      PlayTennis\n    \n  \n  \n    \n      8\n      Sunny\n      Cool\n      Normal\n      Weak\n      Yes\n    \n    \n      10\n      Sunny\n      Mild\n      Normal\n      Strong\n      Yes\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size()\n\nOutlook   PlayTennis\nOvercast  Yes           4\nRain      No            2\n          Yes           3\nSunny     No            3\n          Yes           2\ndtype: int64\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().index\n\nMultiIndex([('Overcast', 'Yes'),\n            (    'Rain',  'No'),\n            (    'Rain', 'Yes'),\n            (   'Sunny',  'No'),\n            (   'Sunny', 'Yes')],\n           names=['Outlook', 'PlayTennis'])\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().unstack()\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      NaN\n      4.0\n    \n    \n      Rain\n      2.0\n      3.0\n    \n    \n      Sunny\n      3.0\n      2.0\n    \n  \n\n\n\n\n\ndf.groupby([\"Outlook\", \"PlayTennis\"]).size().unstack(fill_value=0)\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\n\nUsing pd.pivot_table\n\npivot_table = df.pivot_table(index='Outlook', columns='PlayTennis', aggfunc='size', fill_value=0)\npivot_table\n\n\n\n\n\n  \n    \n      PlayTennis\n      No\n      Yes\n    \n    \n      Outlook\n      \n      \n    \n  \n  \n    \n      Overcast\n      0\n      4\n    \n    \n      Rain\n      2\n      3\n    \n    \n      Sunny\n      3\n      2\n    \n  \n\n\n\n\n\ndf_out.plot(kind='bar', stacked=True)\n\n<AxesSubplot:xlabel='Outlook'>\n\n\n\n\n\n\ndf_out.plot(kind='bar', stacked=False)\n\n<AxesSubplot:xlabel='Outlook'>"
  },
  {
    "objectID": "notebooks/posts/names.html",
    "href": "notebooks/posts/names.html",
    "title": "Generating names using MLPs",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Get some names from https://github.com/MASTREX/List-of-Indian-Names\n\n\n!wget https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv -O names-long.csv\n\n--2023-03-31 17:00:55--  https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 85538 (84K) [text/plain]\nSaving to: ‘names-long.csv’\n\nnames-long.csv      100%[===================>]  83.53K  --.-KB/s    in 0.07s   \n\n2023-03-31 17:00:55 (1.12 MB/s) - ‘names-long.csv’ saved [85538/85538]\n\n\n\n\n!head names-long.csv\n\n,Name\n0,aabid\n1,aabida\n2,aachal\n3,aadesh\n4,aadil\n5,aadish\n6,aaditya\n7,aaenab\n8,aafreen\n\n\n\nwords = pd.read_csv('names-long.csv')[\"Name\"]\nwords = words.str.lower()\nwords = words.str.strip()\nwords = words.str.replace(\" \", \"\")\n\nwords = words[words.str.len() > 2]\nwords = words[words.str.len() < 10]\n\n# Randomly shuffle the words\nwords = words.sample(frac=1).reset_index(drop=True)\nwords = words.tolist()\n\n# Remove words having non alphabets\nwords = [word for word in words if word.isalpha()]\nwords[:10]\n\n['sehran',\n 'iema',\n 'bajinder',\n 'manoj',\n 'nayaka',\n 'ajmal',\n 'navleen',\n 'akhtari',\n 'samsung',\n 'imtyaz']\n\n\n\nlen(words)\n\n6184\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nprint(itos)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n\n\n\nblock_size = 4 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words[:]:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    #print(''.join(itos[i] for i in context), '--->', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([44325, 4]), torch.int64, torch.Size([44325]), torch.int64)\n\n\n\n# Embedding layer for the context\n\nemb_dim = 2\nemb = torch.nn.Embedding(len(stoi), emb_dim)\n\n\nemb.weight\n\nParameter containing:\ntensor([[-0.7321, -0.0133],\n        [ 0.9890, -0.3715],\n        [-1.2565, -0.3346],\n        [ 0.3989,  1.4209],\n        [-0.5718, -0.1615],\n        [ 0.4853,  2.2499],\n        [ 0.1107,  2.7214],\n        [-1.7009,  1.1288],\n        [-0.6636, -1.1188],\n        [-1.6768,  1.0876],\n        [ 0.1645,  0.0063],\n        [-1.0367, -0.9603],\n        [-3.0939,  1.9831],\n        [-0.8332,  0.8572],\n        [-1.4305, -0.6878],\n        [-0.5197, -0.9626],\n        [-2.3395,  0.6205],\n        [-0.7045,  0.2387],\n        [ 2.8074, -0.8545],\n        [-0.2396, -0.7623],\n        [-1.1135, -0.1288],\n        [-0.5464, -0.0345],\n        [-1.2686,  1.0719],\n        [-1.3973, -0.5510],\n        [ 0.9127,  1.2566],\n        [ 0.6278,  1.1915],\n        [ 0.0465,  1.2633]], requires_grad=True)\n\n\n\n# Function to visualize the embedding in 2d space\n\ndef plot_emb(emb, itos, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    for i in range(len(itos)):\n        x, y = emb.weight[i].detach().cpu().numpy()\n        ax.scatter(x, y, color='k')\n        ax.text(x + 0.05, y + 0.05, itos[i])\n    return ax\n\nplot_emb(emb, itos)\n\n<AxesSubplot:>\n\n\n\n\n\n\nclass NextChar(nn.Module):\n  def __init__(self, block_size, vocab_size, emb_dim, hidden_size):\n    super().__init__()\n    self.emb = nn.Embedding(vocab_size, emb_dim)\n    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    x = self.emb(x)\n    x = x.view(x.shape[0], -1)\n    x = F.relu(self.lin1(x))\n    x = self.lin2(x)\n    return x\n    \n\n\n# Generate names from untrained model\n\n\nmodel = NextChar(block_size, len(stoi), emb_dim, 50).to(device)\nmodel = torch.compile(model)\n\ng = torch.Generator()\ng.manual_seed(4000002)\ndef generate_name(model, itos, stoi, block_size, max_len=10):\n    context = [0] * block_size\n    name = ''\n    for i in range(max_len):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        ch = itos[ix]\n        if ch == '.':\n            break\n        name += ch\n        context = context[1:] + [ix]\n    return name\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n\n\nihjzyewjrp\npuijjpnwfk\ngcvvflic\neeokqgmlse\nyiaghsikiv\nsmmggfnsot\nptdmdlmwfi\nyitpyixshw\ngjqapafodl\ndfuhgoowtb\n\n\n\nfor param_name, param in model.named_parameters():\n    print(param_name, param.shape)\n\nemb.weight torch.Size([27, 2])\nlin1.weight torch.Size([50, 8])\nlin1.bias torch.Size([50])\nlin2.weight torch.Size([27, 50])\nlin2.bias torch.Size([27])\n\n\n\n# Train the model\n\nloss_fn = nn.CrossEntropyLoss()\nopt = torch.optim.AdamW(model.parameters(), lr=0.01)\nimport time\n# Mini-batch training\nbatch_size = 4096*32\nprint_every = 400\nelapsed_time = []\nfor epoch in range(10000):\n    start_time = time.time()\n    for i in range(0, X.shape[0], batch_size):\n        x = X[i:i+batch_size]\n        y = Y[i:i+batch_size]\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    end_time = time.time()\n    elapsed_time.append(end_time - start_time)\n    if epoch % print_every == 0:\n        print(epoch, loss.item())\n        print(\"Time taken for 1 epochs: {} seconds\".format(sum(elapsed_time)/print_every))\n\n0 2.0890238285064697\nTime taken for 1 epochs: 6.577968597412109e-06 seconds\n400 2.0890519618988037\nTime taken for 1 epochs: 0.0018871128559112549 seconds\n800 2.088754177093506\nTime taken for 1 epochs: 0.0037400954961776733 seconds\n1200 2.0887293815612793\nTime taken for 1 epochs: 0.005593389868736267 seconds\n1600 2.0884861946105957\nTime taken for 1 epochs: 0.007446771860122681 seconds\n2000 2.0891528129577637\nTime taken for 1 epochs: 0.009300289154052734 seconds\n2400 2.0879929065704346\nTime taken for 1 epochs: 0.011153245568275452 seconds\n2800 2.088449239730835\nTime taken for 1 epochs: 0.013006348609924317 seconds\n3200 2.0879693031311035\nTime taken for 1 epochs: 0.014860434532165527 seconds\n3600 2.0881006717681885\nTime taken for 1 epochs: 0.016712880134582518 seconds\n4000 2.087575912475586\nTime taken for 1 epochs: 0.01856597661972046 seconds\n4400 2.087235689163208\nTime taken for 1 epochs: 0.020418978929519653 seconds\n4800 2.087622880935669\nTime taken for 1 epochs: 0.02227234423160553 seconds\n5200 2.087576150894165\nTime taken for 1 epochs: 0.02412549138069153 seconds\n5600 2.0879015922546387\nTime taken for 1 epochs: 0.025978831052780153 seconds\n6000 2.088137626647949\nTime taken for 1 epochs: 0.027831865549087523 seconds\n6400 2.0875606536865234\nTime taken for 1 epochs: 0.029685078263282774 seconds\n6800 2.0870373249053955\nTime taken for 1 epochs: 0.031538299322128295 seconds\n7200 2.0867090225219727\nTime taken for 1 epochs: 0.033391504883766174 seconds\n7600 2.085026979446411\nTime taken for 1 epochs: 0.035245080590248105 seconds\n8000 2.0854732990264893\nTime taken for 1 epochs: 0.03710031807422638 seconds\n8400 2.0843679904937744\nTime taken for 1 epochs: 0.03895375490188599 seconds\n8800 2.084249258041382\nTime taken for 1 epochs: 0.04080682039260864 seconds\n9200 2.0851705074310303\nTime taken for 1 epochs: 0.042660011649131774 seconds\n9600 2.0844883918762207\nTime taken for 1 epochs: 0.044512977004051206 seconds\n\n\n\n# Visualize the embedding\n\nplot_emb(model.emb, itos)\n\n<AxesSubplot:>\n\n\n\n\n\n\n# Generate names from trained model\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\nswina\nmiter\njakul\nposhaban\nnahmishsir\njaillanam\nfibwahir\ngurs\nsakinu\nramta\n\n\nTuning knobs\n\nEmbedding size\nMLP\nContext length"
  },
  {
    "objectID": "notebooks/posts/maths-jax.html",
    "href": "notebooks/posts/maths-jax.html",
    "title": "Maths and JAX",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\n\n\ndef func(x, y, z):\n    return x**2 + jnp.sin(y) + z\n\n\nfunc(1, 2, 3)\n\nDeviceArray(4.9092975, dtype=float32, weak_type=True)\n\n\n\nfrom sympy import *\ninit_printing()\n\nx, y, z = symbols('x y z')\nf = x**2 + sin(y) + z\nf\n\n\\(\\displaystyle x^{2} + z + \\sin{\\left(y \\right)}\\)\n\n\n\ndiff(f, x)\n\n\\(\\displaystyle 2 x\\)\n\n\n\n# Find the derivative of f with respect to x, y, and z using sympy\ndel_x, del_y, del_z = diff(f, x), diff(f, y), diff(f, z)\ndel_x, del_y, del_z\n\n\\(\\displaystyle \\left( 2 x, \\  \\cos{\\left(y \\right)}, \\  1\\right)\\)\n\n\n\ngrad_f = lambdify((x, y, z), [del_x, del_y, del_z])\ngrad_f(1, 2, 3)\n\n\\(\\displaystyle \\left[ 2, \\  -0.416146836547142, \\  1\\right]\\)\n\n\n\ngrad_f_jax = jax.grad(func, argnums=(0, 1, 2))\ngrad_f_jax(1., 2., 3.)\n\n(DeviceArray(2., dtype=float32, weak_type=True),\n DeviceArray(-0.41614684, dtype=float32, weak_type=True),\n DeviceArray(1., dtype=float32, weak_type=True))\n\n\n\nn = 20\nA = jax.random.normal(shape=(1, n), key=jax.random.PRNGKey(0), dtype=jnp.float32)\ntheta = jax.random.normal(shape=(n, 1), key=jax.random.PRNGKey(0), dtype=jnp.float32)\nb = A @ theta \n\nb\n\nDeviceArray([[28.684494]], dtype=float32)\n\n\n\nb.flatten(), b.item()\n\n(DeviceArray([28.684494], dtype=float32), 28.684494018554688)\n\n\n\ndef a_theta(A, theta):\n    return A @ theta\n\n\na_theta(A, theta)\n\nDeviceArray([[28.684494]], dtype=float32)\n\n\n\ngrad_a_theta = jax.grad(a_theta, argnums=1)\n\n\njax.jacobian(a_theta, argnums=1)(A, theta)[0, 0, :].shape\n\n\\(\\displaystyle \\left( 20, \\  1\\right)\\)\n\n\n\nA.shape\n\n\\(\\displaystyle \\left( 1, \\  20\\right)\\)\n\n\n\n# Sympy version\n\n\nA = MatrixSymbol('A', 1, n)\ntheta = MatrixSymbol('theta', n, 1)\nA, theta\n\n\\(\\displaystyle \\left( A, \\  \\theta\\right)\\)\n\n\n\ndiff(A*theta, theta)\n\n\\(\\displaystyle A^{T}\\)"
  },
  {
    "objectID": "notebooks/posts/tips.html",
    "href": "notebooks/posts/tips.html",
    "title": "Misc tips",
    "section": "",
    "text": "Miscelleneous tips\n\nTab Complete\na.b?\nBlack format\nrich\n\ntable\ninspect\n\ndir()\ntiming\ntiming with sort?!\ntimeit object\nplotting timing\narray .tolist()\nDataclasses\nAnnotation\nPlotting with pandas (sin, log, question …)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nn = 100\nk = 20\nm = 50\n\nnp.random.seed(0)\na = np.random.randn(n, k)\nb = np.random.randn(k, m)\na.shape, b.shape\n\n((100, 20), (20, 50))\n\n\n\na_list_of_list = a.tolist()\na_list_of_list[0]\n\nlist\n\n\n\n\"list\" in dir(a)[:]\n\nFalse\n\n\n\nimport rich\nrich.inspect(a.tolist, methods=True, all=True)\n\n╭──────────────────────── <built-in method tolist of numpy.ndarray object at 0x164cbb8d0> ────────────────────────╮\n│ def ndarray.tolist(...)                                                                                         │\n│                                                                                                                 │\n│ a.tolist()                                                                                                      │\n│                                                                                                                 │\n│            __doc__ = \"a.tolist()\\n\\n    Return the array as an ``a.ndim``-levels deep nested list of Python     │\n│                      scalars.\\n\\n    Return a copy of the array data as a (nested) Python list.\\n    Data items │\n│                      are converted to the nearest compatible builtin Python type, via\\n    the                  │\n│                      `~numpy.ndarray.item` function.\\n\\n    If ``a.ndim`` is 0, then since the depth of the     │\n│                      nested list is 0, it will\\n    not be a list at all, but a simple Python scalar.\\n\\n       │\n│                      Parameters\\n    ----------\\n    none\\n\\n    Returns\\n    -------\\n    y : object, or list  │\n│                      of object, or list of list of object, or ...\\n        The possibly nested list of array    │\n│                      elements.\\n\\n    Notes\\n    -----\\n    The array may be recreated via ``a =                │\n│                      np.array(a.tolist())``, although this\\n    may sometimes lose precision.\\n\\n    Examples\\n │\n│                      --------\\n    For a 1D array, ``a.tolist()`` is almost the same as ``list(a)``,\\n          │\n│                      except that ``tolist`` changes numpy scalars to Python scalars:\\n\\n    >>> a =             │\n│                      np.uint32([1, 2])\\n    >>> a_list = list(a)\\n    >>> a_list\\n    [1, 2]\\n    >>>           │\n│                      type(a_list[0])\\n    <class 'numpy.uint32'>\\n    >>> a_tolist = a.tolist()\\n    >>>        │\n│                      a_tolist\\n    [1, 2]\\n    >>> type(a_tolist[0])\\n    <class 'int'>\\n\\n    Additionally,    │\n│                      for a 2D array, ``tolist`` applies recursively:\\n\\n    >>> a = np.array([[1, 2], [3,       │\n│                      4]])\\n    >>> list(a)\\n    [array([1, 2]), array([3, 4])]\\n    >>> a.tolist()\\n    [[1,    │\n│                      2], [3, 4]]\\n\\n    The base case for this recursion is a 0D array:\\n\\n    >>> a =          │\n│                      np.array(1)\\n    >>> list(a)\\n    Traceback (most recent call last):\\n      ...\\n          │\n│                      TypeError: iteration over a 0-d array\\n    >>> a.tolist()\\n    1\"                          │\n│         __module__ = None                                                                                       │\n│           __name__ = 'tolist'                                                                                   │\n│       __qualname__ = 'ndarray.tolist'                                                                           │\n│           __self__ = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,                            │\n│                              -2.05158264e-01,  3.13067702e-01, -8.54095739e-01],                                │\n│                             [-2.55298982e+00,  6.53618595e-01,  8.64436199e-01, ...,                            │\n│                               1.20237985e+00, -3.87326817e-01, -3.02302751e-01],                                │\n│                             [-1.04855297e+00, -1.42001794e+00, -1.70627019e+00, ...,                            │\n│                               3.02471898e-01, -6.34322094e-01, -3.62741166e-01],                                │\n│                             ...,                                                                                │\n│                             [ 8.73311836e-01,  1.19973618e+00,  4.56153036e-01, ...,                            │\n│                              -1.24021634e+00,  9.00054243e-01,  1.80224223e+00],                                │\n│                             [-2.08285103e-01,  1.57437124e+00,  1.98989494e-01, ...,                            │\n│                               4.32837621e-01, -8.08717532e-01, -1.10412399e+00],                                │\n│                             [-7.89102180e-01,  1.24845579e-03, -1.59939788e-01, ...,                            │\n│                               1.58433847e-01, -1.14190142e+00, -1.31097037e+00]])                               │\n│ __text_signature__ = None                                                                                       │\n│           __call__ = def __call__(*args, **kwargs): Call self as a function.                                    │\n│          __class__ = class __class__():                                                                         │\n│        __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                                   │\n│            __dir__ = def __dir__(): Default dir() implementation.                                               │\n│             __eq__ = def __eq__(value, /): Return self==value.                                                  │\n│         __format__ = def __format__(format_spec, /): Default object formatter.                                  │\n│             __ge__ = def __ge__(value, /): Return self>=value.                                                  │\n│   __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                                 │\n│             __gt__ = def __gt__(value, /): Return self>value.                                                   │\n│           __hash__ = def __hash__(): Return hash(self).                                                         │\n│           __init__ = def __init__(*args, **kwargs): Initialize self.  See help(type(self)) for accurate         │\n│                      signature.                                                                                 │\n│  __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.               │\n│             __le__ = def __le__(value, /): Return self<=value.                                                  │\n│             __lt__ = def __lt__(value, /): Return self<value.                                                   │\n│             __ne__ = def __ne__(value, /): Return self!=value.                                                  │\n│            __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for accurate │\n│                      signature.                                                                                 │\n│         __reduce__ = def __reduce__(...) Helper for pickle.                                                     │\n│      __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                         │\n│           __repr__ = def __repr__(): Return repr(self).                                                         │\n│        __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).                     │\n│         __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                                      │\n│            __str__ = def __str__(): Return str(self).                                                           │\n│   __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize issubclass().    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nlambda x: np.power(x, 3)\n\n\ndef f(x):\n    return np.power(x, 3)\n\n\nd = {\n    r\"$\\sin(x)$\": np.sin,\n    r\"$\\log(x)$\": np.log,\n    r\"$\\frac{sin(x)}{x}$\": lambda x: np.sin(x) / x,\n    r\"$x^3$\": lambda x: np.power(x, 3),\n}\n\n\nd\n\n{'$\\\\sin(x)$': <ufunc 'sin'>,\n '$\\\\log(x)$': <ufunc 'log'>,\n '$\\\\frac{sin(x)}{x}$': <function __main__.<lambda>(x)>,\n '$x^3$': <function __main__.<lambda>(x)>}\n\n\n\nx = np.arange(0.005, 10.0, 0.005)\n\n\ne = {k: v(x) for k, v in d.items()}\n\n\ne\n\n{'$\\\\sin(x)$': array([ 0.00499998,  0.00999983,  0.01499944, ..., -0.53137431,\n        -0.53560333, -0.53981897]),\n '$\\\\log(x)$': array([-5.29831737, -4.60517019, -4.19970508, ...,  2.30108397,\n         2.30158459,  2.30208497]),\n '$\\\\frac{sin(x)}{x}$': array([ 0.99999583,  0.99998333,  0.9999625 , ..., -0.05321726,\n        -0.05361395, -0.0540089 ]),\n '$x^3$': array([1.25000000e-07, 1.00000000e-06, 3.37500000e-06, ...,\n        9.95506747e+02, 9.97002999e+02, 9.98500750e+02])}\n\n\n\ndf = pd.DataFrame(e, index=x)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      $\\sin(x)$\n      $\\log(x)$\n      $\\frac{sin(x)}{x}$\n      $x^3$\n    \n  \n  \n    \n      0.005\n      0.005000\n      -5.298317\n      0.999996\n      1.250000e-07\n    \n    \n      0.010\n      0.010000\n      -4.605170\n      0.999983\n      1.000000e-06\n    \n    \n      0.015\n      0.014999\n      -4.199705\n      0.999963\n      3.375000e-06\n    \n    \n      0.020\n      0.019999\n      -3.912023\n      0.999933\n      8.000000e-06\n    \n    \n      0.025\n      0.024997\n      -3.688879\n      0.999896\n      1.562500e-05\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9.975\n      -0.522877\n      2.300082\n      -0.052419\n      9.925187e+02\n    \n    \n      9.980\n      -0.527132\n      2.300583\n      -0.052819\n      9.940120e+02\n    \n    \n      9.985\n      -0.531374\n      2.301084\n      -0.053217\n      9.955067e+02\n    \n    \n      9.990\n      -0.535603\n      2.301585\n      -0.053614\n      9.970030e+02\n    \n    \n      9.995\n      -0.539819\n      2.302085\n      -0.054009\n      9.985007e+02\n    \n  \n\n1999 rows × 4 columns\n\n\n\n\ndf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.plot(subplots=True)\n\narray([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>],\n      dtype=object)\n\n\n\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      $\\sin$\n      log\n      $\\frac{sin(x)}{x}$\n      x^3\n    \n  \n  \n    \n      0.005\n      0.005000\n      -5.298317\n      0.999996\n      1.250000e-07\n    \n    \n      0.010\n      0.010000\n      -4.605170\n      0.999983\n      1.000000e-06\n    \n    \n      0.015\n      0.014999\n      -4.199705\n      0.999963\n      3.375000e-06\n    \n    \n      0.020\n      0.019999\n      -3.912023\n      0.999933\n      8.000000e-06\n    \n    \n      0.025\n      0.024997\n      -3.688879\n      0.999896\n      1.562500e-05\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9.975\n      -0.522877\n      2.300082\n      -0.052419\n      9.925187e+02\n    \n    \n      9.980\n      -0.527132\n      2.300583\n      -0.052819\n      9.940120e+02\n    \n    \n      9.985\n      -0.531374\n      2.301084\n      -0.053217\n      9.955067e+02\n    \n    \n      9.990\n      -0.535603\n      2.301585\n      -0.053614\n      9.970030e+02\n    \n    \n      9.995\n      -0.539819\n      2.302085\n      -0.054009\n      9.985007e+02\n    \n  \n\n1999 rows × 4 columns\n\n\n\n\n((df - df.min(axis=0)) / (df.max(axis=0) - df.min(axis=0))).plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.randn(1000, 5),\n                  columns=\"a,b,c,d,e\".split(\",\"))\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n      e\n    \n  \n  \n    \n      0\n      1.593274\n      0.568722\n      -0.114487\n      0.251630\n      -1.210856\n    \n    \n      1\n      -0.393734\n      0.085253\n      0.099422\n      -1.530616\n      0.327623\n    \n    \n      2\n      0.279196\n      -0.377051\n      0.004175\n      -1.483492\n      -1.479796\n    \n    \n      3\n      0.134687\n      -0.667723\n      -0.011556\n      0.839491\n      -0.173930\n    \n    \n      4\n      -2.810668\n      -0.150654\n      -0.481044\n      -0.234694\n      0.899731\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.837658\n      1.315288\n      -0.364523\n      1.993571\n      1.584878\n    \n    \n      996\n      -2.104663\n      -2.553118\n      -1.242666\n      0.201987\n      -0.305332\n    \n    \n      997\n      -1.195587\n      -1.577903\n      0.849912\n      0.327590\n      -0.001670\n    \n    \n      998\n      -0.035563\n      -0.489252\n      1.930498\n      -0.262645\n      0.825932\n    \n    \n      999\n      -0.643267\n      -0.828981\n      -0.202735\n      -0.257866\n      0.070815\n    \n  \n\n1000 rows × 5 columns\n\n\n\n\ndf = df.assign(f=df.sum(axis=1))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n      e\n      f\n    \n  \n  \n    \n      0\n      1.593274\n      0.568722\n      -0.114487\n      0.251630\n      -1.210856\n      1.088284\n    \n    \n      1\n      -0.393734\n      0.085253\n      0.099422\n      -1.530616\n      0.327623\n      -1.412052\n    \n    \n      2\n      0.279196\n      -0.377051\n      0.004175\n      -1.483492\n      -1.479796\n      -3.056967\n    \n    \n      3\n      0.134687\n      -0.667723\n      -0.011556\n      0.839491\n      -0.173930\n      0.120969\n    \n    \n      4\n      -2.810668\n      -0.150654\n      -0.481044\n      -0.234694\n      0.899731\n      -2.777329\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.837658\n      1.315288\n      -0.364523\n      1.993571\n      1.584878\n      3.691555\n    \n    \n      996\n      -2.104663\n      -2.553118\n      -1.242666\n      0.201987\n      -0.305332\n      -6.003792\n    \n    \n      997\n      -1.195587\n      -1.577903\n      0.849912\n      0.327590\n      -0.001670\n      -1.597657\n    \n    \n      998\n      -0.035563\n      -0.489252\n      1.930498\n      -0.262645\n      0.825932\n      1.968971\n    \n    \n      999\n      -0.643267\n      -0.828981\n      -0.202735\n      -0.257866\n      0.070815\n      -1.862035\n    \n  \n\n1000 rows × 6 columns\n\n\n\n\ndf.query(\"f > 5\")[\"f\"].std()\n\n0.6609257763922614\n\n\n\ndf.query(\"f <= 5\")[\"f\"].std()\n\n2.1891090850524444\n\n\n\n\"tolist\" in dir(df)\n\nFalse\n\n\n\ndef search(obj, query):\n    import re\n\n    return list(filter(lambda x: re.search(query, x), dir(obj)))\n\n\nsearch(a, \"lis\")\n\n['tolist']\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndef search(obj, query):\n    import re\n    from rich import inspect\n    from rich.pretty import Pretty\n    from rich.panel import Panel\n\n    z = list(filter(lambda x: re.search(query, x), dir(obj)))\n    p = Panel(Pretty(\"a\"), title=f\"Searching for `{query}`\")\n    # return p\n    ps = []\n    for q in z:\n        ps.append(Panel(Pretty(inspect(getattr(obj, q), methods=True, docs=True))))\n\n\nsearch(np.random, \"normal\")\n\n╭─ <built-in method lognormal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.lognormal(...)                                                         │\n│                                                                                        │\n│ lognormal(mean=0.0, sigma=1.0, size=None)                                              │\n│                                                                                        │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                           │\n╰────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method multivariate_normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.multivariate_normal(...)                                                         │\n│                                                                                                  │\n│ multivariate_normal(mean, cov, size=None, check_valid='warn', tol=1e-8)                          │\n│                                                                                                  │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.normal(...)                                                         │\n│                                                                                     │\n│ normal(loc=0.0, scale=1.0, size=None)                                               │\n│                                                                                     │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                        │\n╰─────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n╭─ <built-in method standard_normal of numpy.random.mtrand.RandomState object at 0x10c627840> ─╮\n│ def RandomState.standard_normal(...)                                                         │\n│                                                                                              │\n│ standard_normal(size=None)                                                                   │\n│                                                                                              │\n│ 29 attribute(s) not shown. Run inspect(inspect) for options.                                 │\n╰──────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\ngetattr(pd.DataFrame().values, \"tolist\")\n\n<function ndarray.tolist>\n\n\n\npd.DataFrame.to\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n  \n\n\n\n\n\ndf = pd.DataFrame()\nsearch(df.values, \"list\")\n\n╭───────────────────────────────────────────── Searching for {query} ─────────────────────────────────────────────╮\n│ ['tolist']                                                                                                      │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nsearch(pd.DataFrame, \"to\")\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ [                                                                                                               │\n│     '_constructor',                                                                                             │\n│     '_constructor_sliced',                                                                                      │\n│     '_to_dict_of_blocks',                                                                                       │\n│     'kurtosis',                                                                                                 │\n│     'to_clipboard',                                                                                             │\n│     'to_csv',                                                                                                   │\n│     'to_dict',                                                                                                  │\n│     'to_excel',                                                                                                 │\n│     'to_feather',                                                                                               │\n│     'to_gbq',                                                                                                   │\n│     'to_hdf',                                                                                                   │\n│     'to_html',                                                                                                  │\n│     'to_json',                                                                                                  │\n│     'to_latex',                                                                                                 │\n│     'to_markdown',                                                                                              │\n│     'to_numpy',                                                                                                 │\n│     'to_orc',                                                                                                   │\n│     'to_parquet',                                                                                               │\n│     'to_period',                                                                                                │\n│     'to_pickle',                                                                                                │\n│     'to_records',                                                                                               │\n│     'to_sql',                                                                                                   │\n│     'to_stata',                                                                                                 │\n│     'to_string',                                                                                                │\n│     'to_timestamp',                                                                                             │\n│     'to_xarray',                                                                                                │\n│     'to_xml'                                                                                                    │\n│ ]                                                                                                               │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nq = \"normal\"\n\n\nnp.random.normal\n\n<function RandomState.normal>\n\n\n\nfrom rich import inspect\n\ninspect(eval(f\"np.random.{q}\"), methods=True, all=True)\n\n╭─────────────── <built-in method normal of numpy.random.mtrand.RandomState object at 0x10c627840> ───────────────╮\n│ def RandomState.normal(...)                                                                                     │\n│                                                                                                                 │\n│ normal(loc=0.0, scale=1.0, size=None)                                                                           │\n│                                                                                                                 │\n│            __doc__ = '\\n        normal(loc=0.0, scale=1.0, size=None)\\n\\n        Draw random samples from a     │\n│                      normal (Gaussian) distribution.\\n\\n        The probability density function of the normal  │\n│                      distribution, first\\n        derived by De Moivre and 200 years later by both Gauss and    │\n│                      Laplace\\n        independently [2]_, is often called the bell curve because of\\n           │\n│                      its characteristic shape (see the example below).\\n\\n        The normal distributions      │\n│                      occurs often in nature.  For example, it\\n        describes the commonly occurring         │\n│                      distribution of samples influenced\\n        by a large number of tiny, random              │\n│                      disturbances, each with its own\\n        unique distribution [2]_.\\n\\n        .. note::\\n  │\n│                      New code should use the ``normal`` method of a ``default_rng()``\\n            instance     │\n│                      instead; please see the :ref:`random-quick-start`.\\n\\n        Parameters\\n                 │\n│                      ----------\\n        loc : float or array_like of floats\\n            Mean (\"centre\") of    │\n│                      the distribution.\\n        scale : float or array_like of floats\\n            Standard     │\n│                      deviation (spread or \"width\") of the distribution. Must be\\n            non-negative.\\n    │\n│                      size : int or tuple of ints, optional\\n            Output shape.  If the given shape is,   │\n│                      e.g., ``(m, n, k)``, then\\n            ``m * n * k`` samples are drawn.  If size is        │\n│                      ``None`` (default),\\n            a single value is returned if ``loc`` and ``scale`` are   │\n│                      both scalars.\\n            Otherwise, ``np.broadcast(loc, scale).size`` samples are        │\n│                      drawn.\\n\\n        Returns\\n        -------\\n        out : ndarray or scalar\\n              │\n│                      Drawn samples from the parameterized normal distribution.\\n\\n        See Also\\n            │\n│                      --------\\n        scipy.stats.norm : probability density function, distribution or\\n       │\n│                      cumulative density function, etc.\\n        random.Generator.normal: which should be used   │\n│                      for new code.\\n\\n        Notes\\n        -----\\n        The probability density for the     │\n│                      Gaussian distribution is\\n\\n        .. math:: p(x) = \\\\frac{1}{\\\\sqrt{ 2 \\\\pi \\\\sigma^2    │\n│                      }}\\n                         e^{ - \\\\frac{ (x - \\\\mu)^2 } {2 \\\\sigma^2} },\\n\\n             │\n│                      where :math:`\\\\mu` is the mean and :math:`\\\\sigma` the standard\\n        deviation. The    │\n│                      square of the standard deviation, :math:`\\\\sigma^2`,\\n        is called the variance.\\n\\n  │\n│                      The function has its peak at the mean, and its \"spread\" increases with\\n        the        │\n│                      standard deviation (the function reaches 0.607 times its maximum at\\n        :math:`x +    │\n│                      \\\\sigma` and :math:`x - \\\\sigma` [2]_).  This implies that\\n        normal is more likely  │\n│                      to return samples lying close to the mean, rather\\n        than those far away.\\n\\n        │\n│                      References\\n        ----------\\n        .. [1] Wikipedia, \"Normal distribution\",\\n         │\n│                      https://en.wikipedia.org/wiki/Normal_distribution\\n        .. [2] P. R. Peebles Jr.,       │\n│                      \"Central Limit Theorem\" in \"Probability,\\n               Random Variables and Random       │\n│                      Signal Principles\", 4th ed., 2001,\\n               pp. 51, 51, 125.\\n\\n        Examples\\n  │\n│                      --------\\n        Draw samples from the distribution:\\n\\n        >>> mu, sigma = 0, 0.1 #  │\n│                      mean and standard deviation\\n        >>> s = np.random.normal(mu, sigma, 1000)\\n\\n         │\n│                      Verify the mean and the variance:\\n\\n        >>> abs(mu - np.mean(s))\\n        0.0  # may  │\n│                      vary\\n\\n        >>> abs(sigma - np.std(s, ddof=1))\\n        0.1  # may vary\\n\\n            │\n│                      Display the histogram of the samples, along with\\n        the probability density          │\n│                      function:\\n\\n        >>> import matplotlib.pyplot as plt\\n        >>> count, bins, ignored │\n│                      = plt.hist(s, 30, density=True)\\n        >>> plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) │\n│                      *\\n        ...                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\\n        ...    │\n│                      linewidth=2, color=\\'r\\')\\n        >>> plt.show()\\n\\n        Two-by-four array of samples  │\n│                      from N(3, 6.25):\\n\\n        >>> np.random.normal(3, 2.5, size=(2, 4))\\n                    │\n│                      array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\\n               [  │\n│                      0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\\n\\n        '                │\n│         __module__ = None                                                                                       │\n│           __name__ = 'normal'                                                                                   │\n│       __qualname__ = 'RandomState.normal'                                                                       │\n│           __self__ = RandomState(MT19937) at 0x10C627840                                                        │\n│ __text_signature__ = None                                                                                       │\n│           __call__ = def __call__(*args, **kwargs): Call self as a function.                                    │\n│          __class__ = class __class__():                                                                         │\n│        __delattr__ = def __delattr__(name, /): Implement delattr(self, name).                                   │\n│            __dir__ = def __dir__(): Default dir() implementation.                                               │\n│             __eq__ = def __eq__(value, /): Return self==value.                                                  │\n│         __format__ = def __format__(format_spec, /): Default object formatter.                                  │\n│             __ge__ = def __ge__(value, /): Return self>=value.                                                  │\n│   __getattribute__ = def __getattribute__(name, /): Return getattr(self, name).                                 │\n│             __gt__ = def __gt__(value, /): Return self>value.                                                   │\n│           __hash__ = def __hash__(): Return hash(self).                                                         │\n│           __init__ = def __init__(*args, **kwargs): Initialize self.  See help(type(self)) for accurate         │\n│                      signature.                                                                                 │\n│  __init_subclass__ = def __init_subclass__(...) This method is called when a class is subclassed.               │\n│             __le__ = def __le__(value, /): Return self<=value.                                                  │\n│             __lt__ = def __lt__(value, /): Return self<value.                                                   │\n│             __ne__ = def __ne__(value, /): Return self!=value.                                                  │\n│            __new__ = def __new__(*args, **kwargs): Create and return a new object.  See help(type) for accurate │\n│                      signature.                                                                                 │\n│         __reduce__ = def __reduce__(...) Helper for pickle.                                                     │\n│      __reduce_ex__ = def __reduce_ex__(protocol, /): Helper for pickle.                                         │\n│           __repr__ = def __repr__(): Return repr(self).                                                         │\n│        __setattr__ = def __setattr__(name, value, /): Implement setattr(self, name, value).                     │\n│         __sizeof__ = def __sizeof__(): Size of object in memory, in bytes.                                      │\n│            __str__ = def __str__(): Return str(self).                                                           │\n│   __subclasshook__ = def __subclasshook__(...) Abstract classes can override this to customize issubclass().    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\ninspect(inspect)\n\n╭─────────────────────────────────────── <function inspect at 0x10f3fc550> ───────────────────────────────────────╮\n│ def inspect(obj: Any, *, console: Optional[ForwardRef('Console')] = None, title: Optional[str] = None, help:    │\n│ bool = False, methods: bool = False, docs: bool = True, private: bool = False, dunder: bool = False, sort: bool │\n│ = True, all: bool = False, value: bool = True) -> None:                                                         │\n│                                                                                                                 │\n│ Inspect any Python object.                                                                                      │\n│                                                                                                                 │\n│ * inspect(<OBJECT>) to see summarized info.                                                                     │\n│ * inspect(<OBJECT>, methods=True) to see methods.                                                               │\n│ * inspect(<OBJECT>, help=True) to see full (non-abbreviated) help.                                              │\n│ * inspect(<OBJECT>, private=True) to see private attributes (single underscore).                                │\n│ * inspect(<OBJECT>, dunder=True) to see attributes beginning with double underscore.                            │\n│ * inspect(<OBJECT>, all=True) to see all attributes.                                                            │\n│                                                                                                                 │\n│ Args:                                                                                                           │\n│     obj (Any): An object to inspect.                                                                            │\n│     title (str, optional): Title to display over inspect result, or None use type. Defaults to None.            │\n│     help (bool, optional): Show full help text rather than just first paragraph. Defaults to False.             │\n│     methods (bool, optional): Enable inspection of callables. Defaults to False.                                │\n│     docs (bool, optional): Also render doc strings. Defaults to True.                                           │\n│     private (bool, optional): Show private attributes (beginning with underscore). Defaults to False.           │\n│     dunder (bool, optional): Show attributes starting with double underscore. Defaults to False.                │\n│     sort (bool, optional): Sort attributes alphabetically. Defaults to True.                                    │\n│     all (bool, optional): Show all attributes. Defaults to False.                                               │\n│     value (bool, optional): Pretty print value. Defaults to True.                                               │\n│                                                                                                                 │\n│ 35 attribute(s) not shown. Run inspect(inspect) for options.                                                    │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InventoryItem:\n    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n\n    name: str\n    unit_price: float\n    quantity_on_hand: int = 0\n\n\nc = InventoryItem(\"A\", 20)\n\n\nc\n\nInventoryItem(name='A', unit_price=20, quantity_on_hand=0)\n\n\n\nclass InventoryItemOld:\n    def __init__(self, name: str, unit_price: float, \n                 quantity_on_hand: int = 0):\n        self.name = name\n        self.unit_price = unit_price\n        self.quantity_on_hand = quantity_on_hand\n\n\nd = InventoryItemOld(\"A\", 2)\nd\n\n<__main__.InventoryItemOld at 0x166340430>\n\n\n\ndef greeting(name: str) -> str:\n    return 'Hello ' + name\n\n\ngreeting(\"Abc\")\n\n'Hello Abc'\n\n\n\ngreeting(10)\n\nTypeError: can only concatenate str (not \"int\") to str"
  },
  {
    "objectID": "notebooks/posts/condition-inverse.html",
    "href": "notebooks/posts/condition-inverse.html",
    "title": "Conditioning and Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Showing that np.linalg.solve is better conditioned than np.linalg.inv for linear regression normal equations\n\n# Generate data\nn = 100\np = 10\nX = np.random.randn(n, p)\ntheta = np.random.randn(p)\ny = X @ theta + np.random.randn(n)\n\n# Solve normal equations\ntheta_hat = np.linalg.solve(X.T @ X, X.T @ y)\ntheta_hat_inv = np.linalg.inv(X.T @ X) @ X.T @ y\n\n# Compare the condition numbers\nprint(np.linalg.cond(X.T @ X))\nprint(np.linalg.cond(np.linalg.inv(X.T @ X)))\n\n# Plot the difference between the two solutions\nplt.plot(theta_hat - theta_hat_inv)\nplt.title('Difference between solutions')\nplt.xlabel('Index')\nplt.ylabel('Difference')\nplt.show()\n\n\n2.980877596192165\n2.980877596192165"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebook",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n1d CNN\n\n\nNipun Batra\n\n\n\n\nApr 3, 2023\n\n\nCNN\n\n\nNipun Batra\n\n\n\n\nMar 31, 2023\n\n\nGenerating names using MLPs\n\n\nNipun Batra\n\n\n\n\nMar 1, 2023\n\n\nAutoDiff in JAX and PyTorch\n\n\nNipun Batra\n\n\n\n\nFeb 28, 2023\n\n\nLogistic Regression\n\n\nNipun Batra\n\n\n\n\nFeb 28, 2023\n\n\nNeural Network\n\n\nNipun Batra\n\n\n\n\nFeb 16, 2023\n\n\nGradient Descent\n\n\nNipun Batra\n\n\n\n\nFeb 14, 2023\n\n\nTaylor Series\n\n\nNipun Batra\n\n\n\n\nFeb 10, 2023\n\n\nConditioning and Linear Regression\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nCNN Edge 2d\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nLinear Regression: Geometric Perspective\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nMaths and JAX: Low Rank\n\n\nNipun Batra\n\n\n\n\nJan 31, 2023\n\n\nMaths and JAX\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nDataset splitting for machine learning\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nGrid Search\n\n\nNipun Batra\n\n\n\n\nJan 17, 2023\n\n\nSome Python Utilities\n\n\nNipun Batra\n\n\n\n\nJan 12, 2023\n\n\nDT Regression\n\n\nNipun Batra\n\n\n\n\nJan 12, 2023\n\n\nPandas tips\n\n\nNipun Batra\n\n\n\n\nJan 10, 2023\n\n\nMisc tips\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "grading.html",
    "href": "grading.html",
    "title": "Grading Policy",
    "section": "",
    "text": "Quizzes: 60%\n\n10% each\nBest 6 out of 8\n\n\n\n\nAssignments: 40%\n\nVariable weight (e.g. some assignments would be 5%, some 10%, etc.)\nSome assignments would involve:\n\nMaking pull requests to public repositories\nWriting Hugging Face Spaces like demos\n\n\n\n\n\nBonus: up to 6%\n\nMaking a non-trivial pull request to a well-starred public repo (4%)\nGetting the PR accepted (2%)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 654 Machine Learning",
    "section": "",
    "text": "Summary\n\nInstructor: Nipun Batra (nipun.batra@iitgn.ac.in)\nTeaching Assistants: Zeel B Patel, Shriraj Sawant, Shrutimoy, Sarth Dubey, Hitarth Gandhi, Saagar Parikh, Eshan Gujarathi, Gautam Vashishta, Aadesh Desai\nCourse Timings: Tuesday, Thursday 330-450 PM IST in 1/101\nSlack Invite\n\n\n\nPre-requisites:\n\nGood experience in Python programming\nProbability\nLinear Algebra\n\nCourse preparation: Students are encouraged to study some of the following to refresh their understanding of some of the prerequisities before the course formally begins.\n\nFirst four chapters of the Python Data Science handbook\nSome material on Linear Algebra\nKhan academy course on Stats and Probability\n\n\n\nReference textbooks:\n\nGareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. An Introduction to Statistical Learning with Applications in R\nChristopher Bishop. Pattern Recognition and Machine Learning. Springer, 2006.[Freely available online]\nFriedman J, Hastie T, Tibshirani R. The elements of statistical learning. New York, NY, USA:: Springer series in statistics; 2001.[Freely available online]\nDuda RO, Hart PE, Stork DG. Pattern classification. John Wiley & Sons; 2012 Nov 9.\nMitchell TM. Machine learning. 1997. Burr Ridge, IL: McGraw Hill. 1997;45(37):870-7.\nMurphy, K. Machine Learning: A Probabilistic Perspective. MIT Press\nGoodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning. Cambridge: MIT press; 2016 Nov 18.[Freely available online]\n\n\n\nSome other ML courses\n\nNPTEL course by Balaram Ravindran\nCMU course by Tom Mitchell and Maria-Florina Balcan\nCoursera ML course by Andrew Ng\nFAST.ai course on ML\nPractical deep learning for coders by FAST.ai\nCourse by Alex Ihler, UCI"
  },
  {
    "objectID": "exams/assignment-5.html",
    "href": "exams/assignment-5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "General instructions\n\nFeel free to use any framework of your choice: PyTorch, JAX (+Flax/Equinox), Tensorflow (+Keras)\nWe are not providing any code template. However, it is recommended you adhere to high code quality standards.\nFeel free to make use of ChatGPT, CoPilot, etc. like tools. Cite where you used them. However, you should still be able to explain your code during the viva. Also, you need to be careful with the hallucinations of these tools!\nAs before, this assignment is in a group of two students. You do not need to submit the assignment but can show your private repo to the TA during the viva.\n\n\n\nIn this question, you have to compare the performance on a binary classification task of the following models:\n\nVGG (1 block)\nVGG (3 blocks)\nVGG (3 blocks) with data augmentation\nTransfer learning using VGG16 or VGG19\n\nRefer this article You do not need to write your own code. You can reuse the code from the post. Or, you could roll out your own implemenation. Either way, you should be able to explain your code during the viva.\nYou need to create the dataset on your own based on your first names. For instance if the first name of the team members are: Siya and Raghav, they can choose a dataset of their liking based on any names, place, animal or thing. As examples:\n\nSeoul v/s Riyadh\nSnake v/s Rat\nSquirrel v/s Rabbit\nSambhar v/s Roti\n\nYou can refer to resource 1 or resource 2 or plainly download 100 images of both classes (total 200 images). Of these 100 images of each class, we will use 80 for training and 20 for testing. You get 1 mark for dataset creation [1 mark]\nCreate a table with models as rows and the following columns [2 marks (0.5 marks for each model)]\n\nTraining time\nTraining loss\nTraining accuracy\nTesting accuracy\nNumber of model parameters\n\nWe will now be using Tensorboard for visualizing network performance. You are suggested to refer to:\n\nPyTorch + Tensorboard\nTensorflow + Tensorboard\n\nUse Tensorboard to log the following and present screenshots/images [1 mark]\nScalars\n\nTraining loss v/s iterations (and not epochs)\nTraining accuracy v/s iterations (and not epochs)\nTesting accuracy v/s iterations (and not epochs)\n\nImages\n\nShow all images from the test set and their predictions\n\nNow you have to present various insights. For instance, you should discuss the following: [2 marks (0.5 marks for each question)]\n\nAre the results as expected? Why or why not?\nDoes data augmentation help? Why or why not?\nDoes it matter how many epochs you fine tune the model? Why or why not?\nAre there any particular images that the model is confused about? Why or why not?\n\nNow, create a MLP model with comparable number of parameters as VGG16 and compare your performance with the other models in the table. You can choose the distribution of number of neurons and number of layers. What can you conclude? [1 mark]"
  },
  {
    "objectID": "exams/q4.html",
    "href": "exams/q4.html",
    "title": "Quiz 4 (25 March)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\n\nConsider the figure above, where we fit the model \\(p(y=1 \\mid \\mathbf{x}, \\bm{{\\theta}})=\\sigma\\left(\\theta_0+\\theta_1 x_1+\\theta_2 x_2\\right)\\) by maximum likelihood, i.e., we minimize \\(J_a({\\theta})=-\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)\\) where \\(\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)\\) is the log likelihood on the training set. In the questions below, when multiple decision boundaries are possible, you should choose the one which minimizes the number of classification errors on the training dataset.\n\n\nSketch a decision boundary for the model. How many classification errors does your method make? (1 mark)\n\n\nNow, we regularize only the \\(\\theta_0\\) parameter, i.e., we minimize: \\(J_b({\\theta})=-\\ell\\left(\\bm{{\\theta}}, \\mathcal{D}_{\\text {train }}\\right)+\\lambda \\theta_0^2\\). Suppose \\(\\lambda\\) is a very large number, so we regularize \\(\\theta_0\\) all the way to 0, but all other parameters are unregularized. Sketch a possible decision boundary. How many classification errors does your method make? (1 mark)\n\n\nRepeat part (b), but we now instead regularize the \\(\\theta_1\\) parameter. (1 mark)\n\n\nRepeat part (b), but we now instead regularize the \\(\\theta_2\\) parameter. (1 mark)\n\n\nProve that softmax is equivalent to sigmoid when there are only two classes. (1 mark)\n\\(y = \\sigma(z)\\), where \\(\\sigma\\) is the sigmoid function. We also know that \\(z = f(a)\\). Find \\(\\dfrac{\\partial y}{\\partial a}\\). (1 mark)\nLet us consider a \\(K\\)-class logistic regression problem. For some example, \\(x\\), we get our outputs before the application of softmax as: \\(z_1=x^T\\theta_1\\), \\(\\cdots , z_k=x^T\\theta_k\\), \\(\\cdots ,z_K=x^T\\theta_K\\). We denote the vector of outputs as \\(\\vec{z} = \\left[\\begin{array}{@{}c@{}}  z_{1} \\\\  z_{2} \\\\  \\vdots \\\\  z_{K}  \\end{array} \\right]\\)\n\nWe will try to now use the cross entropy loss function to train our model. One of the terms in the cross entropy loss function is: \\(\\log\\left(\\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\\right)\\) which we refer to as \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\). However, we find that \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) cannot be computed directly for several cases. When \\(z_k\\) is a large number (e.g. 5000), a computer is unable to compute \\(e^{z_k}\\) as an overflow occurs (\\(e^{z_k}\\) = inf). When \\(z_k\\) is a large negative number (e.g. -5000), \\(e^{z_k}\\) = 0.0.\n\n\nWhat problem occurs in computing \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) when all elements of \\(\\vec{z}\\) are large (in magnitude) negative numbers (e.g. all \\(z_i < -6000\\))? (1 mark)\n\n\nModify the \\(\\mathrm{LOGSOFTMAX}(z_k, \\vec{z})\\) expression using some trick so that we are able to compute it for any \\(z_k\\) and \\(\\vec{z}\\). You need to show the steps/simplifications you make. Show that this trick solves both the above problems (overflow and the problem you find in part (a) of this question) (2 marks)\n\n\n\nWe use a new type of coin for coin toss experiments. For this coin, the probability of heads goes down exponentially with the draw. Assuming the probaility of heads for the first draw (\\(i=1\\)) is \\(\\theta\\) and for the \\(i\\)th draw is \\(\\theta_i = \\dfrac{\\theta}{2^{i-1}}\\). What is the maximum likelihood estimate for \\(\\theta\\) for obtaining the draws as: T, H, H. Assume that each draw is independent of the others. Ofcourse, the identical assumption can not be made. (1 mark)"
  },
  {
    "objectID": "exams/q3.html",
    "href": "exams/q3.html",
    "title": "Quiz 3 (27 Feb)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\nMany evaluation metrics decompose over the training examples. For example, the loss function for linear regression (proportional to mean squared error) is given as: \\[L(\\theta) = \\frac{1}{2N}\\sum_{i=1}^N (y_i - \\sum_{d=1}^D \\theta_d x_i^d)^2\\] where \\(N\\) is the number of training examples, \\(x_i\\) is the \\(i^{th}\\) training example and \\(y_i\\) is the corresponding label. Mention any evaluation metric/loss function in machine learning that does not decompose over the training examples. [1 mark]\nWe saw the figure showing SGD convergence. \n\n\n2A) Prove that SGD is an unbiased estimator. [1 mark]\n2B) It seems that the SGD algorithm is not converging to the global minimum. Why do you think this is the case? [1 mark]\n2C). Why is it generally a good idea to use a small learning rate for SGD? [1 mark]\n2D) It seems that while the SGD algorithm is not converging, but it seems to be very quickly moving close to the global minimum. Why is SGD good initially when the loss is high? To help you answer this question, we pose a series of questions. Consider a simplification of linear regression. Our data is 1d. Our model is \\(y=\\theta x\\). Consider a dataset of \\(N\\) examples. Obtain the closed form solution for \\(\\theta\\) in terms of the scalars \\(x_i\\) and \\(y_i\\) for \\(i=1, \\cdots, N\\). [1 marks]\n2E) Consider \\(N=3\\) and a datset of the form \\(x_1=1, x_2=2, x_3=3, y_1=1, y_2=2.2, y_3=2.8\\). Plot the approximate contour plot of the loss function \\(L(\\theta)\\) for \\(\\theta \\in [-1, 3]\\). [1 marks].\n2F) Plot the loss v/s parameters (\\(\\theta\\)) corresponding to the loss for each training input. [1 marks]\n2G) Now, answer why SGD works initially, when the loss is high. [1 marks]\n\n\nIn an above question, we proved that the SGD estimator is an unbiased estimator. We have also previously discussed that we typically have a bias-variance tradeoff in our models. In the recent assignment question, we have plotted the bias and variance for different complexity trees. In this question, you have to derive the mean squared error in terms of three terms: bias, variance and irreducible noise.\n\n\nLet us assume our data is generated from a `true’ function \\(f(x)\\) and we have some additional zero mean normally distributed noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\n\\[y = f(x) + \\epsilon\\]\nWe can use some model such as a decision tree or linear regression to approximate \\(f(x)\\). We now consider a single training example \\((x_0, y_0)\\). We can define the mean squared error as:\n\\[MSE = \\mathbb{E}[(y_0 - \\hat{f}(x_0)^2)]\\]\nwhere \\(y\\) is the true label and \\(\\hat{f}(x_0)\\) is the predicted label. The expectation is over all possible training sets that could have been generated.\nTo keep the notation simple, we refer \\(f(x_0)\\) as \\(f\\) and \\(\\hat{f}(x_0)\\) as \\(\\hat{f}\\). Thus, we can write \\[MSE = \\mathbb{E}[(f - \\hat{f})^2]\\] We also define the bias as the difference between the true function and the predicted function, evaluated at the training example: \\[bias = \\mathbb{E}[\\hat{f}] - f\\] or, \\[bias = \\overline{f} - f\\] where \\(\\overline{f}\\) is the average/expectation of the predicted function over all possible training sets.\nWe define the variance as: \\[variance = VAR(\\hat{f})\\] or,\n\\[variance = \\mathbb{E}[(\\hat{f} - \\overline{f})^2]\\]\nWe define irreducible noise as the variance of the noise term \\(\\epsilon\\):\n\\[irreducible = VAR(\\epsilon)\\] or,\n\\[irreducible = \\sigma^2\\]\nUsing the above definitions, show that the mean squared error can be written as:\n\\[MSE = bias^2 + variance + irreducible\\]\n[2 marks]"
  },
  {
    "objectID": "exams/q1.html",
    "href": "exams/q1.html",
    "title": "Quiz 1 (18 Jan)",
    "section": "",
    "text": "Instructions\n\nTotal Time: 30 mins\n\n\n\nRemember the entropy discussion we had in the lecture. We saw that for the Tennis example, the maximum entropy is 1.0. What is the maximum entropy an Imagenet classification problem, where we have 1024 classes? [1 mark]\nGiven the following dataset, what attribute/feature would the decision tree algorithm choose to split the data on for the first iteration? Why? [1 mark]\n\n\n\n\n\n\n\n\n\n\n\nSample #\nTomato radius\nTomato weight\nTomato color\nTomato quality\n\n\n\n\n1\n1\n1\n1\nGood\n\n\n2\n1\n1\n2\nGood\n\n\n3\n1\n2\n1\nBad\n\n\n4\n1\n2\n2\nBad\n\n\n5\n2\n1\n1\nGood\n\n\n6\n2\n2\n2\nGood\n\n\n\n\nIn the lectures we saw that np.std(x) and pd.Series(x).std() are different. Why? [1 mark]\nQuoting Wikipedia:\n\n\nPruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.\n\n\nPre-pruning procedures prevent a complete induction of the training set by replacing a stop () criterion in the induction algorithm (e.g. max. Tree depth or information gain (Attr)> minGain). Pre-pruning methods are do not induce an entire set, but rather trees remain small from the start.\n\nCreate a decision tree for the following classification problem. Explain why the pre-pruning using information gain approach can be limiting? [2 marks]\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n0\n0\n0\n\n\n0\n1\n1\n\n\n1\n0\n1\n\n\n1\n1\n0\n\n\n\n\nVisualize the decision tree for the following regression problem, where the ground truth is the function \\(y = x + 2\\). Use \\(x = \\{1, 2, \\cdots 4\\}\\) as the training dataset. Also visualize the learnt function [2 marks]\nCreate an example ground truth and prediction where the mean absolute error is 100 and mean error is 0. [0.5 marks]\nCreate one confusion matrix for 100 total examples where the precision is 0.8, recall is 0.5. [1 mark]\nShow visualisation of 1d regression problem for continuous inputs showing a good fit, a high bias and a high variance fit. [1.5 mark]"
  },
  {
    "objectID": "exams/prereq.html",
    "href": "exams/prereq.html",
    "title": "Prerequsite test",
    "section": "",
    "text": "Instructions\n\nThis test is open book, open internet, open notes. You can use any resources you want to solve the problems.\nYou should be typing your answers in a Jupyter notebook.\nThe submission would be a link to a public GitHub repository containing the notebook. Fill this form to submit your solution.\nA random subset of students may have a viva post the exam. The viva would be based on the notebook and the solutions you have provided.\nThe test is open till 6th January 2022 9 PM. You can submit your solutions anytime before that.\nThis problem has to be solved individually. You cannot collaborate with anyone else.\nThe code should be written using Python.\nSome questions may require you to answer in text. You can use markdown cells to write your answers. Some questions may require you to write code. You can use code cells to write your code. Some questions may require you to write mathematical expressions. You can use LaTeX to write your expressions. You can write such LaTeX expressions in markdown cells.\nFor any other questions, please ask on the General channel on Slack.\n\n\n\n\nQuestions\n\nHow many multiplications and additions do you need to perform a matrix multiplication between a (n, k) and (k, m) matrix? Explain.\nWrite Python code to multiply the above two matrices. Solve using list of lists and then use numpy. Compare the timing of both solutions. Which one is faster? Why?\nFinding the highest element in a list requires one pass of the array. Finding the second highest element requires 2 passes of the the array. Using this method, what is the time complexity of finding the median of the array? Can you suggest a better method? Can you implement both these methods in Python and compare against numpy.median routine in terms of time?\nWhat is the gradient of the following function with respect to x and y? \\[\nx^2y+y^3\\sin(x)\n\\]\nUse JAX to confirm the gradient evaluated by your method matches the analytical solution corresponding to a few random values of x and y\nUse sympy to confirm that you obtain the same gradient analytically.\nCreate a Python nested dictionary to represent hierarchical information. We want to store record of students and their marks. Something like:\n\n2022\n\nBranch 1\n\nRoll Number: 1, Name: N, Marks:\n\nMaths: 100, English: 70 …\n\n\nBranch 2\n\n2023\n\nBranch 1\nBranch 2\n\n2024\n\nBranch 1\nBranch 2\n\n2025\n\nBranch 1\nBranch 2\n\n\nStore the same information using Python classes. We have an overall database which is a list of year objects. Each year contains a list of branches. Each branch contains a list of students. Each student has some properties like name, roll number and has marks in some subjects.\nUsing matplotlib plot the following functions on the domain: x = 0.5 to 100.0 in steps of 0.5.\n\n\\(y = x\\)\n\\(y = x^2\\)\n\\(y = \\frac{x^3}{100}\\)\n\\(y = \\sin(x)\\)\n\\(y = \\frac{\\sin(x)}{x}\\)\n\\(y = \\log(x)\\)\n\\(y = e^x\\)\n\nUsing numpy generate a matrix of size 20X5 containing random numbers drawn uniformly from the range of 1 to 2. Using Pandas create a dataframe out of this matrix. Name the columns of the dataframe as “a”, “b”, “c”, “d”, “e”. Find the column with the highest standard deviation. Find the row with the lowest mean.\nAdd a new column to the dataframe called “f” which is the sum of the columns “a”, “b”, “c”, “d”, “e”. Create another column called “g”. The value in the column “g” should be “LT8” if the value in the column “f” is less than 8 and “GT8” otherwise. Find the number of rows in the dataframe where the value in the column “g” is “LT8”. Find the standard deviation of the column “f” for the rows where the value in the column “g” is “LT8” and “GT8” respectively.\nWrite a small piece of code to explain broadcasting in numpy.\nWrite a function to compute the argmin of a numpy array. The function should take a numpy array as input and return the index of the minimum element. You can use the np.argmin function to verify your solution."
  },
  {
    "objectID": "exams/q2.html",
    "href": "exams/q2.html",
    "title": "Quiz 2 (8 Feb)",
    "section": "",
    "text": "Total Time: 1 hour 15 mins AND Total Marks: 10\n\n\nIn bootstrap sampling, we sample with replacement from the original dataset. Let us assume that the original dataset of size \\(N\\) has all distinct elements. As an example if \\(N=8\\), we may have the dataset as \\(\\{1, 2,3, 4, 5, 6, 7, 8\\}\\). A bootstrap sample (or round) is also of size \\(N\\) and can contain some elements more than once. For example, a bootstrap sample may be \\(\\{8, 8, 3, 4, 5, 1, 8, 5\\}\\). The unique elements in this sample are: \\(\\{1, 3, 4, 5, 8\\}\\). This sample has 5 unique elements. Show that on average the number of unique elements in a bootstrap sample is \\(63.2\\%\\) of \\(N\\). [1.5 marks]\nWe studied the ADABoost classification algorithm for binary classification. We wrote the final prediction as: \\(\\mathrm{SIGN}(\\sum{\\alpha_i}h_i(x))\\) where \\(\\alpha_i\\) is the weight of the classifier \\(h_i\\) and \\(h_i(x)\\) is the prediction of the classifier \\(h_i\\) on the input \\(x\\). We also noted that each prediction \\(h_i(x)\\) is either \\(+1\\) or \\(-1\\).\nExtend ADABoost to multi-class classification where we have \\(K\\) classes and each classifier predicts one of the \\(K\\) classes (a number from \\(\\{1 \\cdots K\\}\\)). As an example, if we have \\(m=4\\) members in the ensemble, we may have something like \\(h_1(x) = 1\\), \\(h_2(x) = 2\\), \\(h_3(x) = 3\\) and \\(h_4(x) = 2\\). Now, write the formula for prediction for multi-class classification using the ensemble of classifiers, i.e. for any input \\(x\\), which class amongst \\(\\{1 \\cdots K\\}\\)) will be predicted as a function of \\(\\alpha_i\\)s and \\(h_i(x)\\)? Note: do not use the concept of one-vs-one or one-vs-all here. [2 marks]\nWhich hyperparameter can you vary to control the bias-variance tradeoff (or complexity) for decision trees? Draw the bias variance tradeoff curve for decision trees using this hyperparameter. Explain your answer. [1.5 mark]\nThe normal equation for linear regression is given as: \\(\\hat{\\theta} = (X^TX)^{-1}X^Ty\\). Instead of computing the normal equation directly, let us use the SVD decomposition of X. We decompose X as \\(X = U\\Sigma V^T\\).\n\n\nRewrite the normal equation using the reduced SVD decomposition of X, that is write \\(\\hat{\\theta}\\) in terms of \\(U\\), \\(\\Sigma\\) and \\(V\\) and \\(y\\).\n\n\nFor this question, let us assume that \\(X\\) is of size \\(n \\times m\\) and \\(y\\) is of size \\(n \\times 1\\). Let us also assume that the number of features \\(m\\) is significantly less than the number of samples \\(n\\).\n\n\nOnce you have written \\(\\hat{\\theta}\\) in terms of \\(U\\), \\(\\Sigma\\) and \\(V\\) and \\(y\\), find the time complexity of computing \\(\\hat{\\theta}\\) using the reduced form of SVD decomposition of \\(X\\). [4 marks]\n\n\nWe provide some background on the SVD decomposition of a matrix \\(X\\) below: The reduced form of SVD decomposition of \\(X\\) is given as \\(X = U\\Sigma V^T\\) where \\(U\\) is of size \\(n \\times m\\), \\(\\Sigma\\) is of size \\(m \\times m\\) and \\(V\\) is of size \\(m \\times m\\). The columns of \\(U\\) are called the left singular vectors of \\(X\\) and the columns of \\(V\\) are called the right singular vectors of \\(X\\). The singular matrix \\(\\Sigma\\) is a diagonal matrix: it has zeros everywhere except on the diagonal. The diagonal elements of \\(\\Sigma\\) are the singular values of \\(X\\). The singular values of \\(X\\) are always non-negative and are arranged in decreasing order. The singular values of \\(X\\) are also called the eigenvalues of \\(X^TX\\).\n\n\nFurther, for reduced SVD, \\(U^TU = I\\) and \\(V^TV = I\\) and \\(VV^T = I\\) where \\(I\\) is the identity matrix.\n\n\nWe also provide some background on the time complexity of matrix multiplication below: Let \\(A\\) be of size \\(n \\times m\\), \\(B\\) be of size \\(m \\times p\\) and \\(C\\) be of size \\(n \\times p\\). The time complexity of computing \\(C = AB\\) is \\(O(nmp)\\). Further, the time complexity of inverse of a \\(n \\times n\\) matrix \\(A\\) is \\(O(n^3)\\). The time complexity of computing SVD of the above \\(n \\times m\\) matrix \\(X\\) is \\(O(nm^2)\\). You should factor this time complexity in your answer for computing \\(\\hat{\\theta}\\).\n\n\nBONUS: Solve the above problem (computing \\(\\hat{\\theta}\\) and its time complexity) with the full version of SVD, what changes will you need to make? The full version of SVD is given as \\(X = U\\Sigma V^T\\) where \\(U\\) is of size \\(n \\times n\\), \\(\\Sigma\\) is of size \\(n \\times m\\) and \\(V\\) is of size \\(m \\times m\\). \\(U\\) and \\(V\\) are orthogonal matrices. [2 marks]\n\n\nLet us assume \\(K\\) members in an ensemble. For simplicity let us assume that each member in the ensemble has the same probability of error \\(p<0.5\\). We saw in the class that the probability of error (given by the binomial expansion) reduces as we increase the number of members in the ensemble. But, empirically adding more members in an ensemble may not always reduce the error. Why? [1 mark]"
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2023\n\n\nPrerequsite test\n\n\n\n\n\n\nJan 18, 2023\n\n\nQuiz 1 (18 Jan)\n\n\n\n\n\n\nFeb 8, 2023\n\n\nQuiz 2 (8 Feb)\n\n\n\n\n\n\nFeb 27, 2023\n\n\nQuiz 3 (27 Feb)\n\n\n\n\n\n\nMar 25, 2023\n\n\nQuiz 4 (25 March)\n\n\n\n\n\n\nMar 26, 2023\n\n\nAssignment 5\n\n\n\n\n\n\n\n\nNo matching items"
  }
]